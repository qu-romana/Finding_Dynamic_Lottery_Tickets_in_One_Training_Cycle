{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import math\n",
        "\n",
        "# Enhanced VGG19 with improved mask application and layer-wise sparsity\n",
        "class ImprovedDynamicSparseVGG19(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ImprovedDynamicSparseVGG19, self).__init__()\n",
        "        self.masks = {}  # Store masks for each layer\n",
        "        self.layer_importance = {}  # Store importance scores for layers\n",
        "\n",
        "        # VGG19 configuration: 'M' means MaxPool2d\n",
        "        cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
        "\n",
        "        self.features = self._make_layers(cfg)\n",
        "\n",
        "        # Enhanced Classifier with better regularization\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),  # Reduced dropout for better gradient flow\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),  # Added BatchNorm\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),  # Added BatchNorm\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "        # Better initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                          nn.BatchNorm2d(x),\n",
        "                          nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def apply_masks(self):\n",
        "        \"\"\"Apply masks to weights during forward pass\"\"\"\n",
        "        if not hasattr(self, 'masks') or not self.masks:\n",
        "            return\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.named_parameters():\n",
        "                mask = self.masks.get(name, None)\n",
        "                if mask is not None:\n",
        "                    param.mul_(mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply masks before forward pass\n",
        "        self.apply_masks()\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "# Enhanced pruning functions with layer-wise sparsity\n",
        "def compute_layer_wise_sparsity(model, base_sparsity, layer_importance_scores=None):\n",
        "    \"\"\"Compute layer-wise sparsity based on layer sensitivity\"\"\"\n",
        "    layer_sparsities = {}\n",
        "\n",
        "    # Default importance scores (empirically determined for VGG)\n",
        "    if layer_importance_scores is None:\n",
        "        layer_importance_scores = {\n",
        "            'features.0': 0.7,   # First conv layer - less sparsity\n",
        "            'features.3': 0.85,\n",
        "            'features.7': 0.9,\n",
        "            'features.10': 0.95,\n",
        "            'features.14': 0.95,\n",
        "            'features.17': 0.98,\n",
        "            'features.20': 0.98,\n",
        "            'features.24': 0.98,\n",
        "            'features.27': 0.98,\n",
        "            'features.30': 0.98,\n",
        "            'features.34': 0.98,\n",
        "            'features.37': 0.98,\n",
        "            'features.40': 0.98,\n",
        "            'features.44': 0.98,\n",
        "            'features.47': 0.98,\n",
        "            'features.50': 0.98,\n",
        "            'classifier.1': 0.95,\n",
        "            'classifier.4': 0.95,\n",
        "            'classifier.7': 0.9,  # Last layer - less sparsity\n",
        "        }\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "            # Adjust sparsity based on layer importance\n",
        "            if name in layer_importance_scores:\n",
        "                layer_sparsities[name + '.weight'] = min(layer_importance_scores[name], base_sparsity)\n",
        "            else:\n",
        "                layer_sparsities[name + '.weight'] = base_sparsity\n",
        "\n",
        "    return layer_sparsities\n",
        "\n",
        "def create_magnitude_based_masks_improved(model, sparsity_dict, use_gradient_info=False, gradients=None):\n",
        "    \"\"\"Enhanced UNSTRUCTURED magnitude-based pruning (element-wise like original)\"\"\"\n",
        "    masks = {}\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name and name in sparsity_dict:\n",
        "            sparsity = sparsity_dict[name]\n",
        "\n",
        "            # Compute importance scores\n",
        "            if use_gradient_info and gradients is not None and name in gradients:\n",
        "                # Combine magnitude and gradient information\n",
        "                importance = torch.abs(param.data) * torch.abs(gradients[name])\n",
        "            else:\n",
        "                importance = torch.abs(param.data)\n",
        "\n",
        "            # UNSTRUCTURED pruning for all layers (element-wise)\n",
        "            flattened = importance.flatten()\n",
        "            num_params = flattened.shape[0]\n",
        "            num_pruned = int(num_params * sparsity)\n",
        "\n",
        "            if num_pruned > 0 and num_pruned < num_params:\n",
        "                # Find threshold for this layer\n",
        "                sorted_importance, _ = torch.sort(flattened)\n",
        "                threshold = sorted_importance[num_pruned]\n",
        "                mask = (importance >= threshold).float()\n",
        "            elif num_pruned >= num_params:\n",
        "                # Prune everything (shouldn't happen with proper sparsity values)\n",
        "                mask = torch.zeros_like(param)\n",
        "            else:\n",
        "                # Keep everything\n",
        "                mask = torch.ones_like(param)\n",
        "\n",
        "            masks[name] = mask\n",
        "\n",
        "    return masks\n",
        "\n",
        "def create_global_magnitude_masks(model, target_sparsity):\n",
        "    \"\"\"Global magnitude pruning (exactly like your original approach)\"\"\"\n",
        "    all_weights = []\n",
        "    param_info = []\n",
        "\n",
        "    # Gather all weights\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name and param.dim() > 1:\n",
        "            all_weights.append(param.data.abs().flatten())\n",
        "            param_info.append((name, param))\n",
        "\n",
        "    if not all_weights:\n",
        "        return {}\n",
        "\n",
        "    # Concatenate all weights and find global threshold\n",
        "    all_weights_concat = torch.cat(all_weights)\n",
        "    total_params = all_weights_concat.numel()\n",
        "    num_to_prune = int(total_params * target_sparsity)\n",
        "\n",
        "    if num_to_prune > 0:\n",
        "        sorted_weights, _ = torch.sort(all_weights_concat)\n",
        "        threshold = sorted_weights[num_to_prune]\n",
        "    else:\n",
        "        threshold = 0\n",
        "\n",
        "    # Create masks based on global threshold\n",
        "    masks = {}\n",
        "    for name, param in param_info:\n",
        "        mask = (param.data.abs() >= threshold).float()\n",
        "        masks[name] = mask\n",
        "\n",
        "    return masks\n",
        "\n",
        "def linear_sparsity_schedule(current_epoch, total_epochs, initial_sparsity, target_sparsity):\n",
        "    \"\"\"Linear sparsity schedule as requested\"\"\"\n",
        "    progress = current_epoch / total_epochs\n",
        "    current_sparsity = initial_sparsity + (target_sparsity - initial_sparsity) * progress\n",
        "    return min(current_sparsity, target_sparsity)\n",
        "\n",
        "# Label smoothing loss\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        logprobs = F.log_softmax(x, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss.mean()\n",
        "\n",
        "# Knowledge distillation helper\n",
        "class KnowledgeDistillationLoss(nn.Module):\n",
        "    def __init__(self, temperature=4.0, alpha=0.7):\n",
        "        super(KnowledgeDistillationLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.criterion_kd = nn.KLDivLoss(reduction='batchmean')\n",
        "        self.criterion_ce = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # Distillation loss\n",
        "        soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)\n",
        "        soft_predictions = F.log_softmax(student_logits / self.temperature, dim=1)\n",
        "        distillation_loss = self.criterion_kd(soft_predictions, soft_targets) * (self.temperature ** 2)\n",
        "\n",
        "        # Standard cross-entropy loss\n",
        "        ce_loss = self.criterion_ce(student_logits, labels)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = self.alpha * distillation_loss + (1 - self.alpha) * ce_loss\n",
        "        return total_loss\n",
        "\n",
        "def train_improved(model, device, train_loader, optimizer, epoch, criterion, scaler=None, use_amp=True):\n",
        "    \"\"\"Enhanced training function with mixed precision\"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if use_amp and scaler is not None:\n",
        "            with autocast():\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\t'\n",
        "                  f'Loss: {loss.item():.6f}\\tAcc: {100.*correct/total:.2f}%')\n",
        "\n",
        "    return train_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "def test_improved(model, device, test_loader, criterion):\n",
        "    \"\"\"Enhanced testing function\"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
        "          f'Accuracy: {correct}/{total} ({accuracy:.2f}%)\\n')\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "def calculate_sparsity(model):\n",
        "    \"\"\"Calculate current model sparsity\"\"\"\n",
        "    total_params = 0\n",
        "    zero_params = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            total_params += param.numel()\n",
        "            zero_params += (param == 0).sum().item()\n",
        "\n",
        "    sparsity = 100. * zero_params / total_params\n",
        "    return sparsity\n",
        "\n",
        "# Main training function\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Hyperparameters (optimized for better accuracy)\n",
        "    batch_size = 128\n",
        "    epochs = 400  # Increased epochs for better convergence\n",
        "    initial_lr = 0.1\n",
        "    momentum = 0.9\n",
        "    weight_decay = 5e-4\n",
        "    target_sparsity = 0.98\n",
        "    initial_sparsity = 0.5  # Start with lower sparsity\n",
        "    pruning_start_epoch = 10  # Warm-up before pruning\n",
        "    pruning_end_epoch = 300  # Stop pruning earlier to allow recovery\n",
        "\n",
        "    # Enhanced data augmentation\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "        transforms.RandomErasing(p=0.1, scale=(0.02, 0.33)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-10 dataset\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True,\n",
        "                                              num_workers=4, pin_memory=True, drop_last=True)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False,\n",
        "                                             num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = ImprovedDynamicSparseVGG19(num_classes=10).to(device)\n",
        "\n",
        "    # Use label smoothing loss\n",
        "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "\n",
        "    # Optimizer with better settings\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum,\n",
        "                               weight_decay=weight_decay, nesterov=True)\n",
        "\n",
        "    # Learning rate scheduler - Cosine Annealing with Warm Restarts\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-4)\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    sparsities = []\n",
        "\n",
        "    best_accuracy = 0\n",
        "    best_sparse_accuracy = 0\n",
        "\n",
        "    # Choose pruning strategy\n",
        "    use_global_pruning = True  # Using GLOBAL pruning for consistency with your other codes\n",
        "\n",
        "    print(f\"Starting improved training with linear sparsity schedule...\")\n",
        "    print(f\"Pruning method: {'Global' if use_global_pruning else 'Layer-wise'} Unstructured Pruning\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Linear sparsity schedule (as requested)\n",
        "        if epoch >= pruning_start_epoch and epoch <= pruning_end_epoch:\n",
        "            current_sparsity = linear_sparsity_schedule(\n",
        "                epoch - pruning_start_epoch,\n",
        "                pruning_end_epoch - pruning_start_epoch,\n",
        "                initial_sparsity,\n",
        "                target_sparsity\n",
        "            )\n",
        "\n",
        "            # Create and apply masks\n",
        "            if use_global_pruning:\n",
        "                # Global unstructured pruning (like your original)\n",
        "                model.masks = create_global_magnitude_masks(model, current_sparsity)\n",
        "            else:\n",
        "                # Layer-wise unstructured pruning with sensitivity\n",
        "                layer_sparsities = compute_layer_wise_sparsity(model, current_sparsity)\n",
        "                model.masks = create_magnitude_based_masks_improved(model, layer_sparsities)\n",
        "\n",
        "            model.apply_masks()\n",
        "\n",
        "            actual_sparsity = calculate_sparsity(model)\n",
        "            print(f'Epoch {epoch}: Target Sparsity: {current_sparsity*100:.2f}%, '\n",
        "                  f'Actual Sparsity: {actual_sparsity:.2f}%')\n",
        "        elif epoch > pruning_end_epoch:\n",
        "            # Fine-tuning phase - keep masks but allow remaining weights to adapt\n",
        "            model.apply_masks()\n",
        "            actual_sparsity = calculate_sparsity(model)\n",
        "            print(f'Epoch {epoch}: Fine-tuning with Sparsity: {actual_sparsity:.2f}%')\n",
        "        else:\n",
        "            actual_sparsity = 0\n",
        "            print(f'Epoch {epoch}: Warm-up phase (no pruning)')\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_improved(model, device, train_loader, optimizer,\n",
        "                                              epoch, criterion, scaler, use_amp=torch.cuda.is_available())\n",
        "\n",
        "        # Test\n",
        "        test_loss, test_acc = test_improved(model, device, test_loader, criterion)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save history\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_acc)\n",
        "        sparsities.append(actual_sparsity)\n",
        "\n",
        "        # Track best accuracy\n",
        "        if test_acc > best_accuracy:\n",
        "            best_accuracy = test_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'masks': model.masks,\n",
        "                'accuracy': test_acc,\n",
        "                'sparsity': actual_sparsity,\n",
        "            }, 'best_model.pth')\n",
        "\n",
        "        if actual_sparsity >= 97 and test_acc > best_sparse_accuracy:\n",
        "            best_sparse_accuracy = test_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'masks': model.masks,\n",
        "                'accuracy': test_acc,\n",
        "                'sparsity': actual_sparsity,\n",
        "            }, 'best_sparse_model.pth')\n",
        "\n",
        "    print(f\"\\nTraining completed!\")\n",
        "    print(f\"Best accuracy: {best_accuracy:.2f}%\")\n",
        "    print(f\"Best accuracy at >97% sparsity: {best_sparse_accuracy:.2f}%\")\n",
        "\n",
        "    # Plot results - 3 side-by-side graphs\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Left: Accuracy vs Epoch (Train and Test)\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(train_accuracies, label='Train')\n",
        "    plt.plot(test_accuracies, label='Test')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('VGG19 Training Progress (98% Sparsity)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Middle: Linear Sparsity Schedule\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(sparsities)\n",
        "    plt.axhline(y=98, color='r', linestyle='--', label='Target 98%')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Sparsity (%)')\n",
        "    plt.title('Linear Sparsity Schedule')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Right: Loss vs Epoch (Train and Test)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(train_losses, label='Train')\n",
        "    plt.plot(test_losses, label='Test')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('improved_training_results.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Download the PDF file\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download('improved_training_results.pdf')\n",
        "        print(\"✓ PDF downloaded to your computer!\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not auto-download: {e}\")\n",
        "        print(\"Run this in a new cell: files.download('improved_training_results.pdf')\")\n",
        "\n",
        "\n",
        "    return model, best_accuracy, best_sparse_accuracy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, best_acc, best_sparse_acc = main()\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Final Results:\")\n",
        "    print(f\"Best overall accuracy: {best_acc:.2f}%\")\n",
        "    print(f\"Best accuracy at >97% sparsity: {best_sparse_acc:.2f}%\")\n",
        "    print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "saagOAYqBZ_z",
        "outputId": "f62972ae-1e8b-4e76-905f-f5706ad58117"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Starting improved training with linear sparsity schedule...\n",
            "Pruning method: Global Unstructured Pruning\n",
            "Epoch 1: Warm-up phase (no pruning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2006743408.py:376: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() if torch.cuda.is_available() else None\n",
            "/tmp/ipython-input-2006743408.py:249: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.301358\tAcc: 8.59%\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.967520\tAcc: 23.61%\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.925610\tAcc: 28.95%\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.682172\tAcc: 33.03%\n",
            "\n",
            "Test set: Average loss: 1.5922, Accuracy: 5039/10000 (50.39%)\n",
            "\n",
            "Epoch 2: Warm-up phase (no pruning)\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.672514\tAcc: 47.66%\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.587883\tAcc: 50.90%\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.589500\tAcc: 52.08%\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.391782\tAcc: 53.38%\n",
            "\n",
            "Test set: Average loss: 1.4281, Accuracy: 5900/10000 (59.00%)\n",
            "\n",
            "Epoch 3: Warm-up phase (no pruning)\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.404482\tAcc: 58.59%\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.247322\tAcc: 58.60%\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.210743\tAcc: 59.99%\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.411153\tAcc: 60.82%\n",
            "\n",
            "Test set: Average loss: 1.2561, Accuracy: 6656/10000 (66.56%)\n",
            "\n",
            "Epoch 4: Warm-up phase (no pruning)\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.356942\tAcc: 63.28%\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.227843\tAcc: 64.87%\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.195812\tAcc: 64.79%\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.337857\tAcc: 65.34%\n",
            "\n",
            "Test set: Average loss: 1.1395, Accuracy: 7268/10000 (72.68%)\n",
            "\n",
            "Epoch 5: Warm-up phase (no pruning)\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.234285\tAcc: 72.66%\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.230604\tAcc: 67.94%\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.156550\tAcc: 68.23%\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.349259\tAcc: 68.39%\n",
            "\n",
            "Test set: Average loss: 1.1254, Accuracy: 7309/10000 (73.09%)\n",
            "\n",
            "Epoch 6: Warm-up phase (no pruning)\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.189219\tAcc: 71.88%\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 1.137057\tAcc: 69.62%\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 1.230625\tAcc: 69.90%\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 1.000072\tAcc: 70.11%\n",
            "\n",
            "Test set: Average loss: 1.1820, Accuracy: 7094/10000 (70.94%)\n",
            "\n",
            "Epoch 7: Warm-up phase (no pruning)\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.304060\tAcc: 66.41%\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 1.071029\tAcc: 71.21%\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.291962\tAcc: 71.59%\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 1.083351\tAcc: 71.88%\n",
            "\n",
            "Test set: Average loss: 1.1284, Accuracy: 7357/10000 (73.57%)\n",
            "\n",
            "Epoch 8: Warm-up phase (no pruning)\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.232999\tAcc: 67.97%\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 1.181470\tAcc: 72.95%\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 1.100551\tAcc: 72.83%\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 1.083426\tAcc: 73.02%\n",
            "\n",
            "Test set: Average loss: 1.0447, Accuracy: 7664/10000 (76.64%)\n",
            "\n",
            "Epoch 9: Warm-up phase (no pruning)\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.971118\tAcc: 78.91%\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.949638\tAcc: 73.33%\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.997038\tAcc: 73.86%\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 1.031255\tAcc: 73.75%\n",
            "\n",
            "Test set: Average loss: 1.0282, Accuracy: 7704/10000 (77.04%)\n",
            "\n",
            "Epoch 10: Target Sparsity: 50.00%, Actual Sparsity: 49.98%\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.153929\tAcc: 70.31%\n",
            "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.983120\tAcc: 75.84%\n",
            "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 1.039248\tAcc: 75.72%\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 1.089224\tAcc: 75.67%\n",
            "\n",
            "Test set: Average loss: 1.0940, Accuracy: 7392/10000 (73.92%)\n",
            "\n",
            "Epoch 11: Target Sparsity: 50.17%, Actual Sparsity: 50.15%\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.969580\tAcc: 78.12%\n",
            "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 1.190224\tAcc: 76.47%\n",
            "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 1.023562\tAcc: 76.28%\n",
            "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 1.063427\tAcc: 76.35%\n",
            "\n",
            "Test set: Average loss: 1.0358, Accuracy: 7728/10000 (77.28%)\n",
            "\n",
            "Epoch 12: Target Sparsity: 50.33%, Actual Sparsity: 50.32%\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.141637\tAcc: 72.66%\n",
            "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 0.921405\tAcc: 77.24%\n",
            "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.940708\tAcc: 77.24%\n",
            "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 1.140206\tAcc: 77.07%\n",
            "\n",
            "Test set: Average loss: 1.0099, Accuracy: 7822/10000 (78.22%)\n",
            "\n",
            "Epoch 13: Target Sparsity: 50.50%, Actual Sparsity: 50.48%\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.071390\tAcc: 75.78%\n",
            "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 1.054338\tAcc: 77.02%\n",
            "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.933564\tAcc: 77.37%\n",
            "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 1.031741\tAcc: 77.43%\n",
            "\n",
            "Test set: Average loss: 0.9888, Accuracy: 7940/10000 (79.40%)\n",
            "\n",
            "Epoch 14: Target Sparsity: 50.66%, Actual Sparsity: 50.65%\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.918425\tAcc: 86.72%\n",
            "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 1.010331\tAcc: 77.82%\n",
            "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 1.000401\tAcc: 78.25%\n",
            "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 0.994798\tAcc: 78.08%\n",
            "\n",
            "Test set: Average loss: 0.9537, Accuracy: 8076/10000 (80.76%)\n",
            "\n",
            "Epoch 15: Target Sparsity: 50.83%, Actual Sparsity: 50.81%\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.001491\tAcc: 75.78%\n",
            "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 0.845674\tAcc: 77.75%\n",
            "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 1.057549\tAcc: 77.95%\n",
            "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 0.910076\tAcc: 77.98%\n",
            "\n",
            "Test set: Average loss: 0.9828, Accuracy: 7949/10000 (79.49%)\n",
            "\n",
            "Epoch 16: Target Sparsity: 50.99%, Actual Sparsity: 50.98%\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.062996\tAcc: 77.34%\n",
            "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 1.038536\tAcc: 79.02%\n",
            "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.948743\tAcc: 78.62%\n",
            "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 0.908823\tAcc: 78.94%\n",
            "\n",
            "Test set: Average loss: 0.9422, Accuracy: 8058/10000 (80.58%)\n",
            "\n",
            "Epoch 17: Target Sparsity: 51.16%, Actual Sparsity: 51.14%\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.064587\tAcc: 75.78%\n",
            "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 0.920085\tAcc: 79.41%\n",
            "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.834647\tAcc: 79.13%\n",
            "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 0.966117\tAcc: 79.11%\n",
            "\n",
            "Test set: Average loss: 0.8883, Accuracy: 8321/10000 (83.21%)\n",
            "\n",
            "Epoch 18: Target Sparsity: 51.32%, Actual Sparsity: 51.31%\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.017380\tAcc: 77.34%\n",
            "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 0.886363\tAcc: 79.60%\n",
            "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.985356\tAcc: 79.45%\n",
            "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 1.036969\tAcc: 79.39%\n",
            "\n",
            "Test set: Average loss: 0.9443, Accuracy: 8075/10000 (80.75%)\n",
            "\n",
            "Epoch 19: Target Sparsity: 51.49%, Actual Sparsity: 51.47%\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.985088\tAcc: 79.69%\n",
            "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 0.950123\tAcc: 79.65%\n",
            "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.899397\tAcc: 79.40%\n",
            "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 0.950486\tAcc: 79.46%\n",
            "\n",
            "Test set: Average loss: 0.9914, Accuracy: 7956/10000 (79.56%)\n",
            "\n",
            "Epoch 20: Target Sparsity: 51.66%, Actual Sparsity: 51.64%\n",
            "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.804407\tAcc: 85.94%\n",
            "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 1.102848\tAcc: 80.38%\n",
            "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.978786\tAcc: 79.92%\n",
            "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 0.970559\tAcc: 79.95%\n",
            "\n",
            "Test set: Average loss: 1.1803, Accuracy: 7027/10000 (70.27%)\n",
            "\n",
            "Epoch 21: Target Sparsity: 51.82%, Actual Sparsity: 51.80%\n",
            "Train Epoch: 21 [0/50000 (0%)]\tLoss: 1.041660\tAcc: 75.78%\n",
            "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 0.864130\tAcc: 80.52%\n",
            "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.959505\tAcc: 80.50%\n",
            "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 0.947293\tAcc: 80.41%\n",
            "\n",
            "Test set: Average loss: 0.8668, Accuracy: 8431/10000 (84.31%)\n",
            "\n",
            "Epoch 22: Target Sparsity: 51.99%, Actual Sparsity: 51.97%\n",
            "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.870361\tAcc: 80.47%\n",
            "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 0.979587\tAcc: 81.00%\n",
            "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.892781\tAcc: 81.09%\n",
            "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 0.927061\tAcc: 80.86%\n",
            "\n",
            "Test set: Average loss: 0.8520, Accuracy: 8475/10000 (84.75%)\n",
            "\n",
            "Epoch 23: Target Sparsity: 52.15%, Actual Sparsity: 52.14%\n",
            "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.990428\tAcc: 78.12%\n",
            "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 0.946832\tAcc: 81.52%\n",
            "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.999577\tAcc: 81.34%\n",
            "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 0.845223\tAcc: 81.53%\n",
            "\n",
            "Test set: Average loss: 0.9092, Accuracy: 8264/10000 (82.64%)\n",
            "\n",
            "Epoch 24: Target Sparsity: 52.32%, Actual Sparsity: 52.30%\n",
            "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.876410\tAcc: 83.59%\n",
            "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 1.000314\tAcc: 81.95%\n",
            "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.888088\tAcc: 81.90%\n",
            "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 0.879869\tAcc: 81.65%\n",
            "\n",
            "Test set: Average loss: 0.8863, Accuracy: 8366/10000 (83.66%)\n",
            "\n",
            "Epoch 25: Target Sparsity: 52.48%, Actual Sparsity: 52.47%\n",
            "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.984939\tAcc: 80.47%\n",
            "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 0.937554\tAcc: 83.04%\n",
            "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 1.044442\tAcc: 82.44%\n",
            "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 1.048505\tAcc: 82.45%\n",
            "\n",
            "Test set: Average loss: 0.8326, Accuracy: 8553/10000 (85.53%)\n",
            "\n",
            "Epoch 26: Target Sparsity: 52.65%, Actual Sparsity: 52.63%\n",
            "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.900538\tAcc: 78.91%\n",
            "Train Epoch: 26 [12800/50000 (26%)]\tLoss: 0.883135\tAcc: 82.88%\n",
            "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.981779\tAcc: 83.08%\n",
            "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 0.951781\tAcc: 82.94%\n",
            "\n",
            "Test set: Average loss: 0.8279, Accuracy: 8547/10000 (85.47%)\n",
            "\n",
            "Epoch 27: Target Sparsity: 52.81%, Actual Sparsity: 52.80%\n",
            "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.885077\tAcc: 85.16%\n",
            "Train Epoch: 27 [12800/50000 (26%)]\tLoss: 0.988124\tAcc: 83.95%\n",
            "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.861489\tAcc: 83.54%\n",
            "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 0.933017\tAcc: 83.68%\n",
            "\n",
            "Test set: Average loss: 0.8455, Accuracy: 8498/10000 (84.98%)\n",
            "\n",
            "Epoch 28: Target Sparsity: 52.98%, Actual Sparsity: 52.96%\n",
            "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.869013\tAcc: 83.59%\n",
            "Train Epoch: 28 [12800/50000 (26%)]\tLoss: 0.885479\tAcc: 84.37%\n",
            "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.897743\tAcc: 84.21%\n",
            "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 0.894884\tAcc: 84.09%\n",
            "\n",
            "Test set: Average loss: 0.8660, Accuracy: 8412/10000 (84.12%)\n",
            "\n",
            "Epoch 29: Target Sparsity: 53.14%, Actual Sparsity: 53.13%\n",
            "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.848915\tAcc: 86.72%\n",
            "Train Epoch: 29 [12800/50000 (26%)]\tLoss: 0.857346\tAcc: 84.29%\n",
            "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.862395\tAcc: 84.41%\n",
            "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 0.895197\tAcc: 84.27%\n",
            "\n",
            "Test set: Average loss: 0.7747, Accuracy: 8805/10000 (88.05%)\n",
            "\n",
            "Epoch 30: Target Sparsity: 53.31%, Actual Sparsity: 53.29%\n",
            "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.815482\tAcc: 85.16%\n",
            "Train Epoch: 30 [12800/50000 (26%)]\tLoss: 0.865757\tAcc: 85.33%\n",
            "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.902438\tAcc: 85.35%\n",
            "Train Epoch: 30 [38400/50000 (77%)]\tLoss: 0.814078\tAcc: 85.26%\n",
            "\n",
            "Test set: Average loss: 0.8106, Accuracy: 8642/10000 (86.42%)\n",
            "\n",
            "Epoch 31: Target Sparsity: 53.48%, Actual Sparsity: 53.46%\n",
            "Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.831305\tAcc: 84.38%\n",
            "Train Epoch: 31 [12800/50000 (26%)]\tLoss: 0.886373\tAcc: 85.76%\n",
            "Train Epoch: 31 [25600/50000 (51%)]\tLoss: 0.826262\tAcc: 85.74%\n",
            "Train Epoch: 31 [38400/50000 (77%)]\tLoss: 0.827110\tAcc: 85.50%\n",
            "\n",
            "Test set: Average loss: 0.7947, Accuracy: 8741/10000 (87.41%)\n",
            "\n",
            "Epoch 32: Target Sparsity: 53.64%, Actual Sparsity: 53.62%\n",
            "Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.840188\tAcc: 87.50%\n",
            "Train Epoch: 32 [12800/50000 (26%)]\tLoss: 0.807508\tAcc: 86.19%\n",
            "Train Epoch: 32 [25600/50000 (51%)]\tLoss: 0.875863\tAcc: 86.15%\n",
            "Train Epoch: 32 [38400/50000 (77%)]\tLoss: 0.819518\tAcc: 86.24%\n",
            "\n",
            "Test set: Average loss: 0.7826, Accuracy: 8760/10000 (87.60%)\n",
            "\n",
            "Epoch 33: Target Sparsity: 53.81%, Actual Sparsity: 53.79%\n",
            "Train Epoch: 33 [0/50000 (0%)]\tLoss: 0.797279\tAcc: 89.84%\n",
            "Train Epoch: 33 [12800/50000 (26%)]\tLoss: 0.800407\tAcc: 86.59%\n",
            "Train Epoch: 33 [25600/50000 (51%)]\tLoss: 0.793364\tAcc: 86.82%\n",
            "Train Epoch: 33 [38400/50000 (77%)]\tLoss: 0.851266\tAcc: 86.95%\n",
            "\n",
            "Test set: Average loss: 0.7639, Accuracy: 8866/10000 (88.66%)\n",
            "\n",
            "Epoch 34: Target Sparsity: 53.97%, Actual Sparsity: 53.96%\n",
            "Train Epoch: 34 [0/50000 (0%)]\tLoss: 0.866458\tAcc: 85.94%\n",
            "Train Epoch: 34 [12800/50000 (26%)]\tLoss: 0.883901\tAcc: 87.02%\n",
            "Train Epoch: 34 [25600/50000 (51%)]\tLoss: 0.833436\tAcc: 87.23%\n",
            "Train Epoch: 34 [38400/50000 (77%)]\tLoss: 0.814571\tAcc: 87.39%\n",
            "\n",
            "Test set: Average loss: 0.7523, Accuracy: 8910/10000 (89.10%)\n",
            "\n",
            "Epoch 35: Target Sparsity: 54.14%, Actual Sparsity: 54.12%\n",
            "Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.807453\tAcc: 87.50%\n",
            "Train Epoch: 35 [12800/50000 (26%)]\tLoss: 0.780939\tAcc: 88.14%\n",
            "Train Epoch: 35 [25600/50000 (51%)]\tLoss: 0.664668\tAcc: 88.19%\n",
            "Train Epoch: 35 [38400/50000 (77%)]\tLoss: 0.803742\tAcc: 88.20%\n",
            "\n",
            "Test set: Average loss: 0.7357, Accuracy: 8957/10000 (89.57%)\n",
            "\n",
            "Epoch 36: Target Sparsity: 54.30%, Actual Sparsity: 54.29%\n",
            "Train Epoch: 36 [0/50000 (0%)]\tLoss: 0.869915\tAcc: 87.50%\n",
            "Train Epoch: 36 [12800/50000 (26%)]\tLoss: 0.755038\tAcc: 88.55%\n",
            "Train Epoch: 36 [25600/50000 (51%)]\tLoss: 0.721525\tAcc: 88.89%\n",
            "Train Epoch: 36 [38400/50000 (77%)]\tLoss: 0.888151\tAcc: 88.91%\n",
            "\n",
            "Test set: Average loss: 0.7432, Accuracy: 8950/10000 (89.50%)\n",
            "\n",
            "Epoch 37: Target Sparsity: 54.47%, Actual Sparsity: 54.45%\n",
            "Train Epoch: 37 [0/50000 (0%)]\tLoss: 0.698643\tAcc: 93.75%\n",
            "Train Epoch: 37 [12800/50000 (26%)]\tLoss: 0.755318\tAcc: 89.43%\n",
            "Train Epoch: 37 [25600/50000 (51%)]\tLoss: 0.612740\tAcc: 89.43%\n",
            "Train Epoch: 37 [38400/50000 (77%)]\tLoss: 0.724390\tAcc: 89.36%\n",
            "\n",
            "Test set: Average loss: 0.7327, Accuracy: 8960/10000 (89.60%)\n",
            "\n",
            "Epoch 38: Target Sparsity: 54.63%, Actual Sparsity: 54.62%\n",
            "Train Epoch: 38 [0/50000 (0%)]\tLoss: 0.679063\tAcc: 91.41%\n",
            "Train Epoch: 38 [12800/50000 (26%)]\tLoss: 0.746591\tAcc: 89.93%\n",
            "Train Epoch: 38 [25600/50000 (51%)]\tLoss: 0.776447\tAcc: 89.74%\n",
            "Train Epoch: 38 [38400/50000 (77%)]\tLoss: 0.651428\tAcc: 89.77%\n",
            "\n",
            "Test set: Average loss: 0.7154, Accuracy: 9061/10000 (90.61%)\n",
            "\n",
            "Epoch 39: Target Sparsity: 54.80%, Actual Sparsity: 54.78%\n",
            "Train Epoch: 39 [0/50000 (0%)]\tLoss: 0.695824\tAcc: 89.06%\n",
            "Train Epoch: 39 [12800/50000 (26%)]\tLoss: 0.764684\tAcc: 90.74%\n",
            "Train Epoch: 39 [25600/50000 (51%)]\tLoss: 0.726951\tAcc: 90.70%\n",
            "Train Epoch: 39 [38400/50000 (77%)]\tLoss: 0.774737\tAcc: 90.56%\n",
            "\n",
            "Test set: Average loss: 0.6974, Accuracy: 9140/10000 (91.40%)\n",
            "\n",
            "Epoch 40: Target Sparsity: 54.97%, Actual Sparsity: 54.95%\n",
            "Train Epoch: 40 [0/50000 (0%)]\tLoss: 0.635061\tAcc: 92.97%\n",
            "Train Epoch: 40 [12800/50000 (26%)]\tLoss: 0.772539\tAcc: 91.31%\n",
            "Train Epoch: 40 [25600/50000 (51%)]\tLoss: 0.702399\tAcc: 91.20%\n",
            "Train Epoch: 40 [38400/50000 (77%)]\tLoss: 0.826535\tAcc: 91.25%\n",
            "\n",
            "Test set: Average loss: 0.6982, Accuracy: 9144/10000 (91.44%)\n",
            "\n",
            "Epoch 41: Target Sparsity: 55.13%, Actual Sparsity: 55.11%\n",
            "Train Epoch: 41 [0/50000 (0%)]\tLoss: 0.734347\tAcc: 91.41%\n",
            "Train Epoch: 41 [12800/50000 (26%)]\tLoss: 0.881564\tAcc: 91.89%\n",
            "Train Epoch: 41 [25600/50000 (51%)]\tLoss: 0.784584\tAcc: 91.68%\n",
            "Train Epoch: 41 [38400/50000 (77%)]\tLoss: 0.726747\tAcc: 91.70%\n",
            "\n",
            "Test set: Average loss: 0.6966, Accuracy: 9151/10000 (91.51%)\n",
            "\n",
            "Epoch 42: Target Sparsity: 55.30%, Actual Sparsity: 55.28%\n",
            "Train Epoch: 42 [0/50000 (0%)]\tLoss: 0.660971\tAcc: 93.75%\n",
            "Train Epoch: 42 [12800/50000 (26%)]\tLoss: 0.800460\tAcc: 92.56%\n",
            "Train Epoch: 42 [25600/50000 (51%)]\tLoss: 0.617647\tAcc: 92.41%\n",
            "Train Epoch: 42 [38400/50000 (77%)]\tLoss: 0.702937\tAcc: 92.34%\n",
            "\n",
            "Test set: Average loss: 0.6801, Accuracy: 9225/10000 (92.25%)\n",
            "\n",
            "Epoch 43: Target Sparsity: 55.46%, Actual Sparsity: 55.44%\n",
            "Train Epoch: 43 [0/50000 (0%)]\tLoss: 0.775021\tAcc: 90.62%\n",
            "Train Epoch: 43 [12800/50000 (26%)]\tLoss: 0.688686\tAcc: 92.81%\n",
            "Train Epoch: 43 [25600/50000 (51%)]\tLoss: 0.601865\tAcc: 93.05%\n",
            "Train Epoch: 43 [38400/50000 (77%)]\tLoss: 0.605463\tAcc: 92.95%\n",
            "\n",
            "Test set: Average loss: 0.6867, Accuracy: 9194/10000 (91.94%)\n",
            "\n",
            "Epoch 44: Target Sparsity: 55.63%, Actual Sparsity: 55.61%\n",
            "Train Epoch: 44 [0/50000 (0%)]\tLoss: 0.639193\tAcc: 94.53%\n",
            "Train Epoch: 44 [12800/50000 (26%)]\tLoss: 0.695697\tAcc: 93.31%\n",
            "Train Epoch: 44 [25600/50000 (51%)]\tLoss: 0.671774\tAcc: 93.28%\n",
            "Train Epoch: 44 [38400/50000 (77%)]\tLoss: 0.646471\tAcc: 93.33%\n",
            "\n",
            "Test set: Average loss: 0.6713, Accuracy: 9277/10000 (92.77%)\n",
            "\n",
            "Epoch 45: Target Sparsity: 55.79%, Actual Sparsity: 55.78%\n",
            "Train Epoch: 45 [0/50000 (0%)]\tLoss: 0.624327\tAcc: 96.88%\n",
            "Train Epoch: 45 [12800/50000 (26%)]\tLoss: 0.628593\tAcc: 93.91%\n",
            "Train Epoch: 45 [25600/50000 (51%)]\tLoss: 0.642256\tAcc: 93.86%\n",
            "Train Epoch: 45 [38400/50000 (77%)]\tLoss: 0.721959\tAcc: 93.86%\n",
            "\n",
            "Test set: Average loss: 0.6718, Accuracy: 9261/10000 (92.61%)\n",
            "\n",
            "Epoch 46: Target Sparsity: 55.96%, Actual Sparsity: 55.94%\n",
            "Train Epoch: 46 [0/50000 (0%)]\tLoss: 0.656379\tAcc: 92.19%\n",
            "Train Epoch: 46 [12800/50000 (26%)]\tLoss: 0.623104\tAcc: 94.17%\n",
            "Train Epoch: 46 [25600/50000 (51%)]\tLoss: 0.672640\tAcc: 94.19%\n",
            "Train Epoch: 46 [38400/50000 (77%)]\tLoss: 0.589243\tAcc: 94.08%\n",
            "\n",
            "Test set: Average loss: 0.6663, Accuracy: 9274/10000 (92.74%)\n",
            "\n",
            "Epoch 47: Target Sparsity: 56.12%, Actual Sparsity: 56.11%\n",
            "Train Epoch: 47 [0/50000 (0%)]\tLoss: 0.610151\tAcc: 95.31%\n",
            "Train Epoch: 47 [12800/50000 (26%)]\tLoss: 0.623113\tAcc: 94.40%\n",
            "Train Epoch: 47 [25600/50000 (51%)]\tLoss: 0.608515\tAcc: 94.27%\n",
            "Train Epoch: 47 [38400/50000 (77%)]\tLoss: 0.642487\tAcc: 94.26%\n",
            "\n",
            "Test set: Average loss: 0.6671, Accuracy: 9297/10000 (92.97%)\n",
            "\n",
            "Epoch 48: Target Sparsity: 56.29%, Actual Sparsity: 56.27%\n",
            "Train Epoch: 48 [0/50000 (0%)]\tLoss: 0.637134\tAcc: 94.53%\n",
            "Train Epoch: 48 [12800/50000 (26%)]\tLoss: 0.568395\tAcc: 94.46%\n",
            "Train Epoch: 48 [25600/50000 (51%)]\tLoss: 0.668356\tAcc: 94.44%\n",
            "Train Epoch: 48 [38400/50000 (77%)]\tLoss: 0.623259\tAcc: 94.50%\n",
            "\n",
            "Test set: Average loss: 0.6651, Accuracy: 9325/10000 (93.25%)\n",
            "\n",
            "Epoch 49: Target Sparsity: 56.46%, Actual Sparsity: 56.44%\n",
            "Train Epoch: 49 [0/50000 (0%)]\tLoss: 0.590026\tAcc: 95.31%\n",
            "Train Epoch: 49 [12800/50000 (26%)]\tLoss: 0.598609\tAcc: 94.69%\n",
            "Train Epoch: 49 [25600/50000 (51%)]\tLoss: 0.553558\tAcc: 94.61%\n",
            "Train Epoch: 49 [38400/50000 (77%)]\tLoss: 0.611436\tAcc: 94.51%\n",
            "\n",
            "Test set: Average loss: 0.6629, Accuracy: 9318/10000 (93.18%)\n",
            "\n",
            "Epoch 50: Target Sparsity: 56.62%, Actual Sparsity: 56.60%\n",
            "Train Epoch: 50 [0/50000 (0%)]\tLoss: 0.641216\tAcc: 93.75%\n",
            "Train Epoch: 50 [12800/50000 (26%)]\tLoss: 0.628907\tAcc: 94.88%\n",
            "Train Epoch: 50 [25600/50000 (51%)]\tLoss: 0.667491\tAcc: 94.96%\n",
            "Train Epoch: 50 [38400/50000 (77%)]\tLoss: 0.571371\tAcc: 94.89%\n",
            "\n",
            "Test set: Average loss: 0.6653, Accuracy: 9300/10000 (93.00%)\n",
            "\n",
            "Epoch 51: Target Sparsity: 56.79%, Actual Sparsity: 56.77%\n",
            "Train Epoch: 51 [0/50000 (0%)]\tLoss: 0.610670\tAcc: 94.53%\n",
            "Train Epoch: 51 [12800/50000 (26%)]\tLoss: 0.666134\tAcc: 94.72%\n",
            "Train Epoch: 51 [25600/50000 (51%)]\tLoss: 0.647152\tAcc: 94.79%\n",
            "Train Epoch: 51 [38400/50000 (77%)]\tLoss: 0.626327\tAcc: 94.79%\n",
            "\n",
            "Test set: Average loss: 0.6642, Accuracy: 9315/10000 (93.15%)\n",
            "\n",
            "Epoch 52: Target Sparsity: 56.95%, Actual Sparsity: 56.93%\n",
            "Train Epoch: 52 [0/50000 (0%)]\tLoss: 0.626496\tAcc: 96.09%\n",
            "Train Epoch: 52 [12800/50000 (26%)]\tLoss: 0.658764\tAcc: 94.66%\n",
            "Train Epoch: 52 [25600/50000 (51%)]\tLoss: 0.594572\tAcc: 94.75%\n",
            "Train Epoch: 52 [38400/50000 (77%)]\tLoss: 0.556320\tAcc: 94.70%\n",
            "\n",
            "Test set: Average loss: 0.6632, Accuracy: 9315/10000 (93.15%)\n",
            "\n",
            "Epoch 53: Target Sparsity: 57.12%, Actual Sparsity: 57.10%\n",
            "Train Epoch: 53 [0/50000 (0%)]\tLoss: 0.626945\tAcc: 95.31%\n",
            "Train Epoch: 53 [12800/50000 (26%)]\tLoss: 0.577534\tAcc: 94.86%\n",
            "Train Epoch: 53 [25600/50000 (51%)]\tLoss: 0.584371\tAcc: 94.79%\n",
            "Train Epoch: 53 [38400/50000 (77%)]\tLoss: 0.693676\tAcc: 94.79%\n",
            "\n",
            "Test set: Average loss: 0.6626, Accuracy: 9321/10000 (93.21%)\n",
            "\n",
            "Epoch 54: Target Sparsity: 57.28%, Actual Sparsity: 57.26%\n",
            "Train Epoch: 54 [0/50000 (0%)]\tLoss: 0.636488\tAcc: 95.31%\n",
            "Train Epoch: 54 [12800/50000 (26%)]\tLoss: 0.632729\tAcc: 94.43%\n",
            "Train Epoch: 54 [25600/50000 (51%)]\tLoss: 0.597492\tAcc: 94.63%\n",
            "Train Epoch: 54 [38400/50000 (77%)]\tLoss: 0.677766\tAcc: 94.60%\n",
            "\n",
            "Test set: Average loss: 0.6655, Accuracy: 9312/10000 (93.12%)\n",
            "\n",
            "Epoch 55: Target Sparsity: 57.45%, Actual Sparsity: 57.43%\n",
            "Train Epoch: 55 [0/50000 (0%)]\tLoss: 0.570154\tAcc: 97.66%\n",
            "Train Epoch: 55 [12800/50000 (26%)]\tLoss: 0.639429\tAcc: 94.59%\n",
            "Train Epoch: 55 [25600/50000 (51%)]\tLoss: 0.590548\tAcc: 94.80%\n",
            "Train Epoch: 55 [38400/50000 (77%)]\tLoss: 0.667295\tAcc: 94.68%\n",
            "\n",
            "Test set: Average loss: 0.6641, Accuracy: 9317/10000 (93.17%)\n",
            "\n",
            "Epoch 56: Target Sparsity: 57.61%, Actual Sparsity: 57.60%\n",
            "Train Epoch: 56 [0/50000 (0%)]\tLoss: 0.578688\tAcc: 96.88%\n",
            "Train Epoch: 56 [12800/50000 (26%)]\tLoss: 0.557189\tAcc: 95.03%\n",
            "Train Epoch: 56 [25600/50000 (51%)]\tLoss: 0.563283\tAcc: 94.88%\n",
            "Train Epoch: 56 [38400/50000 (77%)]\tLoss: 0.592484\tAcc: 94.81%\n",
            "\n",
            "Test set: Average loss: 0.6678, Accuracy: 9307/10000 (93.07%)\n",
            "\n",
            "Epoch 57: Target Sparsity: 57.78%, Actual Sparsity: 57.76%\n",
            "Train Epoch: 57 [0/50000 (0%)]\tLoss: 0.638355\tAcc: 92.19%\n",
            "Train Epoch: 57 [12800/50000 (26%)]\tLoss: 0.659463\tAcc: 94.67%\n",
            "Train Epoch: 57 [25600/50000 (51%)]\tLoss: 0.701560\tAcc: 94.64%\n",
            "Train Epoch: 57 [38400/50000 (77%)]\tLoss: 0.637127\tAcc: 94.48%\n",
            "\n",
            "Test set: Average loss: 0.6703, Accuracy: 9277/10000 (92.77%)\n",
            "\n",
            "Epoch 58: Target Sparsity: 57.94%, Actual Sparsity: 57.93%\n",
            "Train Epoch: 58 [0/50000 (0%)]\tLoss: 0.649068\tAcc: 94.53%\n",
            "Train Epoch: 58 [12800/50000 (26%)]\tLoss: 0.658673\tAcc: 94.59%\n",
            "Train Epoch: 58 [25600/50000 (51%)]\tLoss: 0.691974\tAcc: 94.35%\n",
            "Train Epoch: 58 [38400/50000 (77%)]\tLoss: 0.563875\tAcc: 94.28%\n",
            "\n",
            "Test set: Average loss: 0.6678, Accuracy: 9292/10000 (92.92%)\n",
            "\n",
            "Epoch 59: Target Sparsity: 58.11%, Actual Sparsity: 58.09%\n",
            "Train Epoch: 59 [0/50000 (0%)]\tLoss: 0.598279\tAcc: 96.88%\n",
            "Train Epoch: 59 [12800/50000 (26%)]\tLoss: 0.623132\tAcc: 93.99%\n",
            "Train Epoch: 59 [25600/50000 (51%)]\tLoss: 0.623665\tAcc: 94.07%\n",
            "Train Epoch: 59 [38400/50000 (77%)]\tLoss: 0.668415\tAcc: 93.94%\n",
            "\n",
            "Test set: Average loss: 0.6800, Accuracy: 9242/10000 (92.42%)\n",
            "\n",
            "Epoch 60: Target Sparsity: 58.28%, Actual Sparsity: 58.26%\n",
            "Train Epoch: 60 [0/50000 (0%)]\tLoss: 0.610579\tAcc: 93.75%\n",
            "Train Epoch: 60 [12800/50000 (26%)]\tLoss: 0.614705\tAcc: 93.80%\n",
            "Train Epoch: 60 [25600/50000 (51%)]\tLoss: 0.654674\tAcc: 93.69%\n",
            "Train Epoch: 60 [38400/50000 (77%)]\tLoss: 0.656716\tAcc: 93.66%\n",
            "\n",
            "Test set: Average loss: 0.6890, Accuracy: 9212/10000 (92.12%)\n",
            "\n",
            "Epoch 61: Target Sparsity: 58.44%, Actual Sparsity: 58.42%\n",
            "Train Epoch: 61 [0/50000 (0%)]\tLoss: 0.633995\tAcc: 94.53%\n",
            "Train Epoch: 61 [12800/50000 (26%)]\tLoss: 0.632139\tAcc: 93.34%\n",
            "Train Epoch: 61 [25600/50000 (51%)]\tLoss: 0.628946\tAcc: 93.50%\n",
            "Train Epoch: 61 [38400/50000 (77%)]\tLoss: 0.666944\tAcc: 93.33%\n",
            "\n",
            "Test set: Average loss: 0.6889, Accuracy: 9219/10000 (92.19%)\n",
            "\n",
            "Epoch 62: Target Sparsity: 58.61%, Actual Sparsity: 58.59%\n",
            "Train Epoch: 62 [0/50000 (0%)]\tLoss: 0.648215\tAcc: 93.75%\n",
            "Train Epoch: 62 [12800/50000 (26%)]\tLoss: 0.651103\tAcc: 93.28%\n",
            "Train Epoch: 62 [25600/50000 (51%)]\tLoss: 0.676504\tAcc: 92.88%\n",
            "Train Epoch: 62 [38400/50000 (77%)]\tLoss: 0.639903\tAcc: 92.77%\n",
            "\n",
            "Test set: Average loss: 0.6946, Accuracy: 9166/10000 (91.66%)\n",
            "\n",
            "Epoch 63: Target Sparsity: 58.77%, Actual Sparsity: 58.75%\n",
            "Train Epoch: 63 [0/50000 (0%)]\tLoss: 0.607053\tAcc: 96.09%\n",
            "Train Epoch: 63 [12800/50000 (26%)]\tLoss: 0.683383\tAcc: 92.47%\n",
            "Train Epoch: 63 [25600/50000 (51%)]\tLoss: 0.659943\tAcc: 92.19%\n",
            "Train Epoch: 63 [38400/50000 (77%)]\tLoss: 0.685773\tAcc: 92.16%\n",
            "\n",
            "Test set: Average loss: 0.7206, Accuracy: 9094/10000 (90.94%)\n",
            "\n",
            "Epoch 64: Target Sparsity: 58.94%, Actual Sparsity: 58.92%\n",
            "Train Epoch: 64 [0/50000 (0%)]\tLoss: 0.693591\tAcc: 89.06%\n",
            "Train Epoch: 64 [12800/50000 (26%)]\tLoss: 0.643424\tAcc: 91.48%\n",
            "Train Epoch: 64 [25600/50000 (51%)]\tLoss: 0.707070\tAcc: 91.31%\n",
            "Train Epoch: 64 [38400/50000 (77%)]\tLoss: 0.663643\tAcc: 91.42%\n",
            "\n",
            "Test set: Average loss: 0.7184, Accuracy: 9065/10000 (90.65%)\n",
            "\n",
            "Epoch 65: Target Sparsity: 59.10%, Actual Sparsity: 59.08%\n",
            "Train Epoch: 65 [0/50000 (0%)]\tLoss: 0.644699\tAcc: 93.75%\n",
            "Train Epoch: 65 [12800/50000 (26%)]\tLoss: 0.763048\tAcc: 91.24%\n",
            "Train Epoch: 65 [25600/50000 (51%)]\tLoss: 0.780479\tAcc: 90.84%\n",
            "Train Epoch: 65 [38400/50000 (77%)]\tLoss: 0.716149\tAcc: 90.80%\n",
            "\n",
            "Test set: Average loss: 0.7678, Accuracy: 8901/10000 (89.01%)\n",
            "\n",
            "Epoch 66: Target Sparsity: 59.27%, Actual Sparsity: 59.25%\n",
            "Train Epoch: 66 [0/50000 (0%)]\tLoss: 0.765015\tAcc: 89.06%\n",
            "Train Epoch: 66 [12800/50000 (26%)]\tLoss: 0.802730\tAcc: 90.65%\n",
            "Train Epoch: 66 [25600/50000 (51%)]\tLoss: 0.656699\tAcc: 90.26%\n",
            "Train Epoch: 66 [38400/50000 (77%)]\tLoss: 0.702272\tAcc: 90.26%\n",
            "\n",
            "Test set: Average loss: 0.7569, Accuracy: 8921/10000 (89.21%)\n",
            "\n",
            "Epoch 67: Target Sparsity: 59.43%, Actual Sparsity: 59.42%\n",
            "Train Epoch: 67 [0/50000 (0%)]\tLoss: 0.806995\tAcc: 86.72%\n",
            "Train Epoch: 67 [12800/50000 (26%)]\tLoss: 0.759722\tAcc: 89.54%\n",
            "Train Epoch: 67 [25600/50000 (51%)]\tLoss: 0.763101\tAcc: 89.53%\n",
            "Train Epoch: 67 [38400/50000 (77%)]\tLoss: 0.884368\tAcc: 89.30%\n",
            "\n",
            "Test set: Average loss: 0.7568, Accuracy: 8912/10000 (89.12%)\n",
            "\n",
            "Epoch 68: Target Sparsity: 59.60%, Actual Sparsity: 59.58%\n",
            "Train Epoch: 68 [0/50000 (0%)]\tLoss: 0.752202\tAcc: 91.41%\n",
            "Train Epoch: 68 [12800/50000 (26%)]\tLoss: 0.798078\tAcc: 89.09%\n",
            "Train Epoch: 68 [25600/50000 (51%)]\tLoss: 0.838837\tAcc: 88.54%\n",
            "Train Epoch: 68 [38400/50000 (77%)]\tLoss: 0.734600\tAcc: 88.48%\n",
            "\n",
            "Test set: Average loss: 0.8246, Accuracy: 8638/10000 (86.38%)\n",
            "\n",
            "Epoch 69: Target Sparsity: 59.77%, Actual Sparsity: 59.75%\n",
            "Train Epoch: 69 [0/50000 (0%)]\tLoss: 0.825638\tAcc: 84.38%\n",
            "Train Epoch: 69 [12800/50000 (26%)]\tLoss: 0.731338\tAcc: 88.03%\n",
            "Train Epoch: 69 [25600/50000 (51%)]\tLoss: 0.822294\tAcc: 87.66%\n",
            "Train Epoch: 69 [38400/50000 (77%)]\tLoss: 0.820345\tAcc: 87.60%\n",
            "\n",
            "Test set: Average loss: 0.8136, Accuracy: 8670/10000 (86.70%)\n",
            "\n",
            "Epoch 70: Target Sparsity: 59.93%, Actual Sparsity: 59.91%\n",
            "Train Epoch: 70 [0/50000 (0%)]\tLoss: 0.826169\tAcc: 85.16%\n",
            "Train Epoch: 70 [12800/50000 (26%)]\tLoss: 0.668980\tAcc: 86.94%\n",
            "Train Epoch: 70 [25600/50000 (51%)]\tLoss: 0.852296\tAcc: 86.87%\n",
            "Train Epoch: 70 [38400/50000 (77%)]\tLoss: 0.782629\tAcc: 86.53%\n",
            "\n",
            "Test set: Average loss: 0.8472, Accuracy: 8532/10000 (85.32%)\n",
            "\n",
            "Epoch 71: Target Sparsity: 60.10%, Actual Sparsity: 60.08%\n",
            "Train Epoch: 71 [0/50000 (0%)]\tLoss: 0.814407\tAcc: 86.72%\n",
            "Train Epoch: 71 [12800/50000 (26%)]\tLoss: 0.888712\tAcc: 86.75%\n",
            "Train Epoch: 71 [25600/50000 (51%)]\tLoss: 0.837333\tAcc: 85.98%\n",
            "Train Epoch: 71 [38400/50000 (77%)]\tLoss: 0.867867\tAcc: 85.82%\n",
            "\n",
            "Test set: Average loss: 0.8560, Accuracy: 8429/10000 (84.29%)\n",
            "\n",
            "Epoch 72: Target Sparsity: 60.26%, Actual Sparsity: 60.24%\n",
            "Train Epoch: 72 [0/50000 (0%)]\tLoss: 0.936256\tAcc: 82.03%\n",
            "Train Epoch: 72 [12800/50000 (26%)]\tLoss: 0.831025\tAcc: 85.58%\n",
            "Train Epoch: 72 [25600/50000 (51%)]\tLoss: 0.883470\tAcc: 85.17%\n",
            "Train Epoch: 72 [38400/50000 (77%)]\tLoss: 0.920774\tAcc: 85.06%\n",
            "\n",
            "Test set: Average loss: 0.8274, Accuracy: 8595/10000 (85.95%)\n",
            "\n",
            "Epoch 73: Target Sparsity: 60.43%, Actual Sparsity: 60.41%\n",
            "Train Epoch: 73 [0/50000 (0%)]\tLoss: 0.801646\tAcc: 87.50%\n",
            "Train Epoch: 73 [12800/50000 (26%)]\tLoss: 0.868302\tAcc: 84.74%\n",
            "Train Epoch: 73 [25600/50000 (51%)]\tLoss: 0.801615\tAcc: 84.55%\n",
            "Train Epoch: 73 [38400/50000 (77%)]\tLoss: 0.895624\tAcc: 84.52%\n",
            "\n",
            "Test set: Average loss: 0.8310, Accuracy: 8564/10000 (85.64%)\n",
            "\n",
            "Epoch 74: Target Sparsity: 60.59%, Actual Sparsity: 60.57%\n",
            "Train Epoch: 74 [0/50000 (0%)]\tLoss: 0.903786\tAcc: 85.16%\n",
            "Train Epoch: 74 [12800/50000 (26%)]\tLoss: 0.785273\tAcc: 83.90%\n",
            "Train Epoch: 74 [25600/50000 (51%)]\tLoss: 0.733962\tAcc: 83.71%\n",
            "Train Epoch: 74 [38400/50000 (77%)]\tLoss: 0.762828\tAcc: 83.72%\n",
            "\n",
            "Test set: Average loss: 0.9290, Accuracy: 8169/10000 (81.69%)\n",
            "\n",
            "Epoch 75: Target Sparsity: 60.76%, Actual Sparsity: 60.74%\n",
            "Train Epoch: 75 [0/50000 (0%)]\tLoss: 0.907580\tAcc: 84.38%\n",
            "Train Epoch: 75 [12800/50000 (26%)]\tLoss: 0.890070\tAcc: 83.52%\n",
            "Train Epoch: 75 [25600/50000 (51%)]\tLoss: 0.899210\tAcc: 83.52%\n",
            "Train Epoch: 75 [38400/50000 (77%)]\tLoss: 0.936056\tAcc: 83.49%\n",
            "\n",
            "Test set: Average loss: 0.8698, Accuracy: 8439/10000 (84.39%)\n",
            "\n",
            "Epoch 76: Target Sparsity: 60.92%, Actual Sparsity: 60.90%\n",
            "Train Epoch: 76 [0/50000 (0%)]\tLoss: 0.801026\tAcc: 86.72%\n",
            "Train Epoch: 76 [12800/50000 (26%)]\tLoss: 0.915582\tAcc: 83.55%\n",
            "Train Epoch: 76 [25600/50000 (51%)]\tLoss: 0.887666\tAcc: 83.18%\n",
            "Train Epoch: 76 [38400/50000 (77%)]\tLoss: 0.830407\tAcc: 82.94%\n",
            "\n",
            "Test set: Average loss: 0.9552, Accuracy: 8085/10000 (80.85%)\n",
            "\n",
            "Epoch 77: Target Sparsity: 61.09%, Actual Sparsity: 61.07%\n",
            "Train Epoch: 77 [0/50000 (0%)]\tLoss: 0.936266\tAcc: 80.47%\n",
            "Train Epoch: 77 [12800/50000 (26%)]\tLoss: 0.931286\tAcc: 82.60%\n",
            "Train Epoch: 77 [25600/50000 (51%)]\tLoss: 0.764380\tAcc: 82.80%\n",
            "Train Epoch: 77 [38400/50000 (77%)]\tLoss: 1.055107\tAcc: 82.42%\n",
            "\n",
            "Test set: Average loss: 0.9065, Accuracy: 8301/10000 (83.01%)\n",
            "\n",
            "Epoch 78: Target Sparsity: 61.26%, Actual Sparsity: 61.24%\n",
            "Train Epoch: 78 [0/50000 (0%)]\tLoss: 0.807768\tAcc: 85.94%\n",
            "Train Epoch: 78 [12800/50000 (26%)]\tLoss: 1.019392\tAcc: 82.75%\n",
            "Train Epoch: 78 [25600/50000 (51%)]\tLoss: 0.986379\tAcc: 82.49%\n",
            "Train Epoch: 78 [38400/50000 (77%)]\tLoss: 0.991905\tAcc: 82.38%\n",
            "\n",
            "Test set: Average loss: 0.9104, Accuracy: 8253/10000 (82.53%)\n",
            "\n",
            "Epoch 79: Target Sparsity: 61.42%, Actual Sparsity: 61.40%\n",
            "Train Epoch: 79 [0/50000 (0%)]\tLoss: 0.870563\tAcc: 79.69%\n",
            "Train Epoch: 79 [12800/50000 (26%)]\tLoss: 1.065915\tAcc: 82.39%\n",
            "Train Epoch: 79 [25600/50000 (51%)]\tLoss: 0.842774\tAcc: 82.18%\n",
            "Train Epoch: 79 [38400/50000 (77%)]\tLoss: 0.932761\tAcc: 81.97%\n",
            "\n",
            "Test set: Average loss: 0.9167, Accuracy: 8286/10000 (82.86%)\n",
            "\n",
            "Epoch 80: Target Sparsity: 61.59%, Actual Sparsity: 61.57%\n",
            "Train Epoch: 80 [0/50000 (0%)]\tLoss: 0.924535\tAcc: 82.03%\n",
            "Train Epoch: 80 [12800/50000 (26%)]\tLoss: 0.891833\tAcc: 81.92%\n",
            "Train Epoch: 80 [25600/50000 (51%)]\tLoss: 0.876046\tAcc: 82.14%\n",
            "Train Epoch: 80 [38400/50000 (77%)]\tLoss: 0.945222\tAcc: 81.71%\n",
            "\n",
            "Test set: Average loss: 0.8540, Accuracy: 8462/10000 (84.62%)\n",
            "\n",
            "Epoch 81: Target Sparsity: 61.75%, Actual Sparsity: 61.73%\n",
            "Train Epoch: 81 [0/50000 (0%)]\tLoss: 0.877432\tAcc: 82.81%\n",
            "Train Epoch: 81 [12800/50000 (26%)]\tLoss: 0.976966\tAcc: 82.19%\n",
            "Train Epoch: 81 [25600/50000 (51%)]\tLoss: 0.997260\tAcc: 81.99%\n",
            "Train Epoch: 81 [38400/50000 (77%)]\tLoss: 0.941349\tAcc: 81.71%\n",
            "\n",
            "Test set: Average loss: 0.9237, Accuracy: 8217/10000 (82.17%)\n",
            "\n",
            "Epoch 82: Target Sparsity: 61.92%, Actual Sparsity: 61.90%\n",
            "Train Epoch: 82 [0/50000 (0%)]\tLoss: 0.951867\tAcc: 79.69%\n",
            "Train Epoch: 82 [12800/50000 (26%)]\tLoss: 0.954277\tAcc: 81.70%\n",
            "Train Epoch: 82 [25600/50000 (51%)]\tLoss: 0.782415\tAcc: 81.13%\n",
            "Train Epoch: 82 [38400/50000 (77%)]\tLoss: 0.976350\tAcc: 80.95%\n",
            "\n",
            "Test set: Average loss: 1.0489, Accuracy: 7668/10000 (76.68%)\n",
            "\n",
            "Epoch 83: Target Sparsity: 62.08%, Actual Sparsity: 62.06%\n",
            "Train Epoch: 83 [0/50000 (0%)]\tLoss: 0.972006\tAcc: 80.47%\n",
            "Train Epoch: 83 [12800/50000 (26%)]\tLoss: 1.014693\tAcc: 81.36%\n",
            "Train Epoch: 83 [25600/50000 (51%)]\tLoss: 0.977045\tAcc: 81.08%\n",
            "Train Epoch: 83 [38400/50000 (77%)]\tLoss: 0.916349\tAcc: 81.14%\n",
            "\n",
            "Test set: Average loss: 0.9389, Accuracy: 8116/10000 (81.16%)\n",
            "\n",
            "Epoch 84: Target Sparsity: 62.25%, Actual Sparsity: 62.23%\n",
            "Train Epoch: 84 [0/50000 (0%)]\tLoss: 0.907839\tAcc: 82.03%\n",
            "Train Epoch: 84 [12800/50000 (26%)]\tLoss: 0.912051\tAcc: 81.56%\n",
            "Train Epoch: 84 [25600/50000 (51%)]\tLoss: 0.965147\tAcc: 81.15%\n",
            "Train Epoch: 84 [38400/50000 (77%)]\tLoss: 1.023640\tAcc: 81.02%\n",
            "\n",
            "Test set: Average loss: 0.9007, Accuracy: 8289/10000 (82.89%)\n",
            "\n",
            "Epoch 85: Target Sparsity: 62.41%, Actual Sparsity: 62.39%\n",
            "Train Epoch: 85 [0/50000 (0%)]\tLoss: 0.960190\tAcc: 80.47%\n",
            "Train Epoch: 85 [12800/50000 (26%)]\tLoss: 0.977772\tAcc: 81.18%\n",
            "Train Epoch: 85 [25600/50000 (51%)]\tLoss: 0.927261\tAcc: 81.02%\n",
            "Train Epoch: 85 [38400/50000 (77%)]\tLoss: 0.911064\tAcc: 80.69%\n",
            "\n",
            "Test set: Average loss: 0.9387, Accuracy: 8117/10000 (81.17%)\n",
            "\n",
            "Epoch 86: Target Sparsity: 62.58%, Actual Sparsity: 62.56%\n",
            "Train Epoch: 86 [0/50000 (0%)]\tLoss: 0.945444\tAcc: 82.03%\n",
            "Train Epoch: 86 [12800/50000 (26%)]\tLoss: 0.863926\tAcc: 81.08%\n",
            "Train Epoch: 86 [25600/50000 (51%)]\tLoss: 1.080603\tAcc: 80.67%\n",
            "Train Epoch: 86 [38400/50000 (77%)]\tLoss: 0.883861\tAcc: 80.56%\n",
            "\n",
            "Test set: Average loss: 0.9710, Accuracy: 8011/10000 (80.11%)\n",
            "\n",
            "Epoch 87: Target Sparsity: 62.74%, Actual Sparsity: 62.72%\n",
            "Train Epoch: 87 [0/50000 (0%)]\tLoss: 0.945461\tAcc: 77.34%\n",
            "Train Epoch: 87 [12800/50000 (26%)]\tLoss: 0.976092\tAcc: 80.99%\n",
            "Train Epoch: 87 [25600/50000 (51%)]\tLoss: 1.050351\tAcc: 80.59%\n",
            "Train Epoch: 87 [38400/50000 (77%)]\tLoss: 0.987369\tAcc: 80.34%\n",
            "\n",
            "Test set: Average loss: 1.0840, Accuracy: 7602/10000 (76.02%)\n",
            "\n",
            "Epoch 88: Target Sparsity: 62.91%, Actual Sparsity: 62.89%\n",
            "Train Epoch: 88 [0/50000 (0%)]\tLoss: 1.058333\tAcc: 75.78%\n",
            "Train Epoch: 88 [12800/50000 (26%)]\tLoss: 0.923829\tAcc: 80.57%\n",
            "Train Epoch: 88 [25600/50000 (51%)]\tLoss: 1.014978\tAcc: 80.32%\n",
            "Train Epoch: 88 [38400/50000 (77%)]\tLoss: 0.937845\tAcc: 80.05%\n",
            "\n",
            "Test set: Average loss: 1.0526, Accuracy: 7690/10000 (76.90%)\n",
            "\n",
            "Epoch 89: Target Sparsity: 63.08%, Actual Sparsity: 63.06%\n",
            "Train Epoch: 89 [0/50000 (0%)]\tLoss: 0.899249\tAcc: 86.72%\n",
            "Train Epoch: 89 [12800/50000 (26%)]\tLoss: 0.987035\tAcc: 80.29%\n",
            "Train Epoch: 89 [25600/50000 (51%)]\tLoss: 0.980852\tAcc: 79.88%\n",
            "Train Epoch: 89 [38400/50000 (77%)]\tLoss: 0.930049\tAcc: 79.90%\n",
            "\n",
            "Test set: Average loss: 1.0603, Accuracy: 7664/10000 (76.64%)\n",
            "\n",
            "Epoch 90: Target Sparsity: 63.24%, Actual Sparsity: 63.22%\n",
            "Train Epoch: 90 [0/50000 (0%)]\tLoss: 0.908216\tAcc: 80.47%\n",
            "Train Epoch: 90 [12800/50000 (26%)]\tLoss: 1.027173\tAcc: 80.16%\n",
            "Train Epoch: 90 [25600/50000 (51%)]\tLoss: 1.151794\tAcc: 79.80%\n",
            "Train Epoch: 90 [38400/50000 (77%)]\tLoss: 0.851636\tAcc: 79.88%\n",
            "\n",
            "Test set: Average loss: 0.9077, Accuracy: 8298/10000 (82.98%)\n",
            "\n",
            "Epoch 91: Target Sparsity: 63.41%, Actual Sparsity: 63.39%\n",
            "Train Epoch: 91 [0/50000 (0%)]\tLoss: 1.018778\tAcc: 77.34%\n",
            "Train Epoch: 91 [12800/50000 (26%)]\tLoss: 1.144657\tAcc: 80.54%\n",
            "Train Epoch: 91 [25600/50000 (51%)]\tLoss: 0.939224\tAcc: 80.07%\n",
            "Train Epoch: 91 [38400/50000 (77%)]\tLoss: 1.022264\tAcc: 79.99%\n",
            "\n",
            "Test set: Average loss: 1.1049, Accuracy: 7540/10000 (75.40%)\n",
            "\n",
            "Epoch 92: Target Sparsity: 63.57%, Actual Sparsity: 63.55%\n",
            "Train Epoch: 92 [0/50000 (0%)]\tLoss: 0.905971\tAcc: 85.16%\n",
            "Train Epoch: 92 [12800/50000 (26%)]\tLoss: 1.000230\tAcc: 79.74%\n",
            "Train Epoch: 92 [25600/50000 (51%)]\tLoss: 0.989398\tAcc: 79.91%\n",
            "Train Epoch: 92 [38400/50000 (77%)]\tLoss: 0.927810\tAcc: 79.94%\n",
            "\n",
            "Test set: Average loss: 0.9918, Accuracy: 7873/10000 (78.73%)\n",
            "\n",
            "Epoch 93: Target Sparsity: 63.74%, Actual Sparsity: 63.72%\n",
            "Train Epoch: 93 [0/50000 (0%)]\tLoss: 0.980276\tAcc: 77.34%\n",
            "Train Epoch: 93 [12800/50000 (26%)]\tLoss: 1.057920\tAcc: 80.04%\n",
            "Train Epoch: 93 [25600/50000 (51%)]\tLoss: 0.952724\tAcc: 79.93%\n",
            "Train Epoch: 93 [38400/50000 (77%)]\tLoss: 0.956577\tAcc: 80.05%\n",
            "\n",
            "Test set: Average loss: 1.1025, Accuracy: 7564/10000 (75.64%)\n",
            "\n",
            "Epoch 94: Target Sparsity: 63.90%, Actual Sparsity: 63.88%\n",
            "Train Epoch: 94 [0/50000 (0%)]\tLoss: 0.836365\tAcc: 85.94%\n",
            "Train Epoch: 94 [12800/50000 (26%)]\tLoss: 0.898387\tAcc: 79.74%\n",
            "Train Epoch: 94 [25600/50000 (51%)]\tLoss: 0.988255\tAcc: 79.86%\n",
            "Train Epoch: 94 [38400/50000 (77%)]\tLoss: 0.913732\tAcc: 79.60%\n",
            "\n",
            "Test set: Average loss: 1.0070, Accuracy: 7785/10000 (77.85%)\n",
            "\n",
            "Epoch 95: Target Sparsity: 64.07%, Actual Sparsity: 64.05%\n",
            "Train Epoch: 95 [0/50000 (0%)]\tLoss: 1.019587\tAcc: 77.34%\n",
            "Train Epoch: 95 [12800/50000 (26%)]\tLoss: 0.930573\tAcc: 80.13%\n",
            "Train Epoch: 95 [25600/50000 (51%)]\tLoss: 1.184336\tAcc: 79.91%\n",
            "Train Epoch: 95 [38400/50000 (77%)]\tLoss: 1.006060\tAcc: 79.99%\n",
            "\n",
            "Test set: Average loss: 1.0032, Accuracy: 7907/10000 (79.07%)\n",
            "\n",
            "Epoch 96: Target Sparsity: 64.23%, Actual Sparsity: 64.21%\n",
            "Train Epoch: 96 [0/50000 (0%)]\tLoss: 1.002807\tAcc: 73.44%\n",
            "Train Epoch: 96 [12800/50000 (26%)]\tLoss: 0.927775\tAcc: 79.30%\n",
            "Train Epoch: 96 [25600/50000 (51%)]\tLoss: 1.037637\tAcc: 79.33%\n",
            "Train Epoch: 96 [38400/50000 (77%)]\tLoss: 1.041021\tAcc: 79.44%\n",
            "\n",
            "Test set: Average loss: 1.0098, Accuracy: 7840/10000 (78.40%)\n",
            "\n",
            "Epoch 97: Target Sparsity: 64.40%, Actual Sparsity: 64.38%\n",
            "Train Epoch: 97 [0/50000 (0%)]\tLoss: 0.820858\tAcc: 88.28%\n",
            "Train Epoch: 97 [12800/50000 (26%)]\tLoss: 0.898614\tAcc: 79.54%\n",
            "Train Epoch: 97 [25600/50000 (51%)]\tLoss: 1.010045\tAcc: 79.82%\n",
            "Train Epoch: 97 [38400/50000 (77%)]\tLoss: 0.843063\tAcc: 79.77%\n",
            "\n",
            "Test set: Average loss: 1.0243, Accuracy: 7775/10000 (77.75%)\n",
            "\n",
            "Epoch 98: Target Sparsity: 64.57%, Actual Sparsity: 64.55%\n",
            "Train Epoch: 98 [0/50000 (0%)]\tLoss: 0.887510\tAcc: 82.03%\n",
            "Train Epoch: 98 [12800/50000 (26%)]\tLoss: 0.922619\tAcc: 79.81%\n",
            "Train Epoch: 98 [25600/50000 (51%)]\tLoss: 1.004977\tAcc: 79.59%\n",
            "Train Epoch: 98 [38400/50000 (77%)]\tLoss: 0.910008\tAcc: 79.60%\n",
            "\n",
            "Test set: Average loss: 0.8991, Accuracy: 8279/10000 (82.79%)\n",
            "\n",
            "Epoch 99: Target Sparsity: 64.73%, Actual Sparsity: 64.71%\n",
            "Train Epoch: 99 [0/50000 (0%)]\tLoss: 0.854762\tAcc: 87.50%\n",
            "Train Epoch: 99 [12800/50000 (26%)]\tLoss: 1.032588\tAcc: 79.87%\n",
            "Train Epoch: 99 [25600/50000 (51%)]\tLoss: 0.958163\tAcc: 79.78%\n",
            "Train Epoch: 99 [38400/50000 (77%)]\tLoss: 0.914188\tAcc: 79.59%\n",
            "\n",
            "Test set: Average loss: 1.0032, Accuracy: 7861/10000 (78.61%)\n",
            "\n",
            "Epoch 100: Target Sparsity: 64.90%, Actual Sparsity: 64.88%\n",
            "Train Epoch: 100 [0/50000 (0%)]\tLoss: 1.045994\tAcc: 74.22%\n",
            "Train Epoch: 100 [12800/50000 (26%)]\tLoss: 0.946378\tAcc: 80.13%\n",
            "Train Epoch: 100 [25600/50000 (51%)]\tLoss: 0.947697\tAcc: 79.94%\n",
            "Train Epoch: 100 [38400/50000 (77%)]\tLoss: 0.896465\tAcc: 79.73%\n",
            "\n",
            "Test set: Average loss: 1.0777, Accuracy: 7538/10000 (75.38%)\n",
            "\n",
            "Epoch 101: Target Sparsity: 65.06%, Actual Sparsity: 65.04%\n",
            "Train Epoch: 101 [0/50000 (0%)]\tLoss: 0.880586\tAcc: 85.16%\n",
            "Train Epoch: 101 [12800/50000 (26%)]\tLoss: 0.921045\tAcc: 80.42%\n",
            "Train Epoch: 101 [25600/50000 (51%)]\tLoss: 0.957552\tAcc: 80.33%\n",
            "Train Epoch: 101 [38400/50000 (77%)]\tLoss: 1.039457\tAcc: 80.01%\n",
            "\n",
            "Test set: Average loss: 0.9541, Accuracy: 8025/10000 (80.25%)\n",
            "\n",
            "Epoch 102: Target Sparsity: 65.23%, Actual Sparsity: 65.21%\n",
            "Train Epoch: 102 [0/50000 (0%)]\tLoss: 0.999024\tAcc: 78.12%\n",
            "Train Epoch: 102 [12800/50000 (26%)]\tLoss: 1.016710\tAcc: 80.36%\n",
            "Train Epoch: 102 [25600/50000 (51%)]\tLoss: 1.065752\tAcc: 79.88%\n",
            "Train Epoch: 102 [38400/50000 (77%)]\tLoss: 0.975154\tAcc: 79.73%\n",
            "\n",
            "Test set: Average loss: 0.9287, Accuracy: 8165/10000 (81.65%)\n",
            "\n",
            "Epoch 103: Target Sparsity: 65.39%, Actual Sparsity: 65.37%\n",
            "Train Epoch: 103 [0/50000 (0%)]\tLoss: 1.006881\tAcc: 79.69%\n",
            "Train Epoch: 103 [12800/50000 (26%)]\tLoss: 1.013371\tAcc: 80.12%\n",
            "Train Epoch: 103 [25600/50000 (51%)]\tLoss: 0.839189\tAcc: 80.08%\n",
            "Train Epoch: 103 [38400/50000 (77%)]\tLoss: 0.994237\tAcc: 79.83%\n",
            "\n",
            "Test set: Average loss: 0.9964, Accuracy: 7891/10000 (78.91%)\n",
            "\n",
            "Epoch 104: Target Sparsity: 65.56%, Actual Sparsity: 65.54%\n",
            "Train Epoch: 104 [0/50000 (0%)]\tLoss: 1.026143\tAcc: 77.34%\n",
            "Train Epoch: 104 [12800/50000 (26%)]\tLoss: 1.048938\tAcc: 79.67%\n",
            "Train Epoch: 104 [25600/50000 (51%)]\tLoss: 0.960989\tAcc: 79.76%\n",
            "Train Epoch: 104 [38400/50000 (77%)]\tLoss: 0.964826\tAcc: 79.86%\n",
            "\n",
            "Test set: Average loss: 0.9680, Accuracy: 7956/10000 (79.56%)\n",
            "\n",
            "Epoch 105: Target Sparsity: 65.72%, Actual Sparsity: 65.70%\n",
            "Train Epoch: 105 [0/50000 (0%)]\tLoss: 0.961752\tAcc: 81.25%\n",
            "Train Epoch: 105 [12800/50000 (26%)]\tLoss: 0.909153\tAcc: 79.93%\n",
            "Train Epoch: 105 [25600/50000 (51%)]\tLoss: 0.948372\tAcc: 79.83%\n",
            "Train Epoch: 105 [38400/50000 (77%)]\tLoss: 1.009602\tAcc: 79.66%\n",
            "\n",
            "Test set: Average loss: 0.9465, Accuracy: 8110/10000 (81.10%)\n",
            "\n",
            "Epoch 106: Target Sparsity: 65.89%, Actual Sparsity: 65.87%\n",
            "Train Epoch: 106 [0/50000 (0%)]\tLoss: 0.929068\tAcc: 81.25%\n",
            "Train Epoch: 106 [12800/50000 (26%)]\tLoss: 0.849101\tAcc: 79.94%\n",
            "Train Epoch: 106 [25600/50000 (51%)]\tLoss: 1.016201\tAcc: 79.93%\n",
            "Train Epoch: 106 [38400/50000 (77%)]\tLoss: 0.939937\tAcc: 80.13%\n",
            "\n",
            "Test set: Average loss: 0.9770, Accuracy: 7915/10000 (79.15%)\n",
            "\n",
            "Epoch 107: Target Sparsity: 66.06%, Actual Sparsity: 66.03%\n",
            "Train Epoch: 107 [0/50000 (0%)]\tLoss: 0.891100\tAcc: 85.16%\n",
            "Train Epoch: 107 [12800/50000 (26%)]\tLoss: 0.911370\tAcc: 80.11%\n",
            "Train Epoch: 107 [25600/50000 (51%)]\tLoss: 1.026155\tAcc: 80.20%\n",
            "Train Epoch: 107 [38400/50000 (77%)]\tLoss: 0.932961\tAcc: 80.03%\n",
            "\n",
            "Test set: Average loss: 1.1361, Accuracy: 7380/10000 (73.80%)\n",
            "\n",
            "Epoch 108: Target Sparsity: 66.22%, Actual Sparsity: 66.20%\n",
            "Train Epoch: 108 [0/50000 (0%)]\tLoss: 1.079194\tAcc: 77.34%\n",
            "Train Epoch: 108 [12800/50000 (26%)]\tLoss: 0.978280\tAcc: 80.50%\n",
            "Train Epoch: 108 [25600/50000 (51%)]\tLoss: 1.047972\tAcc: 80.37%\n",
            "Train Epoch: 108 [38400/50000 (77%)]\tLoss: 1.142575\tAcc: 80.31%\n",
            "\n",
            "Test set: Average loss: 0.9299, Accuracy: 8165/10000 (81.65%)\n",
            "\n",
            "Epoch 109: Target Sparsity: 66.39%, Actual Sparsity: 66.37%\n",
            "Train Epoch: 109 [0/50000 (0%)]\tLoss: 0.978115\tAcc: 78.91%\n",
            "Train Epoch: 109 [12800/50000 (26%)]\tLoss: 0.974693\tAcc: 80.35%\n",
            "Train Epoch: 109 [25600/50000 (51%)]\tLoss: 0.899981\tAcc: 80.58%\n",
            "Train Epoch: 109 [38400/50000 (77%)]\tLoss: 1.052984\tAcc: 80.42%\n",
            "\n",
            "Test set: Average loss: 0.9770, Accuracy: 7989/10000 (79.89%)\n",
            "\n",
            "Epoch 110: Target Sparsity: 66.55%, Actual Sparsity: 66.53%\n",
            "Train Epoch: 110 [0/50000 (0%)]\tLoss: 1.018345\tAcc: 78.12%\n",
            "Train Epoch: 110 [12800/50000 (26%)]\tLoss: 0.859283\tAcc: 81.12%\n",
            "Train Epoch: 110 [25600/50000 (51%)]\tLoss: 1.089365\tAcc: 81.02%\n",
            "Train Epoch: 110 [38400/50000 (77%)]\tLoss: 0.851823\tAcc: 80.74%\n",
            "\n",
            "Test set: Average loss: 0.8732, Accuracy: 8406/10000 (84.06%)\n",
            "\n",
            "Epoch 111: Target Sparsity: 66.72%, Actual Sparsity: 66.70%\n",
            "Train Epoch: 111 [0/50000 (0%)]\tLoss: 0.989610\tAcc: 79.69%\n",
            "Train Epoch: 111 [12800/50000 (26%)]\tLoss: 0.900903\tAcc: 81.54%\n",
            "Train Epoch: 111 [25600/50000 (51%)]\tLoss: 0.981340\tAcc: 81.14%\n",
            "Train Epoch: 111 [38400/50000 (77%)]\tLoss: 0.819134\tAcc: 81.08%\n",
            "\n",
            "Test set: Average loss: 0.9127, Accuracy: 8210/10000 (82.10%)\n",
            "\n",
            "Epoch 112: Target Sparsity: 66.88%, Actual Sparsity: 66.86%\n",
            "Train Epoch: 112 [0/50000 (0%)]\tLoss: 0.893849\tAcc: 83.59%\n",
            "Train Epoch: 112 [12800/50000 (26%)]\tLoss: 0.881546\tAcc: 80.76%\n",
            "Train Epoch: 112 [25600/50000 (51%)]\tLoss: 0.955738\tAcc: 81.06%\n",
            "Train Epoch: 112 [38400/50000 (77%)]\tLoss: 1.085364\tAcc: 81.04%\n",
            "\n",
            "Test set: Average loss: 1.0993, Accuracy: 7614/10000 (76.14%)\n",
            "\n",
            "Epoch 113: Target Sparsity: 67.05%, Actual Sparsity: 67.03%\n",
            "Train Epoch: 113 [0/50000 (0%)]\tLoss: 0.893649\tAcc: 83.59%\n",
            "Train Epoch: 113 [12800/50000 (26%)]\tLoss: 1.009367\tAcc: 80.69%\n",
            "Train Epoch: 113 [25600/50000 (51%)]\tLoss: 0.998312\tAcc: 80.83%\n",
            "Train Epoch: 113 [38400/50000 (77%)]\tLoss: 0.939076\tAcc: 80.83%\n",
            "\n",
            "Test set: Average loss: 1.0064, Accuracy: 7851/10000 (78.51%)\n",
            "\n",
            "Epoch 114: Target Sparsity: 67.21%, Actual Sparsity: 67.19%\n",
            "Train Epoch: 114 [0/50000 (0%)]\tLoss: 1.039801\tAcc: 78.12%\n",
            "Train Epoch: 114 [12800/50000 (26%)]\tLoss: 0.881185\tAcc: 81.21%\n",
            "Train Epoch: 114 [25600/50000 (51%)]\tLoss: 0.914413\tAcc: 81.24%\n",
            "Train Epoch: 114 [38400/50000 (77%)]\tLoss: 0.916088\tAcc: 81.23%\n",
            "\n",
            "Test set: Average loss: 0.9011, Accuracy: 8279/10000 (82.79%)\n",
            "\n",
            "Epoch 115: Target Sparsity: 67.38%, Actual Sparsity: 67.36%\n",
            "Train Epoch: 115 [0/50000 (0%)]\tLoss: 0.960540\tAcc: 78.91%\n",
            "Train Epoch: 115 [12800/50000 (26%)]\tLoss: 0.875102\tAcc: 81.60%\n",
            "Train Epoch: 115 [25600/50000 (51%)]\tLoss: 0.792449\tAcc: 81.65%\n",
            "Train Epoch: 115 [38400/50000 (77%)]\tLoss: 0.913737\tAcc: 81.57%\n",
            "\n",
            "Test set: Average loss: 0.9158, Accuracy: 8242/10000 (82.42%)\n",
            "\n",
            "Epoch 116: Target Sparsity: 67.54%, Actual Sparsity: 67.52%\n",
            "Train Epoch: 116 [0/50000 (0%)]\tLoss: 0.879939\tAcc: 84.38%\n",
            "Train Epoch: 116 [12800/50000 (26%)]\tLoss: 0.868153\tAcc: 82.27%\n",
            "Train Epoch: 116 [25600/50000 (51%)]\tLoss: 1.001201\tAcc: 82.36%\n",
            "Train Epoch: 116 [38400/50000 (77%)]\tLoss: 1.001347\tAcc: 82.14%\n",
            "\n",
            "Test set: Average loss: 0.9745, Accuracy: 8039/10000 (80.39%)\n",
            "\n",
            "Epoch 117: Target Sparsity: 67.71%, Actual Sparsity: 67.69%\n",
            "Train Epoch: 117 [0/50000 (0%)]\tLoss: 0.871215\tAcc: 81.25%\n",
            "Train Epoch: 117 [12800/50000 (26%)]\tLoss: 0.915540\tAcc: 82.24%\n",
            "Train Epoch: 117 [25600/50000 (51%)]\tLoss: 0.957056\tAcc: 81.80%\n",
            "Train Epoch: 117 [38400/50000 (77%)]\tLoss: 0.937250\tAcc: 81.97%\n",
            "\n",
            "Test set: Average loss: 0.9679, Accuracy: 7972/10000 (79.72%)\n",
            "\n",
            "Epoch 118: Target Sparsity: 67.88%, Actual Sparsity: 67.85%\n",
            "Train Epoch: 118 [0/50000 (0%)]\tLoss: 0.824275\tAcc: 87.50%\n",
            "Train Epoch: 118 [12800/50000 (26%)]\tLoss: 0.964713\tAcc: 82.63%\n",
            "Train Epoch: 118 [25600/50000 (51%)]\tLoss: 0.954769\tAcc: 82.34%\n",
            "Train Epoch: 118 [38400/50000 (77%)]\tLoss: 0.968292\tAcc: 82.35%\n",
            "\n",
            "Test set: Average loss: 0.8990, Accuracy: 8313/10000 (83.13%)\n",
            "\n",
            "Epoch 119: Target Sparsity: 68.04%, Actual Sparsity: 68.02%\n",
            "Train Epoch: 119 [0/50000 (0%)]\tLoss: 0.891159\tAcc: 81.25%\n",
            "Train Epoch: 119 [12800/50000 (26%)]\tLoss: 0.888594\tAcc: 82.63%\n",
            "Train Epoch: 119 [25600/50000 (51%)]\tLoss: 1.004038\tAcc: 82.61%\n",
            "Train Epoch: 119 [38400/50000 (77%)]\tLoss: 0.970055\tAcc: 82.76%\n",
            "\n",
            "Test set: Average loss: 1.1633, Accuracy: 7341/10000 (73.41%)\n",
            "\n",
            "Epoch 120: Target Sparsity: 68.21%, Actual Sparsity: 68.19%\n",
            "Train Epoch: 120 [0/50000 (0%)]\tLoss: 0.946850\tAcc: 80.47%\n",
            "Train Epoch: 120 [12800/50000 (26%)]\tLoss: 0.900955\tAcc: 83.49%\n",
            "Train Epoch: 120 [25600/50000 (51%)]\tLoss: 0.831349\tAcc: 83.43%\n",
            "Train Epoch: 120 [38400/50000 (77%)]\tLoss: 0.849605\tAcc: 83.38%\n",
            "\n",
            "Test set: Average loss: 0.8898, Accuracy: 8272/10000 (82.72%)\n",
            "\n",
            "Epoch 121: Target Sparsity: 68.37%, Actual Sparsity: 68.35%\n",
            "Train Epoch: 121 [0/50000 (0%)]\tLoss: 0.860328\tAcc: 85.16%\n",
            "Train Epoch: 121 [12800/50000 (26%)]\tLoss: 0.810002\tAcc: 83.51%\n",
            "Train Epoch: 121 [25600/50000 (51%)]\tLoss: 0.802676\tAcc: 83.48%\n",
            "Train Epoch: 121 [38400/50000 (77%)]\tLoss: 0.910113\tAcc: 83.43%\n",
            "\n",
            "Test set: Average loss: 1.0383, Accuracy: 7760/10000 (77.60%)\n",
            "\n",
            "Epoch 122: Target Sparsity: 68.54%, Actual Sparsity: 68.52%\n",
            "Train Epoch: 122 [0/50000 (0%)]\tLoss: 0.920739\tAcc: 80.47%\n",
            "Train Epoch: 122 [12800/50000 (26%)]\tLoss: 0.797550\tAcc: 83.58%\n",
            "Train Epoch: 122 [25600/50000 (51%)]\tLoss: 0.867851\tAcc: 83.48%\n",
            "Train Epoch: 122 [38400/50000 (77%)]\tLoss: 0.847740\tAcc: 83.39%\n",
            "\n",
            "Test set: Average loss: 0.8207, Accuracy: 8616/10000 (86.16%)\n",
            "\n",
            "Epoch 123: Target Sparsity: 68.70%, Actual Sparsity: 68.68%\n",
            "Train Epoch: 123 [0/50000 (0%)]\tLoss: 0.900401\tAcc: 85.16%\n",
            "Train Epoch: 123 [12800/50000 (26%)]\tLoss: 0.967021\tAcc: 84.24%\n",
            "Train Epoch: 123 [25600/50000 (51%)]\tLoss: 0.861864\tAcc: 84.11%\n",
            "Train Epoch: 123 [38400/50000 (77%)]\tLoss: 0.834365\tAcc: 84.00%\n",
            "\n",
            "Test set: Average loss: 0.8649, Accuracy: 8436/10000 (84.36%)\n",
            "\n",
            "Epoch 124: Target Sparsity: 68.87%, Actual Sparsity: 68.85%\n",
            "Train Epoch: 124 [0/50000 (0%)]\tLoss: 0.780513\tAcc: 89.84%\n",
            "Train Epoch: 124 [12800/50000 (26%)]\tLoss: 0.891059\tAcc: 84.83%\n",
            "Train Epoch: 124 [25600/50000 (51%)]\tLoss: 0.792046\tAcc: 84.60%\n",
            "Train Epoch: 124 [38400/50000 (77%)]\tLoss: 0.802704\tAcc: 84.54%\n",
            "\n",
            "Test set: Average loss: 0.8520, Accuracy: 8542/10000 (85.42%)\n",
            "\n",
            "Epoch 125: Target Sparsity: 69.03%, Actual Sparsity: 69.01%\n",
            "Train Epoch: 125 [0/50000 (0%)]\tLoss: 0.826645\tAcc: 88.28%\n",
            "Train Epoch: 125 [12800/50000 (26%)]\tLoss: 0.707554\tAcc: 85.25%\n",
            "Train Epoch: 125 [25600/50000 (51%)]\tLoss: 0.892411\tAcc: 85.24%\n",
            "Train Epoch: 125 [38400/50000 (77%)]\tLoss: 0.775247\tAcc: 84.98%\n",
            "\n",
            "Test set: Average loss: 0.8390, Accuracy: 8550/10000 (85.50%)\n",
            "\n",
            "Epoch 126: Target Sparsity: 69.20%, Actual Sparsity: 69.18%\n",
            "Train Epoch: 126 [0/50000 (0%)]\tLoss: 0.871532\tAcc: 84.38%\n",
            "Train Epoch: 126 [12800/50000 (26%)]\tLoss: 0.856653\tAcc: 85.34%\n",
            "Train Epoch: 126 [25600/50000 (51%)]\tLoss: 0.843795\tAcc: 85.38%\n",
            "Train Epoch: 126 [38400/50000 (77%)]\tLoss: 0.819918\tAcc: 85.35%\n",
            "\n",
            "Test set: Average loss: 0.8823, Accuracy: 8363/10000 (83.63%)\n",
            "\n",
            "Epoch 127: Target Sparsity: 69.37%, Actual Sparsity: 69.34%\n",
            "Train Epoch: 127 [0/50000 (0%)]\tLoss: 0.747423\tAcc: 92.19%\n",
            "Train Epoch: 127 [12800/50000 (26%)]\tLoss: 0.874273\tAcc: 86.13%\n",
            "Train Epoch: 127 [25600/50000 (51%)]\tLoss: 0.830431\tAcc: 85.98%\n",
            "Train Epoch: 127 [38400/50000 (77%)]\tLoss: 0.859295\tAcc: 85.81%\n",
            "\n",
            "Test set: Average loss: 0.7728, Accuracy: 8772/10000 (87.72%)\n",
            "\n",
            "Epoch 128: Target Sparsity: 69.53%, Actual Sparsity: 69.51%\n",
            "Train Epoch: 128 [0/50000 (0%)]\tLoss: 0.666352\tAcc: 94.53%\n",
            "Train Epoch: 128 [12800/50000 (26%)]\tLoss: 0.835262\tAcc: 85.99%\n",
            "Train Epoch: 128 [25600/50000 (51%)]\tLoss: 0.738050\tAcc: 86.41%\n",
            "Train Epoch: 128 [38400/50000 (77%)]\tLoss: 0.752059\tAcc: 86.29%\n",
            "\n",
            "Test set: Average loss: 0.8056, Accuracy: 8640/10000 (86.40%)\n",
            "\n",
            "Epoch 129: Target Sparsity: 69.70%, Actual Sparsity: 69.67%\n",
            "Train Epoch: 129 [0/50000 (0%)]\tLoss: 0.819784\tAcc: 82.81%\n",
            "Train Epoch: 129 [12800/50000 (26%)]\tLoss: 0.860808\tAcc: 86.85%\n",
            "Train Epoch: 129 [25600/50000 (51%)]\tLoss: 0.829175\tAcc: 86.91%\n",
            "Train Epoch: 129 [38400/50000 (77%)]\tLoss: 0.700300\tAcc: 86.90%\n",
            "\n",
            "Test set: Average loss: 0.7536, Accuracy: 8901/10000 (89.01%)\n",
            "\n",
            "Epoch 130: Target Sparsity: 69.86%, Actual Sparsity: 69.84%\n",
            "Train Epoch: 130 [0/50000 (0%)]\tLoss: 0.932148\tAcc: 79.69%\n",
            "Train Epoch: 130 [12800/50000 (26%)]\tLoss: 0.751550\tAcc: 87.12%\n",
            "Train Epoch: 130 [25600/50000 (51%)]\tLoss: 0.960766\tAcc: 87.39%\n",
            "Train Epoch: 130 [38400/50000 (77%)]\tLoss: 0.764294\tAcc: 87.39%\n",
            "\n",
            "Test set: Average loss: 0.7456, Accuracy: 8935/10000 (89.35%)\n",
            "\n",
            "Epoch 131: Target Sparsity: 70.03%, Actual Sparsity: 70.01%\n",
            "Train Epoch: 131 [0/50000 (0%)]\tLoss: 0.809377\tAcc: 88.28%\n",
            "Train Epoch: 131 [12800/50000 (26%)]\tLoss: 0.850596\tAcc: 87.87%\n",
            "Train Epoch: 131 [25600/50000 (51%)]\tLoss: 0.873119\tAcc: 88.01%\n",
            "Train Epoch: 131 [38400/50000 (77%)]\tLoss: 0.814563\tAcc: 87.78%\n",
            "\n",
            "Test set: Average loss: 0.7435, Accuracy: 8943/10000 (89.43%)\n",
            "\n",
            "Epoch 132: Target Sparsity: 70.19%, Actual Sparsity: 70.17%\n",
            "Train Epoch: 132 [0/50000 (0%)]\tLoss: 0.748331\tAcc: 89.84%\n",
            "Train Epoch: 132 [12800/50000 (26%)]\tLoss: 0.775293\tAcc: 88.54%\n",
            "Train Epoch: 132 [25600/50000 (51%)]\tLoss: 0.661596\tAcc: 88.78%\n",
            "Train Epoch: 132 [38400/50000 (77%)]\tLoss: 0.732771\tAcc: 88.70%\n",
            "\n",
            "Test set: Average loss: 0.7426, Accuracy: 8957/10000 (89.57%)\n",
            "\n",
            "Epoch 133: Target Sparsity: 70.36%, Actual Sparsity: 70.34%\n",
            "Train Epoch: 133 [0/50000 (0%)]\tLoss: 0.776019\tAcc: 84.38%\n",
            "Train Epoch: 133 [12800/50000 (26%)]\tLoss: 0.685974\tAcc: 89.43%\n",
            "Train Epoch: 133 [25600/50000 (51%)]\tLoss: 0.779519\tAcc: 89.01%\n",
            "Train Epoch: 133 [38400/50000 (77%)]\tLoss: 0.826995\tAcc: 89.03%\n",
            "\n",
            "Test set: Average loss: 0.7555, Accuracy: 8882/10000 (88.82%)\n",
            "\n",
            "Epoch 134: Target Sparsity: 70.52%, Actual Sparsity: 70.50%\n",
            "Train Epoch: 134 [0/50000 (0%)]\tLoss: 0.708158\tAcc: 92.19%\n",
            "Train Epoch: 134 [12800/50000 (26%)]\tLoss: 0.776998\tAcc: 89.83%\n",
            "Train Epoch: 134 [25600/50000 (51%)]\tLoss: 0.645911\tAcc: 89.50%\n",
            "Train Epoch: 134 [38400/50000 (77%)]\tLoss: 0.755443\tAcc: 89.48%\n",
            "\n",
            "Test set: Average loss: 0.7346, Accuracy: 8993/10000 (89.93%)\n",
            "\n",
            "Epoch 135: Target Sparsity: 70.69%, Actual Sparsity: 70.67%\n",
            "Train Epoch: 135 [0/50000 (0%)]\tLoss: 0.710010\tAcc: 89.84%\n",
            "Train Epoch: 135 [12800/50000 (26%)]\tLoss: 0.766460\tAcc: 90.18%\n",
            "Train Epoch: 135 [25600/50000 (51%)]\tLoss: 0.659485\tAcc: 89.98%\n",
            "Train Epoch: 135 [38400/50000 (77%)]\tLoss: 0.682866\tAcc: 89.98%\n",
            "\n",
            "Test set: Average loss: 0.7544, Accuracy: 8929/10000 (89.29%)\n",
            "\n",
            "Epoch 136: Target Sparsity: 70.86%, Actual Sparsity: 70.83%\n",
            "Train Epoch: 136 [0/50000 (0%)]\tLoss: 0.725503\tAcc: 89.06%\n",
            "Train Epoch: 136 [12800/50000 (26%)]\tLoss: 0.730146\tAcc: 90.66%\n",
            "Train Epoch: 136 [25600/50000 (51%)]\tLoss: 0.788812\tAcc: 90.60%\n",
            "Train Epoch: 136 [38400/50000 (77%)]\tLoss: 0.684068\tAcc: 90.59%\n",
            "\n",
            "Test set: Average loss: 0.7111, Accuracy: 9066/10000 (90.66%)\n",
            "\n",
            "Epoch 137: Target Sparsity: 71.02%, Actual Sparsity: 71.00%\n",
            "Train Epoch: 137 [0/50000 (0%)]\tLoss: 0.743435\tAcc: 89.06%\n",
            "Train Epoch: 137 [12800/50000 (26%)]\tLoss: 0.878538\tAcc: 91.03%\n",
            "Train Epoch: 137 [25600/50000 (51%)]\tLoss: 0.679166\tAcc: 91.07%\n",
            "Train Epoch: 137 [38400/50000 (77%)]\tLoss: 0.642045\tAcc: 91.12%\n",
            "\n",
            "Test set: Average loss: 0.6941, Accuracy: 9151/10000 (91.51%)\n",
            "\n",
            "Epoch 138: Target Sparsity: 71.19%, Actual Sparsity: 71.16%\n",
            "Train Epoch: 138 [0/50000 (0%)]\tLoss: 0.608216\tAcc: 95.31%\n",
            "Train Epoch: 138 [12800/50000 (26%)]\tLoss: 0.605732\tAcc: 91.82%\n",
            "Train Epoch: 138 [25600/50000 (51%)]\tLoss: 0.652061\tAcc: 91.95%\n",
            "Train Epoch: 138 [38400/50000 (77%)]\tLoss: 0.692518\tAcc: 91.85%\n",
            "\n",
            "Test set: Average loss: 0.7009, Accuracy: 9140/10000 (91.40%)\n",
            "\n",
            "Epoch 139: Target Sparsity: 71.35%, Actual Sparsity: 71.33%\n",
            "Train Epoch: 139 [0/50000 (0%)]\tLoss: 0.768264\tAcc: 89.06%\n",
            "Train Epoch: 139 [12800/50000 (26%)]\tLoss: 0.633729\tAcc: 92.44%\n",
            "Train Epoch: 139 [25600/50000 (51%)]\tLoss: 0.668825\tAcc: 92.23%\n",
            "Train Epoch: 139 [38400/50000 (77%)]\tLoss: 0.725893\tAcc: 92.19%\n",
            "\n",
            "Test set: Average loss: 0.7016, Accuracy: 9154/10000 (91.54%)\n",
            "\n",
            "Epoch 140: Target Sparsity: 71.52%, Actual Sparsity: 71.49%\n",
            "Train Epoch: 140 [0/50000 (0%)]\tLoss: 0.696117\tAcc: 92.19%\n",
            "Train Epoch: 140 [12800/50000 (26%)]\tLoss: 0.679479\tAcc: 92.81%\n",
            "Train Epoch: 140 [25600/50000 (51%)]\tLoss: 0.622181\tAcc: 92.75%\n",
            "Train Epoch: 140 [38400/50000 (77%)]\tLoss: 0.686565\tAcc: 92.59%\n",
            "\n",
            "Test set: Average loss: 0.6756, Accuracy: 9260/10000 (92.60%)\n",
            "\n",
            "Epoch 141: Target Sparsity: 71.68%, Actual Sparsity: 71.66%\n",
            "Train Epoch: 141 [0/50000 (0%)]\tLoss: 0.718430\tAcc: 91.41%\n",
            "Train Epoch: 141 [12800/50000 (26%)]\tLoss: 0.635433\tAcc: 93.02%\n",
            "Train Epoch: 141 [25600/50000 (51%)]\tLoss: 0.644227\tAcc: 93.32%\n",
            "Train Epoch: 141 [38400/50000 (77%)]\tLoss: 0.640078\tAcc: 93.30%\n",
            "\n",
            "Test set: Average loss: 0.6708, Accuracy: 9282/10000 (92.82%)\n",
            "\n",
            "Epoch 142: Target Sparsity: 71.85%, Actual Sparsity: 71.83%\n",
            "Train Epoch: 142 [0/50000 (0%)]\tLoss: 0.644316\tAcc: 93.75%\n",
            "Train Epoch: 142 [12800/50000 (26%)]\tLoss: 0.672775\tAcc: 93.84%\n",
            "Train Epoch: 142 [25600/50000 (51%)]\tLoss: 0.603339\tAcc: 93.90%\n",
            "Train Epoch: 142 [38400/50000 (77%)]\tLoss: 0.631438\tAcc: 93.86%\n",
            "\n",
            "Test set: Average loss: 0.6737, Accuracy: 9275/10000 (92.75%)\n",
            "\n",
            "Epoch 143: Target Sparsity: 72.01%, Actual Sparsity: 71.99%\n",
            "Train Epoch: 143 [0/50000 (0%)]\tLoss: 0.666433\tAcc: 92.97%\n",
            "Train Epoch: 143 [12800/50000 (26%)]\tLoss: 0.574255\tAcc: 94.06%\n",
            "Train Epoch: 143 [25600/50000 (51%)]\tLoss: 0.662065\tAcc: 94.08%\n",
            "Train Epoch: 143 [38400/50000 (77%)]\tLoss: 0.610887\tAcc: 94.13%\n",
            "\n",
            "Test set: Average loss: 0.6724, Accuracy: 9277/10000 (92.77%)\n",
            "\n",
            "Epoch 144: Target Sparsity: 72.18%, Actual Sparsity: 72.16%\n",
            "Train Epoch: 144 [0/50000 (0%)]\tLoss: 0.584191\tAcc: 96.09%\n",
            "Train Epoch: 144 [12800/50000 (26%)]\tLoss: 0.609441\tAcc: 95.19%\n",
            "Train Epoch: 144 [25600/50000 (51%)]\tLoss: 0.574132\tAcc: 95.02%\n",
            "Train Epoch: 144 [38400/50000 (77%)]\tLoss: 0.605586\tAcc: 94.92%\n",
            "\n",
            "Test set: Average loss: 0.6662, Accuracy: 9297/10000 (92.97%)\n",
            "\n",
            "Epoch 145: Target Sparsity: 72.34%, Actual Sparsity: 72.32%\n",
            "Train Epoch: 145 [0/50000 (0%)]\tLoss: 0.627338\tAcc: 94.53%\n",
            "Train Epoch: 145 [12800/50000 (26%)]\tLoss: 0.578773\tAcc: 95.34%\n",
            "Train Epoch: 145 [25600/50000 (51%)]\tLoss: 0.582109\tAcc: 95.21%\n",
            "Train Epoch: 145 [38400/50000 (77%)]\tLoss: 0.625783\tAcc: 95.14%\n",
            "\n",
            "Test set: Average loss: 0.6616, Accuracy: 9351/10000 (93.51%)\n",
            "\n",
            "Epoch 146: Target Sparsity: 72.51%, Actual Sparsity: 72.49%\n",
            "Train Epoch: 146 [0/50000 (0%)]\tLoss: 0.600022\tAcc: 96.88%\n",
            "Train Epoch: 146 [12800/50000 (26%)]\tLoss: 0.706518\tAcc: 95.42%\n",
            "Train Epoch: 146 [25600/50000 (51%)]\tLoss: 0.562713\tAcc: 95.20%\n",
            "Train Epoch: 146 [38400/50000 (77%)]\tLoss: 0.556690\tAcc: 95.21%\n",
            "\n",
            "Test set: Average loss: 0.6557, Accuracy: 9366/10000 (93.66%)\n",
            "\n",
            "Epoch 147: Target Sparsity: 72.68%, Actual Sparsity: 72.65%\n",
            "Train Epoch: 147 [0/50000 (0%)]\tLoss: 0.549347\tAcc: 99.22%\n",
            "Train Epoch: 147 [12800/50000 (26%)]\tLoss: 0.655782\tAcc: 95.29%\n",
            "Train Epoch: 147 [25600/50000 (51%)]\tLoss: 0.595226\tAcc: 95.55%\n",
            "Train Epoch: 147 [38400/50000 (77%)]\tLoss: 0.612975\tAcc: 95.54%\n",
            "\n",
            "Test set: Average loss: 0.6566, Accuracy: 9360/10000 (93.60%)\n",
            "\n",
            "Epoch 148: Target Sparsity: 72.84%, Actual Sparsity: 72.82%\n",
            "Train Epoch: 148 [0/50000 (0%)]\tLoss: 0.579724\tAcc: 97.66%\n",
            "Train Epoch: 148 [12800/50000 (26%)]\tLoss: 0.583892\tAcc: 95.77%\n",
            "Train Epoch: 148 [25600/50000 (51%)]\tLoss: 0.591902\tAcc: 95.71%\n",
            "Train Epoch: 148 [38400/50000 (77%)]\tLoss: 0.602547\tAcc: 95.71%\n",
            "\n",
            "Test set: Average loss: 0.6565, Accuracy: 9349/10000 (93.49%)\n",
            "\n",
            "Epoch 149: Target Sparsity: 73.01%, Actual Sparsity: 72.98%\n",
            "Train Epoch: 149 [0/50000 (0%)]\tLoss: 0.559711\tAcc: 97.66%\n",
            "Train Epoch: 149 [12800/50000 (26%)]\tLoss: 0.570022\tAcc: 95.43%\n",
            "Train Epoch: 149 [25600/50000 (51%)]\tLoss: 0.561890\tAcc: 95.51%\n",
            "Train Epoch: 149 [38400/50000 (77%)]\tLoss: 0.639657\tAcc: 95.65%\n",
            "\n",
            "Test set: Average loss: 0.6553, Accuracy: 9356/10000 (93.56%)\n",
            "\n",
            "Epoch 150: Target Sparsity: 73.17%, Actual Sparsity: 73.15%\n",
            "Train Epoch: 150 [0/50000 (0%)]\tLoss: 0.588569\tAcc: 96.88%\n",
            "Train Epoch: 150 [12800/50000 (26%)]\tLoss: 0.548140\tAcc: 95.94%\n",
            "Train Epoch: 150 [25600/50000 (51%)]\tLoss: 0.600436\tAcc: 96.10%\n",
            "Train Epoch: 150 [38400/50000 (77%)]\tLoss: 0.561711\tAcc: 96.12%\n",
            "\n",
            "Test set: Average loss: 0.6559, Accuracy: 9353/10000 (93.53%)\n",
            "\n",
            "Epoch 151: Target Sparsity: 73.34%, Actual Sparsity: 73.31%\n",
            "Train Epoch: 151 [0/50000 (0%)]\tLoss: 0.614423\tAcc: 94.53%\n",
            "Train Epoch: 151 [12800/50000 (26%)]\tLoss: 0.675926\tAcc: 96.12%\n",
            "Train Epoch: 151 [25600/50000 (51%)]\tLoss: 0.573104\tAcc: 95.99%\n",
            "Train Epoch: 151 [38400/50000 (77%)]\tLoss: 0.617758\tAcc: 95.90%\n",
            "\n",
            "Test set: Average loss: 0.6551, Accuracy: 9354/10000 (93.54%)\n",
            "\n",
            "Epoch 152: Target Sparsity: 73.50%, Actual Sparsity: 73.48%\n",
            "Train Epoch: 152 [0/50000 (0%)]\tLoss: 0.529264\tAcc: 99.22%\n",
            "Train Epoch: 152 [12800/50000 (26%)]\tLoss: 0.617984\tAcc: 96.02%\n",
            "Train Epoch: 152 [25600/50000 (51%)]\tLoss: 0.562630\tAcc: 96.07%\n",
            "Train Epoch: 152 [38400/50000 (77%)]\tLoss: 0.599287\tAcc: 96.05%\n",
            "\n",
            "Test set: Average loss: 0.6565, Accuracy: 9361/10000 (93.61%)\n",
            "\n",
            "Epoch 153: Target Sparsity: 73.67%, Actual Sparsity: 73.65%\n",
            "Train Epoch: 153 [0/50000 (0%)]\tLoss: 0.525997\tAcc: 100.00%\n",
            "Train Epoch: 153 [12800/50000 (26%)]\tLoss: 0.591843\tAcc: 96.20%\n",
            "Train Epoch: 153 [25600/50000 (51%)]\tLoss: 0.597668\tAcc: 96.02%\n",
            "Train Epoch: 153 [38400/50000 (77%)]\tLoss: 0.671528\tAcc: 95.84%\n",
            "\n",
            "Test set: Average loss: 0.6561, Accuracy: 9360/10000 (93.60%)\n",
            "\n",
            "Epoch 154: Target Sparsity: 73.83%, Actual Sparsity: 73.81%\n",
            "Train Epoch: 154 [0/50000 (0%)]\tLoss: 0.583583\tAcc: 97.66%\n",
            "Train Epoch: 154 [12800/50000 (26%)]\tLoss: 0.608305\tAcc: 96.15%\n",
            "Train Epoch: 154 [25600/50000 (51%)]\tLoss: 0.602472\tAcc: 95.93%\n",
            "Train Epoch: 154 [38400/50000 (77%)]\tLoss: 0.627061\tAcc: 95.85%\n",
            "\n",
            "Test set: Average loss: 0.6558, Accuracy: 9356/10000 (93.56%)\n",
            "\n",
            "Epoch 155: Target Sparsity: 74.00%, Actual Sparsity: 73.98%\n",
            "Train Epoch: 155 [0/50000 (0%)]\tLoss: 0.608819\tAcc: 96.88%\n",
            "Train Epoch: 155 [12800/50000 (26%)]\tLoss: 0.616454\tAcc: 95.93%\n",
            "Train Epoch: 155 [25600/50000 (51%)]\tLoss: 0.583292\tAcc: 95.95%\n",
            "Train Epoch: 155 [38400/50000 (77%)]\tLoss: 0.607222\tAcc: 95.91%\n",
            "\n",
            "Test set: Average loss: 0.6561, Accuracy: 9342/10000 (93.42%)\n",
            "\n",
            "Epoch 156: Target Sparsity: 74.17%, Actual Sparsity: 74.14%\n",
            "Train Epoch: 156 [0/50000 (0%)]\tLoss: 0.613276\tAcc: 93.75%\n",
            "Train Epoch: 156 [12800/50000 (26%)]\tLoss: 0.631315\tAcc: 95.82%\n",
            "Train Epoch: 156 [25600/50000 (51%)]\tLoss: 0.614129\tAcc: 95.76%\n",
            "Train Epoch: 156 [38400/50000 (77%)]\tLoss: 0.574337\tAcc: 95.72%\n",
            "\n",
            "Test set: Average loss: 0.6600, Accuracy: 9339/10000 (93.39%)\n",
            "\n",
            "Epoch 157: Target Sparsity: 74.33%, Actual Sparsity: 74.31%\n",
            "Train Epoch: 157 [0/50000 (0%)]\tLoss: 0.649110\tAcc: 92.97%\n",
            "Train Epoch: 157 [12800/50000 (26%)]\tLoss: 0.581307\tAcc: 95.82%\n",
            "Train Epoch: 157 [25600/50000 (51%)]\tLoss: 0.578928\tAcc: 95.78%\n",
            "Train Epoch: 157 [38400/50000 (77%)]\tLoss: 0.560447\tAcc: 95.67%\n",
            "\n",
            "Test set: Average loss: 0.6638, Accuracy: 9350/10000 (93.50%)\n",
            "\n",
            "Epoch 158: Target Sparsity: 74.50%, Actual Sparsity: 74.47%\n",
            "Train Epoch: 158 [0/50000 (0%)]\tLoss: 0.652759\tAcc: 92.97%\n",
            "Train Epoch: 158 [12800/50000 (26%)]\tLoss: 0.640056\tAcc: 95.61%\n",
            "Train Epoch: 158 [25600/50000 (51%)]\tLoss: 0.613762\tAcc: 95.44%\n",
            "Train Epoch: 158 [38400/50000 (77%)]\tLoss: 0.624838\tAcc: 95.48%\n",
            "\n",
            "Test set: Average loss: 0.6681, Accuracy: 9328/10000 (93.28%)\n",
            "\n",
            "Epoch 159: Target Sparsity: 74.66%, Actual Sparsity: 74.64%\n",
            "Train Epoch: 159 [0/50000 (0%)]\tLoss: 0.574555\tAcc: 96.09%\n",
            "Train Epoch: 159 [12800/50000 (26%)]\tLoss: 0.610045\tAcc: 95.30%\n",
            "Train Epoch: 159 [25600/50000 (51%)]\tLoss: 0.631540\tAcc: 95.39%\n",
            "Train Epoch: 159 [38400/50000 (77%)]\tLoss: 0.552457\tAcc: 95.34%\n",
            "\n",
            "Test set: Average loss: 0.6708, Accuracy: 9308/10000 (93.08%)\n",
            "\n",
            "Epoch 160: Target Sparsity: 74.83%, Actual Sparsity: 74.80%\n",
            "Train Epoch: 160 [0/50000 (0%)]\tLoss: 0.622509\tAcc: 94.53%\n",
            "Train Epoch: 160 [12800/50000 (26%)]\tLoss: 0.644896\tAcc: 95.60%\n",
            "Train Epoch: 160 [25600/50000 (51%)]\tLoss: 0.619555\tAcc: 95.24%\n",
            "Train Epoch: 160 [38400/50000 (77%)]\tLoss: 0.608376\tAcc: 95.00%\n",
            "\n",
            "Test set: Average loss: 0.6819, Accuracy: 9282/10000 (92.82%)\n",
            "\n",
            "Epoch 161: Target Sparsity: 74.99%, Actual Sparsity: 74.97%\n",
            "Train Epoch: 161 [0/50000 (0%)]\tLoss: 0.635286\tAcc: 93.75%\n",
            "Train Epoch: 161 [12800/50000 (26%)]\tLoss: 0.586358\tAcc: 94.87%\n",
            "Train Epoch: 161 [25600/50000 (51%)]\tLoss: 0.652615\tAcc: 94.67%\n",
            "Train Epoch: 161 [38400/50000 (77%)]\tLoss: 0.601526\tAcc: 94.58%\n",
            "\n",
            "Test set: Average loss: 0.6697, Accuracy: 9292/10000 (92.92%)\n",
            "\n",
            "Epoch 162: Target Sparsity: 75.16%, Actual Sparsity: 75.13%\n",
            "Train Epoch: 162 [0/50000 (0%)]\tLoss: 0.578761\tAcc: 94.53%\n",
            "Train Epoch: 162 [12800/50000 (26%)]\tLoss: 0.746428\tAcc: 94.31%\n",
            "Train Epoch: 162 [25600/50000 (51%)]\tLoss: 0.598758\tAcc: 94.18%\n",
            "Train Epoch: 162 [38400/50000 (77%)]\tLoss: 0.686618\tAcc: 94.00%\n",
            "\n",
            "Test set: Average loss: 0.6864, Accuracy: 9217/10000 (92.17%)\n",
            "\n",
            "Epoch 163: Target Sparsity: 75.32%, Actual Sparsity: 75.30%\n",
            "Train Epoch: 163 [0/50000 (0%)]\tLoss: 0.574336\tAcc: 94.53%\n",
            "Train Epoch: 163 [12800/50000 (26%)]\tLoss: 0.626082\tAcc: 93.93%\n",
            "Train Epoch: 163 [25600/50000 (51%)]\tLoss: 0.611954\tAcc: 93.63%\n",
            "Train Epoch: 163 [38400/50000 (77%)]\tLoss: 0.737694\tAcc: 93.57%\n",
            "\n",
            "Test set: Average loss: 0.7034, Accuracy: 9161/10000 (91.61%)\n",
            "\n",
            "Epoch 164: Target Sparsity: 75.49%, Actual Sparsity: 75.47%\n",
            "Train Epoch: 164 [0/50000 (0%)]\tLoss: 0.625801\tAcc: 95.31%\n",
            "Train Epoch: 164 [12800/50000 (26%)]\tLoss: 0.707036\tAcc: 93.24%\n",
            "Train Epoch: 164 [25600/50000 (51%)]\tLoss: 0.723818\tAcc: 93.20%\n",
            "Train Epoch: 164 [38400/50000 (77%)]\tLoss: 0.641014\tAcc: 92.95%\n",
            "\n",
            "Test set: Average loss: 0.7069, Accuracy: 9165/10000 (91.65%)\n",
            "\n",
            "Epoch 165: Target Sparsity: 75.66%, Actual Sparsity: 75.63%\n",
            "Train Epoch: 165 [0/50000 (0%)]\tLoss: 0.693374\tAcc: 92.19%\n",
            "Train Epoch: 165 [12800/50000 (26%)]\tLoss: 0.614697\tAcc: 92.69%\n",
            "Train Epoch: 165 [25600/50000 (51%)]\tLoss: 0.632891\tAcc: 92.52%\n",
            "Train Epoch: 165 [38400/50000 (77%)]\tLoss: 0.674931\tAcc: 92.36%\n",
            "\n",
            "Test set: Average loss: 0.7210, Accuracy: 9055/10000 (90.55%)\n",
            "\n",
            "Epoch 166: Target Sparsity: 75.82%, Actual Sparsity: 75.80%\n",
            "Train Epoch: 166 [0/50000 (0%)]\tLoss: 0.677671\tAcc: 91.41%\n",
            "Train Epoch: 166 [12800/50000 (26%)]\tLoss: 0.634067\tAcc: 92.30%\n",
            "Train Epoch: 166 [25600/50000 (51%)]\tLoss: 0.721235\tAcc: 91.82%\n",
            "Train Epoch: 166 [38400/50000 (77%)]\tLoss: 0.810996\tAcc: 91.75%\n",
            "\n",
            "Test set: Average loss: 0.7304, Accuracy: 9012/10000 (90.12%)\n",
            "\n",
            "Epoch 167: Target Sparsity: 75.99%, Actual Sparsity: 75.96%\n",
            "Train Epoch: 167 [0/50000 (0%)]\tLoss: 0.690394\tAcc: 92.19%\n",
            "Train Epoch: 167 [12800/50000 (26%)]\tLoss: 0.780494\tAcc: 91.02%\n",
            "Train Epoch: 167 [25600/50000 (51%)]\tLoss: 0.707759\tAcc: 90.90%\n",
            "Train Epoch: 167 [38400/50000 (77%)]\tLoss: 0.724979\tAcc: 90.90%\n",
            "\n",
            "Test set: Average loss: 0.7373, Accuracy: 9001/10000 (90.01%)\n",
            "\n",
            "Epoch 168: Target Sparsity: 76.15%, Actual Sparsity: 76.13%\n",
            "Train Epoch: 168 [0/50000 (0%)]\tLoss: 0.656352\tAcc: 92.19%\n",
            "Train Epoch: 168 [12800/50000 (26%)]\tLoss: 0.732647\tAcc: 90.22%\n",
            "Train Epoch: 168 [25600/50000 (51%)]\tLoss: 0.737172\tAcc: 90.11%\n",
            "Train Epoch: 168 [38400/50000 (77%)]\tLoss: 0.753277\tAcc: 89.99%\n",
            "\n",
            "Test set: Average loss: 0.7472, Accuracy: 8961/10000 (89.61%)\n",
            "\n",
            "Epoch 169: Target Sparsity: 76.32%, Actual Sparsity: 76.29%\n",
            "Train Epoch: 169 [0/50000 (0%)]\tLoss: 0.794891\tAcc: 85.16%\n",
            "Train Epoch: 169 [12800/50000 (26%)]\tLoss: 0.689707\tAcc: 90.14%\n",
            "Train Epoch: 169 [25600/50000 (51%)]\tLoss: 0.768344\tAcc: 89.60%\n",
            "Train Epoch: 169 [38400/50000 (77%)]\tLoss: 0.795901\tAcc: 89.36%\n",
            "\n",
            "Test set: Average loss: 0.7421, Accuracy: 8977/10000 (89.77%)\n",
            "\n",
            "Epoch 170: Target Sparsity: 76.48%, Actual Sparsity: 76.46%\n",
            "Train Epoch: 170 [0/50000 (0%)]\tLoss: 0.753725\tAcc: 90.62%\n",
            "Train Epoch: 170 [12800/50000 (26%)]\tLoss: 0.674116\tAcc: 89.35%\n",
            "Train Epoch: 170 [25600/50000 (51%)]\tLoss: 0.716976\tAcc: 88.94%\n",
            "Train Epoch: 170 [38400/50000 (77%)]\tLoss: 0.707765\tAcc: 88.72%\n",
            "\n",
            "Test set: Average loss: 0.7632, Accuracy: 8873/10000 (88.73%)\n",
            "\n",
            "Epoch 171: Target Sparsity: 76.65%, Actual Sparsity: 76.62%\n",
            "Train Epoch: 171 [0/50000 (0%)]\tLoss: 0.684229\tAcc: 90.62%\n",
            "Train Epoch: 171 [12800/50000 (26%)]\tLoss: 0.732849\tAcc: 88.35%\n",
            "Train Epoch: 171 [25600/50000 (51%)]\tLoss: 0.820979\tAcc: 87.88%\n",
            "Train Epoch: 171 [38400/50000 (77%)]\tLoss: 0.792689\tAcc: 87.77%\n",
            "\n",
            "Test set: Average loss: 0.7437, Accuracy: 8978/10000 (89.78%)\n",
            "\n",
            "Epoch 172: Target Sparsity: 76.81%, Actual Sparsity: 76.79%\n",
            "Train Epoch: 172 [0/50000 (0%)]\tLoss: 0.774110\tAcc: 88.28%\n",
            "Train Epoch: 172 [12800/50000 (26%)]\tLoss: 0.797382\tAcc: 86.43%\n",
            "Train Epoch: 172 [25600/50000 (51%)]\tLoss: 0.799807\tAcc: 86.80%\n",
            "Train Epoch: 172 [38400/50000 (77%)]\tLoss: 0.823858\tAcc: 86.98%\n",
            "\n",
            "Test set: Average loss: 0.7798, Accuracy: 8795/10000 (87.95%)\n",
            "\n",
            "Epoch 173: Target Sparsity: 76.98%, Actual Sparsity: 76.95%\n",
            "Train Epoch: 173 [0/50000 (0%)]\tLoss: 0.812210\tAcc: 85.16%\n",
            "Train Epoch: 173 [12800/50000 (26%)]\tLoss: 0.735354\tAcc: 86.46%\n",
            "Train Epoch: 173 [25600/50000 (51%)]\tLoss: 0.829064\tAcc: 86.39%\n",
            "Train Epoch: 173 [38400/50000 (77%)]\tLoss: 0.764522\tAcc: 86.49%\n",
            "\n",
            "Test set: Average loss: 0.8375, Accuracy: 8582/10000 (85.82%)\n",
            "\n",
            "Epoch 174: Target Sparsity: 77.14%, Actual Sparsity: 77.12%\n",
            "Train Epoch: 174 [0/50000 (0%)]\tLoss: 0.750674\tAcc: 90.62%\n",
            "Train Epoch: 174 [12800/50000 (26%)]\tLoss: 0.797163\tAcc: 86.70%\n",
            "Train Epoch: 174 [25600/50000 (51%)]\tLoss: 0.811022\tAcc: 86.37%\n",
            "Train Epoch: 174 [38400/50000 (77%)]\tLoss: 0.857593\tAcc: 85.96%\n",
            "\n",
            "Test set: Average loss: 0.9176, Accuracy: 8256/10000 (82.56%)\n",
            "\n",
            "Epoch 175: Target Sparsity: 77.31%, Actual Sparsity: 77.29%\n",
            "Train Epoch: 175 [0/50000 (0%)]\tLoss: 0.894963\tAcc: 82.03%\n",
            "Train Epoch: 175 [12800/50000 (26%)]\tLoss: 0.915070\tAcc: 85.56%\n",
            "Train Epoch: 175 [25600/50000 (51%)]\tLoss: 0.785309\tAcc: 85.40%\n",
            "Train Epoch: 175 [38400/50000 (77%)]\tLoss: 0.841019\tAcc: 85.20%\n",
            "\n",
            "Test set: Average loss: 0.8292, Accuracy: 8571/10000 (85.71%)\n",
            "\n",
            "Epoch 176: Target Sparsity: 77.48%, Actual Sparsity: 77.45%\n",
            "Train Epoch: 176 [0/50000 (0%)]\tLoss: 0.864766\tAcc: 83.59%\n",
            "Train Epoch: 176 [12800/50000 (26%)]\tLoss: 0.742677\tAcc: 85.55%\n",
            "Train Epoch: 176 [25600/50000 (51%)]\tLoss: 1.001453\tAcc: 85.18%\n",
            "Train Epoch: 176 [38400/50000 (77%)]\tLoss: 0.849150\tAcc: 85.03%\n",
            "\n",
            "Test set: Average loss: 0.8530, Accuracy: 8493/10000 (84.93%)\n",
            "\n",
            "Epoch 177: Target Sparsity: 77.64%, Actual Sparsity: 77.62%\n",
            "Train Epoch: 177 [0/50000 (0%)]\tLoss: 0.839509\tAcc: 85.16%\n",
            "Train Epoch: 177 [12800/50000 (26%)]\tLoss: 0.853418\tAcc: 84.58%\n",
            "Train Epoch: 177 [25600/50000 (51%)]\tLoss: 0.993452\tAcc: 84.47%\n",
            "Train Epoch: 177 [38400/50000 (77%)]\tLoss: 0.881933\tAcc: 84.23%\n",
            "\n",
            "Test set: Average loss: 0.7999, Accuracy: 8710/10000 (87.10%)\n",
            "\n",
            "Epoch 178: Target Sparsity: 77.81%, Actual Sparsity: 77.78%\n",
            "Train Epoch: 178 [0/50000 (0%)]\tLoss: 0.793415\tAcc: 85.16%\n",
            "Train Epoch: 178 [12800/50000 (26%)]\tLoss: 0.946166\tAcc: 84.65%\n",
            "Train Epoch: 178 [25600/50000 (51%)]\tLoss: 0.990349\tAcc: 84.25%\n",
            "Train Epoch: 178 [38400/50000 (77%)]\tLoss: 0.846014\tAcc: 84.37%\n",
            "\n",
            "Test set: Average loss: 0.9545, Accuracy: 8139/10000 (81.39%)\n",
            "\n",
            "Epoch 179: Target Sparsity: 77.97%, Actual Sparsity: 77.95%\n",
            "Train Epoch: 179 [0/50000 (0%)]\tLoss: 0.804024\tAcc: 86.72%\n",
            "Train Epoch: 179 [12800/50000 (26%)]\tLoss: 0.871265\tAcc: 83.57%\n",
            "Train Epoch: 179 [25600/50000 (51%)]\tLoss: 0.905985\tAcc: 83.73%\n",
            "Train Epoch: 179 [38400/50000 (77%)]\tLoss: 0.810644\tAcc: 83.88%\n",
            "\n",
            "Test set: Average loss: 0.9205, Accuracy: 8224/10000 (82.24%)\n",
            "\n",
            "Epoch 180: Target Sparsity: 78.14%, Actual Sparsity: 78.11%\n",
            "Train Epoch: 180 [0/50000 (0%)]\tLoss: 0.821729\tAcc: 87.50%\n",
            "Train Epoch: 180 [12800/50000 (26%)]\tLoss: 0.788752\tAcc: 83.92%\n",
            "Train Epoch: 180 [25600/50000 (51%)]\tLoss: 0.857469\tAcc: 83.61%\n",
            "Train Epoch: 180 [38400/50000 (77%)]\tLoss: 1.021564\tAcc: 83.58%\n",
            "\n",
            "Test set: Average loss: 0.9352, Accuracy: 8197/10000 (81.97%)\n",
            "\n",
            "Epoch 181: Target Sparsity: 78.30%, Actual Sparsity: 78.28%\n",
            "Train Epoch: 181 [0/50000 (0%)]\tLoss: 0.832644\tAcc: 87.50%\n",
            "Train Epoch: 181 [12800/50000 (26%)]\tLoss: 0.900635\tAcc: 83.76%\n",
            "Train Epoch: 181 [25600/50000 (51%)]\tLoss: 0.832796\tAcc: 83.45%\n",
            "Train Epoch: 181 [38400/50000 (77%)]\tLoss: 0.902837\tAcc: 83.29%\n",
            "\n",
            "Test set: Average loss: 0.8505, Accuracy: 8458/10000 (84.58%)\n",
            "\n",
            "Epoch 182: Target Sparsity: 78.47%, Actual Sparsity: 78.44%\n",
            "Train Epoch: 182 [0/50000 (0%)]\tLoss: 0.880736\tAcc: 81.25%\n",
            "Train Epoch: 182 [12800/50000 (26%)]\tLoss: 0.929493\tAcc: 83.11%\n",
            "Train Epoch: 182 [25600/50000 (51%)]\tLoss: 0.856541\tAcc: 83.05%\n",
            "Train Epoch: 182 [38400/50000 (77%)]\tLoss: 0.805872\tAcc: 83.08%\n",
            "\n",
            "Test set: Average loss: 0.8731, Accuracy: 8421/10000 (84.21%)\n",
            "\n",
            "Epoch 183: Target Sparsity: 78.63%, Actual Sparsity: 78.61%\n",
            "Train Epoch: 183 [0/50000 (0%)]\tLoss: 0.874249\tAcc: 85.94%\n",
            "Train Epoch: 183 [12800/50000 (26%)]\tLoss: 0.950789\tAcc: 82.83%\n",
            "Train Epoch: 183 [25600/50000 (51%)]\tLoss: 0.839110\tAcc: 82.70%\n",
            "Train Epoch: 183 [38400/50000 (77%)]\tLoss: 0.956710\tAcc: 82.70%\n",
            "\n",
            "Test set: Average loss: 0.9041, Accuracy: 8211/10000 (82.11%)\n",
            "\n",
            "Epoch 184: Target Sparsity: 78.80%, Actual Sparsity: 78.77%\n",
            "Train Epoch: 184 [0/50000 (0%)]\tLoss: 0.847630\tAcc: 84.38%\n",
            "Train Epoch: 184 [12800/50000 (26%)]\tLoss: 0.838244\tAcc: 83.24%\n",
            "Train Epoch: 184 [25600/50000 (51%)]\tLoss: 0.819797\tAcc: 82.61%\n",
            "Train Epoch: 184 [38400/50000 (77%)]\tLoss: 0.897184\tAcc: 82.62%\n",
            "\n",
            "Test set: Average loss: 0.8417, Accuracy: 8548/10000 (85.48%)\n",
            "\n",
            "Epoch 185: Target Sparsity: 78.97%, Actual Sparsity: 78.94%\n",
            "Train Epoch: 185 [0/50000 (0%)]\tLoss: 0.859800\tAcc: 85.16%\n",
            "Train Epoch: 185 [12800/50000 (26%)]\tLoss: 0.953762\tAcc: 82.51%\n",
            "Train Epoch: 185 [25600/50000 (51%)]\tLoss: 0.887377\tAcc: 82.59%\n",
            "Train Epoch: 185 [38400/50000 (77%)]\tLoss: 0.914297\tAcc: 82.50%\n",
            "\n",
            "Test set: Average loss: 0.8435, Accuracy: 8535/10000 (85.35%)\n",
            "\n",
            "Epoch 186: Target Sparsity: 79.13%, Actual Sparsity: 79.11%\n",
            "Train Epoch: 186 [0/50000 (0%)]\tLoss: 0.871994\tAcc: 83.59%\n",
            "Train Epoch: 186 [12800/50000 (26%)]\tLoss: 0.982932\tAcc: 82.62%\n",
            "Train Epoch: 186 [25600/50000 (51%)]\tLoss: 1.043987\tAcc: 82.12%\n",
            "Train Epoch: 186 [38400/50000 (77%)]\tLoss: 0.945007\tAcc: 82.22%\n",
            "\n",
            "Test set: Average loss: 0.8697, Accuracy: 8408/10000 (84.08%)\n",
            "\n",
            "Epoch 187: Target Sparsity: 79.30%, Actual Sparsity: 79.27%\n",
            "Train Epoch: 187 [0/50000 (0%)]\tLoss: 1.071535\tAcc: 78.91%\n",
            "Train Epoch: 187 [12800/50000 (26%)]\tLoss: 0.974276\tAcc: 82.50%\n",
            "Train Epoch: 187 [25600/50000 (51%)]\tLoss: 0.863489\tAcc: 82.31%\n",
            "Train Epoch: 187 [38400/50000 (77%)]\tLoss: 0.993046\tAcc: 82.22%\n",
            "\n",
            "Test set: Average loss: 0.9073, Accuracy: 8239/10000 (82.39%)\n",
            "\n",
            "Epoch 188: Target Sparsity: 79.46%, Actual Sparsity: 79.44%\n",
            "Train Epoch: 188 [0/50000 (0%)]\tLoss: 0.918969\tAcc: 82.81%\n",
            "Train Epoch: 188 [12800/50000 (26%)]\tLoss: 0.977263\tAcc: 81.83%\n",
            "Train Epoch: 188 [25600/50000 (51%)]\tLoss: 0.889866\tAcc: 81.67%\n",
            "Train Epoch: 188 [38400/50000 (77%)]\tLoss: 0.931389\tAcc: 81.81%\n",
            "\n",
            "Test set: Average loss: 0.9583, Accuracy: 8097/10000 (80.97%)\n",
            "\n",
            "Epoch 189: Target Sparsity: 79.63%, Actual Sparsity: 79.60%\n",
            "Train Epoch: 189 [0/50000 (0%)]\tLoss: 0.930404\tAcc: 83.59%\n",
            "Train Epoch: 189 [12800/50000 (26%)]\tLoss: 1.021455\tAcc: 81.94%\n",
            "Train Epoch: 189 [25600/50000 (51%)]\tLoss: 0.928132\tAcc: 82.03%\n",
            "Train Epoch: 189 [38400/50000 (77%)]\tLoss: 0.992606\tAcc: 82.03%\n",
            "\n",
            "Test set: Average loss: 0.9615, Accuracy: 8075/10000 (80.75%)\n",
            "\n",
            "Epoch 190: Target Sparsity: 79.79%, Actual Sparsity: 79.77%\n",
            "Train Epoch: 190 [0/50000 (0%)]\tLoss: 0.816847\tAcc: 87.50%\n",
            "Train Epoch: 190 [12800/50000 (26%)]\tLoss: 0.898455\tAcc: 82.05%\n",
            "Train Epoch: 190 [25600/50000 (51%)]\tLoss: 0.985469\tAcc: 81.98%\n",
            "Train Epoch: 190 [38400/50000 (77%)]\tLoss: 0.922015\tAcc: 81.68%\n",
            "\n",
            "Test set: Average loss: 0.9646, Accuracy: 8007/10000 (80.07%)\n",
            "\n",
            "Epoch 191: Target Sparsity: 79.96%, Actual Sparsity: 79.93%\n",
            "Train Epoch: 191 [0/50000 (0%)]\tLoss: 0.888895\tAcc: 84.38%\n",
            "Train Epoch: 191 [12800/50000 (26%)]\tLoss: 1.011286\tAcc: 81.21%\n",
            "Train Epoch: 191 [25600/50000 (51%)]\tLoss: 0.963894\tAcc: 81.20%\n",
            "Train Epoch: 191 [38400/50000 (77%)]\tLoss: 0.996420\tAcc: 81.48%\n",
            "\n",
            "Test set: Average loss: 0.9505, Accuracy: 8152/10000 (81.52%)\n",
            "\n",
            "Epoch 192: Target Sparsity: 80.12%, Actual Sparsity: 80.10%\n",
            "Train Epoch: 192 [0/50000 (0%)]\tLoss: 0.805436\tAcc: 89.06%\n",
            "Train Epoch: 192 [12800/50000 (26%)]\tLoss: 0.797438\tAcc: 81.59%\n",
            "Train Epoch: 192 [25600/50000 (51%)]\tLoss: 0.821646\tAcc: 81.77%\n",
            "Train Epoch: 192 [38400/50000 (77%)]\tLoss: 0.942314\tAcc: 81.44%\n",
            "\n",
            "Test set: Average loss: 0.9000, Accuracy: 8276/10000 (82.76%)\n",
            "\n",
            "Epoch 193: Target Sparsity: 80.29%, Actual Sparsity: 80.26%\n",
            "Train Epoch: 193 [0/50000 (0%)]\tLoss: 0.929842\tAcc: 82.81%\n",
            "Train Epoch: 193 [12800/50000 (26%)]\tLoss: 0.964753\tAcc: 82.05%\n",
            "Train Epoch: 193 [25600/50000 (51%)]\tLoss: 1.026172\tAcc: 81.59%\n",
            "Train Epoch: 193 [38400/50000 (77%)]\tLoss: 0.948994\tAcc: 81.47%\n",
            "\n",
            "Test set: Average loss: 0.9284, Accuracy: 8169/10000 (81.69%)\n",
            "\n",
            "Epoch 194: Target Sparsity: 80.46%, Actual Sparsity: 80.43%\n",
            "Train Epoch: 194 [0/50000 (0%)]\tLoss: 0.954385\tAcc: 81.25%\n",
            "Train Epoch: 194 [12800/50000 (26%)]\tLoss: 1.203288\tAcc: 81.73%\n",
            "Train Epoch: 194 [25600/50000 (51%)]\tLoss: 0.876034\tAcc: 81.39%\n",
            "Train Epoch: 194 [38400/50000 (77%)]\tLoss: 1.014827\tAcc: 81.04%\n",
            "\n",
            "Test set: Average loss: 0.8792, Accuracy: 8400/10000 (84.00%)\n",
            "\n",
            "Epoch 195: Target Sparsity: 80.62%, Actual Sparsity: 80.60%\n",
            "Train Epoch: 195 [0/50000 (0%)]\tLoss: 0.974667\tAcc: 75.00%\n",
            "Train Epoch: 195 [12800/50000 (26%)]\tLoss: 0.922444\tAcc: 81.50%\n",
            "Train Epoch: 195 [25600/50000 (51%)]\tLoss: 1.086764\tAcc: 81.08%\n",
            "Train Epoch: 195 [38400/50000 (77%)]\tLoss: 0.937050\tAcc: 81.17%\n",
            "\n",
            "Test set: Average loss: 0.9173, Accuracy: 8267/10000 (82.67%)\n",
            "\n",
            "Epoch 196: Target Sparsity: 80.79%, Actual Sparsity: 80.76%\n",
            "Train Epoch: 196 [0/50000 (0%)]\tLoss: 0.954524\tAcc: 77.34%\n",
            "Train Epoch: 196 [12800/50000 (26%)]\tLoss: 0.959601\tAcc: 81.51%\n",
            "Train Epoch: 196 [25600/50000 (51%)]\tLoss: 0.878038\tAcc: 81.34%\n",
            "Train Epoch: 196 [38400/50000 (77%)]\tLoss: 0.918844\tAcc: 81.47%\n",
            "\n",
            "Test set: Average loss: 0.9996, Accuracy: 7892/10000 (78.92%)\n",
            "\n",
            "Epoch 197: Target Sparsity: 80.95%, Actual Sparsity: 80.93%\n",
            "Train Epoch: 197 [0/50000 (0%)]\tLoss: 0.856622\tAcc: 84.38%\n",
            "Train Epoch: 197 [12800/50000 (26%)]\tLoss: 0.900419\tAcc: 81.59%\n",
            "Train Epoch: 197 [25600/50000 (51%)]\tLoss: 0.873871\tAcc: 81.13%\n",
            "Train Epoch: 197 [38400/50000 (77%)]\tLoss: 0.912545\tAcc: 81.10%\n",
            "\n",
            "Test set: Average loss: 0.9380, Accuracy: 8195/10000 (81.95%)\n",
            "\n",
            "Epoch 198: Target Sparsity: 81.12%, Actual Sparsity: 81.09%\n",
            "Train Epoch: 198 [0/50000 (0%)]\tLoss: 1.016046\tAcc: 79.69%\n",
            "Train Epoch: 198 [12800/50000 (26%)]\tLoss: 0.855855\tAcc: 81.29%\n",
            "Train Epoch: 198 [25600/50000 (51%)]\tLoss: 0.861649\tAcc: 81.18%\n",
            "Train Epoch: 198 [38400/50000 (77%)]\tLoss: 0.859981\tAcc: 80.99%\n",
            "\n",
            "Test set: Average loss: 0.8755, Accuracy: 8392/10000 (83.92%)\n",
            "\n",
            "Epoch 199: Target Sparsity: 81.28%, Actual Sparsity: 81.26%\n",
            "Train Epoch: 199 [0/50000 (0%)]\tLoss: 0.958841\tAcc: 80.47%\n",
            "Train Epoch: 199 [12800/50000 (26%)]\tLoss: 0.950811\tAcc: 81.15%\n",
            "Train Epoch: 199 [25600/50000 (51%)]\tLoss: 0.929206\tAcc: 81.28%\n",
            "Train Epoch: 199 [38400/50000 (77%)]\tLoss: 0.930974\tAcc: 81.37%\n",
            "\n",
            "Test set: Average loss: 0.9500, Accuracy: 8033/10000 (80.33%)\n",
            "\n",
            "Epoch 200: Target Sparsity: 81.45%, Actual Sparsity: 81.42%\n",
            "Train Epoch: 200 [0/50000 (0%)]\tLoss: 0.915449\tAcc: 83.59%\n",
            "Train Epoch: 200 [12800/50000 (26%)]\tLoss: 0.857954\tAcc: 82.02%\n",
            "Train Epoch: 200 [25600/50000 (51%)]\tLoss: 0.938553\tAcc: 81.77%\n",
            "Train Epoch: 200 [38400/50000 (77%)]\tLoss: 0.932930\tAcc: 81.31%\n",
            "\n",
            "Test set: Average loss: 0.9832, Accuracy: 7981/10000 (79.81%)\n",
            "\n",
            "Epoch 201: Target Sparsity: 81.61%, Actual Sparsity: 81.59%\n",
            "Train Epoch: 201 [0/50000 (0%)]\tLoss: 0.863547\tAcc: 84.38%\n",
            "Train Epoch: 201 [12800/50000 (26%)]\tLoss: 0.894176\tAcc: 81.50%\n",
            "Train Epoch: 201 [25600/50000 (51%)]\tLoss: 1.019987\tAcc: 81.28%\n",
            "Train Epoch: 201 [38400/50000 (77%)]\tLoss: 0.918755\tAcc: 81.16%\n",
            "\n",
            "Test set: Average loss: 0.9441, Accuracy: 8132/10000 (81.32%)\n",
            "\n",
            "Epoch 202: Target Sparsity: 81.78%, Actual Sparsity: 81.75%\n",
            "Train Epoch: 202 [0/50000 (0%)]\tLoss: 0.858640\tAcc: 85.16%\n",
            "Train Epoch: 202 [12800/50000 (26%)]\tLoss: 1.010423\tAcc: 81.56%\n",
            "Train Epoch: 202 [25600/50000 (51%)]\tLoss: 0.994405\tAcc: 81.48%\n",
            "Train Epoch: 202 [38400/50000 (77%)]\tLoss: 0.966645\tAcc: 81.15%\n",
            "\n",
            "Test set: Average loss: 0.9784, Accuracy: 7890/10000 (78.90%)\n",
            "\n",
            "Epoch 203: Target Sparsity: 81.94%, Actual Sparsity: 81.92%\n",
            "Train Epoch: 203 [0/50000 (0%)]\tLoss: 0.952809\tAcc: 75.78%\n",
            "Train Epoch: 203 [12800/50000 (26%)]\tLoss: 0.906870\tAcc: 81.90%\n",
            "Train Epoch: 203 [25600/50000 (51%)]\tLoss: 1.024864\tAcc: 81.75%\n",
            "Train Epoch: 203 [38400/50000 (77%)]\tLoss: 0.861643\tAcc: 81.60%\n",
            "\n",
            "Test set: Average loss: 0.9620, Accuracy: 8026/10000 (80.26%)\n",
            "\n",
            "Epoch 204: Target Sparsity: 82.11%, Actual Sparsity: 82.08%\n",
            "Train Epoch: 204 [0/50000 (0%)]\tLoss: 0.924867\tAcc: 82.03%\n",
            "Train Epoch: 204 [12800/50000 (26%)]\tLoss: 0.919790\tAcc: 81.61%\n",
            "Train Epoch: 204 [25600/50000 (51%)]\tLoss: 0.914509\tAcc: 81.43%\n",
            "Train Epoch: 204 [38400/50000 (77%)]\tLoss: 0.925953\tAcc: 81.69%\n",
            "\n",
            "Test set: Average loss: 0.9782, Accuracy: 8014/10000 (80.14%)\n",
            "\n",
            "Epoch 205: Target Sparsity: 82.28%, Actual Sparsity: 82.25%\n",
            "Train Epoch: 205 [0/50000 (0%)]\tLoss: 0.952271\tAcc: 78.12%\n",
            "Train Epoch: 205 [12800/50000 (26%)]\tLoss: 0.859647\tAcc: 81.72%\n",
            "Train Epoch: 205 [25600/50000 (51%)]\tLoss: 0.955501\tAcc: 81.95%\n",
            "Train Epoch: 205 [38400/50000 (77%)]\tLoss: 0.977323\tAcc: 81.65%\n",
            "\n",
            "Test set: Average loss: 0.9442, Accuracy: 8127/10000 (81.27%)\n",
            "\n",
            "Epoch 206: Target Sparsity: 82.44%, Actual Sparsity: 82.42%\n",
            "Train Epoch: 206 [0/50000 (0%)]\tLoss: 0.907623\tAcc: 84.38%\n",
            "Train Epoch: 206 [12800/50000 (26%)]\tLoss: 0.914332\tAcc: 81.64%\n",
            "Train Epoch: 206 [25600/50000 (51%)]\tLoss: 0.858308\tAcc: 81.60%\n",
            "Train Epoch: 206 [38400/50000 (77%)]\tLoss: 0.906606\tAcc: 81.75%\n",
            "\n",
            "Test set: Average loss: 0.9096, Accuracy: 8269/10000 (82.69%)\n",
            "\n",
            "Epoch 207: Target Sparsity: 82.61%, Actual Sparsity: 82.58%\n",
            "Train Epoch: 207 [0/50000 (0%)]\tLoss: 0.890173\tAcc: 84.38%\n",
            "Train Epoch: 207 [12800/50000 (26%)]\tLoss: 0.776415\tAcc: 81.91%\n",
            "Train Epoch: 207 [25600/50000 (51%)]\tLoss: 0.914402\tAcc: 81.90%\n",
            "Train Epoch: 207 [38400/50000 (77%)]\tLoss: 1.004365\tAcc: 81.99%\n",
            "\n",
            "Test set: Average loss: 0.8917, Accuracy: 8333/10000 (83.33%)\n",
            "\n",
            "Epoch 208: Target Sparsity: 82.77%, Actual Sparsity: 82.75%\n",
            "Train Epoch: 208 [0/50000 (0%)]\tLoss: 0.892560\tAcc: 82.81%\n",
            "Train Epoch: 208 [12800/50000 (26%)]\tLoss: 0.997693\tAcc: 81.34%\n",
            "Train Epoch: 208 [25600/50000 (51%)]\tLoss: 1.023988\tAcc: 81.90%\n",
            "Train Epoch: 208 [38400/50000 (77%)]\tLoss: 0.879995\tAcc: 81.72%\n",
            "\n",
            "Test set: Average loss: 1.0486, Accuracy: 7730/10000 (77.30%)\n",
            "\n",
            "Epoch 209: Target Sparsity: 82.94%, Actual Sparsity: 82.91%\n",
            "Train Epoch: 209 [0/50000 (0%)]\tLoss: 0.894771\tAcc: 83.59%\n",
            "Train Epoch: 209 [12800/50000 (26%)]\tLoss: 0.905736\tAcc: 83.07%\n",
            "Train Epoch: 209 [25600/50000 (51%)]\tLoss: 0.962636\tAcc: 82.68%\n",
            "Train Epoch: 209 [38400/50000 (77%)]\tLoss: 0.963362\tAcc: 82.28%\n",
            "\n",
            "Test set: Average loss: 0.9091, Accuracy: 8261/10000 (82.61%)\n",
            "\n",
            "Epoch 210: Target Sparsity: 83.10%, Actual Sparsity: 83.08%\n",
            "Train Epoch: 210 [0/50000 (0%)]\tLoss: 0.831768\tAcc: 85.16%\n",
            "Train Epoch: 210 [12800/50000 (26%)]\tLoss: 0.970301\tAcc: 82.44%\n",
            "Train Epoch: 210 [25600/50000 (51%)]\tLoss: 0.912030\tAcc: 82.19%\n",
            "Train Epoch: 210 [38400/50000 (77%)]\tLoss: 0.971680\tAcc: 82.19%\n",
            "\n",
            "Test set: Average loss: 0.9058, Accuracy: 8240/10000 (82.40%)\n",
            "\n",
            "Epoch 211: Target Sparsity: 83.27%, Actual Sparsity: 83.24%\n",
            "Train Epoch: 211 [0/50000 (0%)]\tLoss: 0.836785\tAcc: 86.72%\n",
            "Train Epoch: 211 [12800/50000 (26%)]\tLoss: 0.936828\tAcc: 82.51%\n",
            "Train Epoch: 211 [25600/50000 (51%)]\tLoss: 0.799631\tAcc: 82.20%\n",
            "Train Epoch: 211 [38400/50000 (77%)]\tLoss: 0.878522\tAcc: 82.08%\n",
            "\n",
            "Test set: Average loss: 0.9626, Accuracy: 8038/10000 (80.38%)\n",
            "\n",
            "Epoch 212: Target Sparsity: 83.43%, Actual Sparsity: 83.41%\n",
            "Train Epoch: 212 [0/50000 (0%)]\tLoss: 0.900032\tAcc: 83.59%\n",
            "Train Epoch: 212 [12800/50000 (26%)]\tLoss: 0.874213\tAcc: 82.59%\n",
            "Train Epoch: 212 [25600/50000 (51%)]\tLoss: 0.880726\tAcc: 82.57%\n",
            "Train Epoch: 212 [38400/50000 (77%)]\tLoss: 0.777209\tAcc: 82.46%\n",
            "\n",
            "Test set: Average loss: 0.8645, Accuracy: 8423/10000 (84.23%)\n",
            "\n",
            "Epoch 213: Target Sparsity: 83.60%, Actual Sparsity: 83.57%\n",
            "Train Epoch: 213 [0/50000 (0%)]\tLoss: 0.809112\tAcc: 87.50%\n",
            "Train Epoch: 213 [12800/50000 (26%)]\tLoss: 0.912239\tAcc: 83.00%\n",
            "Train Epoch: 213 [25600/50000 (51%)]\tLoss: 0.919495\tAcc: 82.97%\n",
            "Train Epoch: 213 [38400/50000 (77%)]\tLoss: 0.858310\tAcc: 82.86%\n",
            "\n",
            "Test set: Average loss: 0.9037, Accuracy: 8200/10000 (82.00%)\n",
            "\n",
            "Epoch 214: Target Sparsity: 83.77%, Actual Sparsity: 83.74%\n",
            "Train Epoch: 214 [0/50000 (0%)]\tLoss: 0.937098\tAcc: 83.59%\n",
            "Train Epoch: 214 [12800/50000 (26%)]\tLoss: 0.879843\tAcc: 83.47%\n",
            "Train Epoch: 214 [25600/50000 (51%)]\tLoss: 0.890090\tAcc: 83.06%\n",
            "Train Epoch: 214 [38400/50000 (77%)]\tLoss: 0.874135\tAcc: 82.94%\n",
            "\n",
            "Test set: Average loss: 0.8450, Accuracy: 8492/10000 (84.92%)\n",
            "\n",
            "Epoch 215: Target Sparsity: 83.93%, Actual Sparsity: 83.90%\n",
            "Train Epoch: 215 [0/50000 (0%)]\tLoss: 0.908103\tAcc: 78.12%\n",
            "Train Epoch: 215 [12800/50000 (26%)]\tLoss: 0.907124\tAcc: 83.23%\n",
            "Train Epoch: 215 [25600/50000 (51%)]\tLoss: 0.913879\tAcc: 83.16%\n",
            "Train Epoch: 215 [38400/50000 (77%)]\tLoss: 0.853655\tAcc: 83.06%\n",
            "\n",
            "Test set: Average loss: 0.9325, Accuracy: 8135/10000 (81.35%)\n",
            "\n",
            "Epoch 216: Target Sparsity: 84.10%, Actual Sparsity: 84.07%\n",
            "Train Epoch: 216 [0/50000 (0%)]\tLoss: 0.917090\tAcc: 83.59%\n",
            "Train Epoch: 216 [12800/50000 (26%)]\tLoss: 0.933857\tAcc: 83.66%\n",
            "Train Epoch: 216 [25600/50000 (51%)]\tLoss: 0.872099\tAcc: 83.52%\n",
            "Train Epoch: 216 [38400/50000 (77%)]\tLoss: 0.976956\tAcc: 83.37%\n",
            "\n",
            "Test set: Average loss: 0.9153, Accuracy: 8227/10000 (82.27%)\n",
            "\n",
            "Epoch 217: Target Sparsity: 84.26%, Actual Sparsity: 84.24%\n",
            "Train Epoch: 217 [0/50000 (0%)]\tLoss: 0.897448\tAcc: 80.47%\n",
            "Train Epoch: 217 [12800/50000 (26%)]\tLoss: 0.993870\tAcc: 84.52%\n",
            "Train Epoch: 217 [25600/50000 (51%)]\tLoss: 0.879847\tAcc: 84.07%\n",
            "Train Epoch: 217 [38400/50000 (77%)]\tLoss: 0.927326\tAcc: 83.87%\n",
            "\n",
            "Test set: Average loss: 0.8177, Accuracy: 8609/10000 (86.09%)\n",
            "\n",
            "Epoch 218: Target Sparsity: 84.43%, Actual Sparsity: 84.40%\n",
            "Train Epoch: 218 [0/50000 (0%)]\tLoss: 1.017346\tAcc: 78.12%\n",
            "Train Epoch: 218 [12800/50000 (26%)]\tLoss: 0.908849\tAcc: 84.67%\n",
            "Train Epoch: 218 [25600/50000 (51%)]\tLoss: 0.928761\tAcc: 84.35%\n",
            "Train Epoch: 218 [38400/50000 (77%)]\tLoss: 0.768580\tAcc: 84.34%\n",
            "\n",
            "Test set: Average loss: 0.8283, Accuracy: 8540/10000 (85.40%)\n",
            "\n",
            "Epoch 219: Target Sparsity: 84.59%, Actual Sparsity: 84.57%\n",
            "Train Epoch: 219 [0/50000 (0%)]\tLoss: 0.851652\tAcc: 83.59%\n",
            "Train Epoch: 219 [12800/50000 (26%)]\tLoss: 0.849912\tAcc: 84.49%\n",
            "Train Epoch: 219 [25600/50000 (51%)]\tLoss: 0.829468\tAcc: 84.42%\n",
            "Train Epoch: 219 [38400/50000 (77%)]\tLoss: 0.917612\tAcc: 84.25%\n",
            "\n",
            "Test set: Average loss: 1.0210, Accuracy: 7907/10000 (79.07%)\n",
            "\n",
            "Epoch 220: Target Sparsity: 84.76%, Actual Sparsity: 84.73%\n",
            "Train Epoch: 220 [0/50000 (0%)]\tLoss: 0.871216\tAcc: 82.03%\n",
            "Train Epoch: 220 [12800/50000 (26%)]\tLoss: 0.882998\tAcc: 84.68%\n",
            "Train Epoch: 220 [25600/50000 (51%)]\tLoss: 0.840405\tAcc: 84.38%\n",
            "Train Epoch: 220 [38400/50000 (77%)]\tLoss: 0.976574\tAcc: 84.41%\n",
            "\n",
            "Test set: Average loss: 0.8337, Accuracy: 8549/10000 (85.49%)\n",
            "\n",
            "Epoch 221: Target Sparsity: 84.92%, Actual Sparsity: 84.90%\n",
            "Train Epoch: 221 [0/50000 (0%)]\tLoss: 0.862635\tAcc: 83.59%\n",
            "Train Epoch: 221 [12800/50000 (26%)]\tLoss: 0.818438\tAcc: 85.04%\n",
            "Train Epoch: 221 [25600/50000 (51%)]\tLoss: 0.759205\tAcc: 85.07%\n",
            "Train Epoch: 221 [38400/50000 (77%)]\tLoss: 0.941737\tAcc: 84.80%\n",
            "\n",
            "Test set: Average loss: 0.7968, Accuracy: 8692/10000 (86.92%)\n",
            "\n",
            "Epoch 222: Target Sparsity: 85.09%, Actual Sparsity: 85.06%\n",
            "Train Epoch: 222 [0/50000 (0%)]\tLoss: 0.816726\tAcc: 89.06%\n",
            "Train Epoch: 222 [12800/50000 (26%)]\tLoss: 0.932519\tAcc: 85.76%\n",
            "Train Epoch: 222 [25600/50000 (51%)]\tLoss: 0.896231\tAcc: 85.63%\n",
            "Train Epoch: 222 [38400/50000 (77%)]\tLoss: 0.848289\tAcc: 85.52%\n",
            "\n",
            "Test set: Average loss: 0.8322, Accuracy: 8570/10000 (85.70%)\n",
            "\n",
            "Epoch 223: Target Sparsity: 85.26%, Actual Sparsity: 85.23%\n",
            "Train Epoch: 223 [0/50000 (0%)]\tLoss: 0.824465\tAcc: 82.03%\n",
            "Train Epoch: 223 [12800/50000 (26%)]\tLoss: 0.823343\tAcc: 85.75%\n",
            "Train Epoch: 223 [25600/50000 (51%)]\tLoss: 0.876104\tAcc: 85.94%\n",
            "Train Epoch: 223 [38400/50000 (77%)]\tLoss: 0.805457\tAcc: 85.70%\n",
            "\n",
            "Test set: Average loss: 0.7938, Accuracy: 8717/10000 (87.17%)\n",
            "\n",
            "Epoch 224: Target Sparsity: 85.42%, Actual Sparsity: 85.39%\n",
            "Train Epoch: 224 [0/50000 (0%)]\tLoss: 0.891730\tAcc: 84.38%\n",
            "Train Epoch: 224 [12800/50000 (26%)]\tLoss: 0.893677\tAcc: 86.05%\n",
            "Train Epoch: 224 [25600/50000 (51%)]\tLoss: 0.789011\tAcc: 86.26%\n",
            "Train Epoch: 224 [38400/50000 (77%)]\tLoss: 0.865688\tAcc: 86.16%\n",
            "\n",
            "Test set: Average loss: 0.7975, Accuracy: 8697/10000 (86.97%)\n",
            "\n",
            "Epoch 225: Target Sparsity: 85.59%, Actual Sparsity: 85.56%\n",
            "Train Epoch: 225 [0/50000 (0%)]\tLoss: 0.887806\tAcc: 85.94%\n",
            "Train Epoch: 225 [12800/50000 (26%)]\tLoss: 0.753923\tAcc: 86.69%\n",
            "Train Epoch: 225 [25600/50000 (51%)]\tLoss: 0.881959\tAcc: 86.49%\n",
            "Train Epoch: 225 [38400/50000 (77%)]\tLoss: 0.840921\tAcc: 86.55%\n",
            "\n",
            "Test set: Average loss: 0.8168, Accuracy: 8591/10000 (85.91%)\n",
            "\n",
            "Epoch 226: Target Sparsity: 85.75%, Actual Sparsity: 85.72%\n",
            "Train Epoch: 226 [0/50000 (0%)]\tLoss: 0.840776\tAcc: 85.16%\n",
            "Train Epoch: 226 [12800/50000 (26%)]\tLoss: 0.829422\tAcc: 87.52%\n",
            "Train Epoch: 226 [25600/50000 (51%)]\tLoss: 0.787793\tAcc: 86.97%\n",
            "Train Epoch: 226 [38400/50000 (77%)]\tLoss: 0.807890\tAcc: 86.84%\n",
            "\n",
            "Test set: Average loss: 0.8076, Accuracy: 8703/10000 (87.03%)\n",
            "\n",
            "Epoch 227: Target Sparsity: 85.92%, Actual Sparsity: 85.89%\n",
            "Train Epoch: 227 [0/50000 (0%)]\tLoss: 0.836358\tAcc: 82.81%\n",
            "Train Epoch: 227 [12800/50000 (26%)]\tLoss: 0.868611\tAcc: 87.48%\n",
            "Train Epoch: 227 [25600/50000 (51%)]\tLoss: 0.970746\tAcc: 87.39%\n",
            "Train Epoch: 227 [38400/50000 (77%)]\tLoss: 0.778179\tAcc: 87.25%\n",
            "\n",
            "Test set: Average loss: 0.7614, Accuracy: 8900/10000 (89.00%)\n",
            "\n",
            "Epoch 228: Target Sparsity: 86.08%, Actual Sparsity: 86.06%\n",
            "Train Epoch: 228 [0/50000 (0%)]\tLoss: 0.724939\tAcc: 88.28%\n",
            "Train Epoch: 228 [12800/50000 (26%)]\tLoss: 0.858639\tAcc: 87.78%\n",
            "Train Epoch: 228 [25600/50000 (51%)]\tLoss: 0.725575\tAcc: 87.92%\n",
            "Train Epoch: 228 [38400/50000 (77%)]\tLoss: 0.783001\tAcc: 87.82%\n",
            "\n",
            "Test set: Average loss: 0.7633, Accuracy: 8875/10000 (88.75%)\n",
            "\n",
            "Epoch 229: Target Sparsity: 86.25%, Actual Sparsity: 86.22%\n",
            "Train Epoch: 229 [0/50000 (0%)]\tLoss: 0.775916\tAcc: 88.28%\n",
            "Train Epoch: 229 [12800/50000 (26%)]\tLoss: 0.759712\tAcc: 88.40%\n",
            "Train Epoch: 229 [25600/50000 (51%)]\tLoss: 0.710905\tAcc: 88.06%\n",
            "Train Epoch: 229 [38400/50000 (77%)]\tLoss: 0.823554\tAcc: 88.19%\n",
            "\n",
            "Test set: Average loss: 0.7495, Accuracy: 8913/10000 (89.13%)\n",
            "\n",
            "Epoch 230: Target Sparsity: 86.41%, Actual Sparsity: 86.39%\n",
            "Train Epoch: 230 [0/50000 (0%)]\tLoss: 0.719069\tAcc: 92.19%\n",
            "Train Epoch: 230 [12800/50000 (26%)]\tLoss: 0.707328\tAcc: 89.09%\n",
            "Train Epoch: 230 [25600/50000 (51%)]\tLoss: 0.766490\tAcc: 88.79%\n",
            "Train Epoch: 230 [38400/50000 (77%)]\tLoss: 0.685253\tAcc: 88.81%\n",
            "\n",
            "Test set: Average loss: 0.7275, Accuracy: 9039/10000 (90.39%)\n",
            "\n",
            "Epoch 231: Target Sparsity: 86.58%, Actual Sparsity: 86.55%\n",
            "Train Epoch: 231 [0/50000 (0%)]\tLoss: 0.764842\tAcc: 92.19%\n",
            "Train Epoch: 231 [12800/50000 (26%)]\tLoss: 0.740332\tAcc: 88.97%\n",
            "Train Epoch: 231 [25600/50000 (51%)]\tLoss: 0.772840\tAcc: 88.94%\n",
            "Train Epoch: 231 [38400/50000 (77%)]\tLoss: 0.816518\tAcc: 88.92%\n",
            "\n",
            "Test set: Average loss: 0.7969, Accuracy: 8753/10000 (87.53%)\n",
            "\n",
            "Epoch 232: Target Sparsity: 86.74%, Actual Sparsity: 86.72%\n",
            "Train Epoch: 232 [0/50000 (0%)]\tLoss: 0.725822\tAcc: 91.41%\n",
            "Train Epoch: 232 [12800/50000 (26%)]\tLoss: 0.706527\tAcc: 89.19%\n",
            "Train Epoch: 232 [25600/50000 (51%)]\tLoss: 0.805970\tAcc: 89.39%\n",
            "Train Epoch: 232 [38400/50000 (77%)]\tLoss: 0.710458\tAcc: 89.38%\n",
            "\n",
            "Test set: Average loss: 0.7364, Accuracy: 9008/10000 (90.08%)\n",
            "\n",
            "Epoch 233: Target Sparsity: 86.91%, Actual Sparsity: 86.88%\n",
            "Train Epoch: 233 [0/50000 (0%)]\tLoss: 0.843674\tAcc: 85.94%\n",
            "Train Epoch: 233 [12800/50000 (26%)]\tLoss: 0.806639\tAcc: 90.25%\n",
            "Train Epoch: 233 [25600/50000 (51%)]\tLoss: 0.749782\tAcc: 90.08%\n",
            "Train Epoch: 233 [38400/50000 (77%)]\tLoss: 0.713492\tAcc: 89.99%\n",
            "\n",
            "Test set: Average loss: 0.7140, Accuracy: 9081/10000 (90.81%)\n",
            "\n",
            "Epoch 234: Target Sparsity: 87.08%, Actual Sparsity: 87.05%\n",
            "Train Epoch: 234 [0/50000 (0%)]\tLoss: 0.735758\tAcc: 89.06%\n",
            "Train Epoch: 234 [12800/50000 (26%)]\tLoss: 0.688971\tAcc: 90.65%\n",
            "Train Epoch: 234 [25600/50000 (51%)]\tLoss: 0.762002\tAcc: 90.70%\n",
            "Train Epoch: 234 [38400/50000 (77%)]\tLoss: 0.704247\tAcc: 90.58%\n",
            "\n",
            "Test set: Average loss: 0.7012, Accuracy: 9134/10000 (91.34%)\n",
            "\n",
            "Epoch 235: Target Sparsity: 87.24%, Actual Sparsity: 87.21%\n",
            "Train Epoch: 235 [0/50000 (0%)]\tLoss: 0.704369\tAcc: 92.19%\n",
            "Train Epoch: 235 [12800/50000 (26%)]\tLoss: 0.684470\tAcc: 91.14%\n",
            "Train Epoch: 235 [25600/50000 (51%)]\tLoss: 0.664924\tAcc: 91.06%\n",
            "Train Epoch: 235 [38400/50000 (77%)]\tLoss: 0.734041\tAcc: 91.11%\n",
            "\n",
            "Test set: Average loss: 0.7071, Accuracy: 9107/10000 (91.07%)\n",
            "\n",
            "Epoch 236: Target Sparsity: 87.41%, Actual Sparsity: 87.38%\n",
            "Train Epoch: 236 [0/50000 (0%)]\tLoss: 0.706614\tAcc: 91.41%\n",
            "Train Epoch: 236 [12800/50000 (26%)]\tLoss: 0.742626\tAcc: 91.51%\n",
            "Train Epoch: 236 [25600/50000 (51%)]\tLoss: 0.690233\tAcc: 91.74%\n",
            "Train Epoch: 236 [38400/50000 (77%)]\tLoss: 0.670974\tAcc: 91.69%\n",
            "\n",
            "Test set: Average loss: 0.7172, Accuracy: 9071/10000 (90.71%)\n",
            "\n",
            "Epoch 237: Target Sparsity: 87.57%, Actual Sparsity: 87.54%\n",
            "Train Epoch: 237 [0/50000 (0%)]\tLoss: 0.640487\tAcc: 94.53%\n",
            "Train Epoch: 237 [12800/50000 (26%)]\tLoss: 0.680124\tAcc: 92.37%\n",
            "Train Epoch: 237 [25600/50000 (51%)]\tLoss: 0.663423\tAcc: 92.28%\n",
            "Train Epoch: 237 [38400/50000 (77%)]\tLoss: 0.672917\tAcc: 92.02%\n",
            "\n",
            "Test set: Average loss: 0.6844, Accuracy: 9222/10000 (92.22%)\n",
            "\n",
            "Epoch 238: Target Sparsity: 87.74%, Actual Sparsity: 87.71%\n",
            "Train Epoch: 238 [0/50000 (0%)]\tLoss: 0.647010\tAcc: 93.75%\n",
            "Train Epoch: 238 [12800/50000 (26%)]\tLoss: 0.689206\tAcc: 92.61%\n",
            "Train Epoch: 238 [25600/50000 (51%)]\tLoss: 0.679471\tAcc: 92.55%\n",
            "Train Epoch: 238 [38400/50000 (77%)]\tLoss: 0.660997\tAcc: 92.45%\n",
            "\n",
            "Test set: Average loss: 0.6877, Accuracy: 9209/10000 (92.09%)\n",
            "\n",
            "Epoch 239: Target Sparsity: 87.90%, Actual Sparsity: 87.88%\n",
            "Train Epoch: 239 [0/50000 (0%)]\tLoss: 0.678845\tAcc: 92.19%\n",
            "Train Epoch: 239 [12800/50000 (26%)]\tLoss: 0.638005\tAcc: 92.98%\n",
            "Train Epoch: 239 [25600/50000 (51%)]\tLoss: 0.746739\tAcc: 93.04%\n",
            "Train Epoch: 239 [38400/50000 (77%)]\tLoss: 0.703765\tAcc: 92.87%\n",
            "\n",
            "Test set: Average loss: 0.6790, Accuracy: 9235/10000 (92.35%)\n",
            "\n",
            "Epoch 240: Target Sparsity: 88.07%, Actual Sparsity: 88.04%\n",
            "Train Epoch: 240 [0/50000 (0%)]\tLoss: 0.670908\tAcc: 93.75%\n",
            "Train Epoch: 240 [12800/50000 (26%)]\tLoss: 0.635275\tAcc: 93.54%\n",
            "Train Epoch: 240 [25600/50000 (51%)]\tLoss: 0.712436\tAcc: 93.29%\n",
            "Train Epoch: 240 [38400/50000 (77%)]\tLoss: 0.643225\tAcc: 93.37%\n",
            "\n",
            "Test set: Average loss: 0.6714, Accuracy: 9283/10000 (92.83%)\n",
            "\n",
            "Epoch 241: Target Sparsity: 88.23%, Actual Sparsity: 88.21%\n",
            "Train Epoch: 241 [0/50000 (0%)]\tLoss: 0.671293\tAcc: 92.19%\n",
            "Train Epoch: 241 [12800/50000 (26%)]\tLoss: 0.762687\tAcc: 93.85%\n",
            "Train Epoch: 241 [25600/50000 (51%)]\tLoss: 0.592144\tAcc: 94.10%\n",
            "Train Epoch: 241 [38400/50000 (77%)]\tLoss: 0.669203\tAcc: 94.02%\n",
            "\n",
            "Test set: Average loss: 0.6721, Accuracy: 9279/10000 (92.79%)\n",
            "\n",
            "Epoch 242: Target Sparsity: 88.40%, Actual Sparsity: 88.37%\n",
            "Train Epoch: 242 [0/50000 (0%)]\tLoss: 0.659796\tAcc: 93.75%\n",
            "Train Epoch: 242 [12800/50000 (26%)]\tLoss: 0.573015\tAcc: 94.38%\n",
            "Train Epoch: 242 [25600/50000 (51%)]\tLoss: 0.555805\tAcc: 94.45%\n",
            "Train Epoch: 242 [38400/50000 (77%)]\tLoss: 0.594726\tAcc: 94.45%\n",
            "\n",
            "Test set: Average loss: 0.6614, Accuracy: 9319/10000 (93.19%)\n",
            "\n",
            "Epoch 243: Target Sparsity: 88.57%, Actual Sparsity: 88.54%\n",
            "Train Epoch: 243 [0/50000 (0%)]\tLoss: 0.630424\tAcc: 95.31%\n",
            "Train Epoch: 243 [12800/50000 (26%)]\tLoss: 0.681374\tAcc: 95.10%\n",
            "Train Epoch: 243 [25600/50000 (51%)]\tLoss: 0.583901\tAcc: 95.02%\n",
            "Train Epoch: 243 [38400/50000 (77%)]\tLoss: 0.606637\tAcc: 95.00%\n",
            "\n",
            "Test set: Average loss: 0.6593, Accuracy: 9335/10000 (93.35%)\n",
            "\n",
            "Epoch 244: Target Sparsity: 88.73%, Actual Sparsity: 88.70%\n",
            "Train Epoch: 244 [0/50000 (0%)]\tLoss: 0.589481\tAcc: 96.88%\n",
            "Train Epoch: 244 [12800/50000 (26%)]\tLoss: 0.612546\tAcc: 95.24%\n",
            "Train Epoch: 244 [25600/50000 (51%)]\tLoss: 0.700547\tAcc: 95.24%\n",
            "Train Epoch: 244 [38400/50000 (77%)]\tLoss: 0.573316\tAcc: 95.22%\n",
            "\n",
            "Test set: Average loss: 0.6578, Accuracy: 9352/10000 (93.52%)\n",
            "\n",
            "Epoch 245: Target Sparsity: 88.90%, Actual Sparsity: 88.87%\n",
            "Train Epoch: 245 [0/50000 (0%)]\tLoss: 0.607969\tAcc: 96.09%\n",
            "Train Epoch: 245 [12800/50000 (26%)]\tLoss: 0.628131\tAcc: 95.30%\n",
            "Train Epoch: 245 [25600/50000 (51%)]\tLoss: 0.660625\tAcc: 95.26%\n",
            "Train Epoch: 245 [38400/50000 (77%)]\tLoss: 0.572536\tAcc: 95.28%\n",
            "\n",
            "Test set: Average loss: 0.6533, Accuracy: 9368/10000 (93.68%)\n",
            "\n",
            "Epoch 246: Target Sparsity: 89.06%, Actual Sparsity: 89.03%\n",
            "Train Epoch: 246 [0/50000 (0%)]\tLoss: 0.662406\tAcc: 92.19%\n",
            "Train Epoch: 246 [12800/50000 (26%)]\tLoss: 0.616653\tAcc: 95.58%\n",
            "Train Epoch: 246 [25600/50000 (51%)]\tLoss: 0.552317\tAcc: 95.62%\n",
            "Train Epoch: 246 [38400/50000 (77%)]\tLoss: 0.586654\tAcc: 95.68%\n",
            "\n",
            "Test set: Average loss: 0.6529, Accuracy: 9367/10000 (93.67%)\n",
            "\n",
            "Epoch 247: Target Sparsity: 89.23%, Actual Sparsity: 89.20%\n",
            "Train Epoch: 247 [0/50000 (0%)]\tLoss: 0.660429\tAcc: 94.53%\n",
            "Train Epoch: 247 [12800/50000 (26%)]\tLoss: 0.578645\tAcc: 95.83%\n",
            "Train Epoch: 247 [25600/50000 (51%)]\tLoss: 0.645162\tAcc: 95.72%\n",
            "Train Epoch: 247 [38400/50000 (77%)]\tLoss: 0.570875\tAcc: 95.78%\n",
            "\n",
            "Test set: Average loss: 0.6499, Accuracy: 9377/10000 (93.77%)\n",
            "\n",
            "Epoch 248: Target Sparsity: 89.39%, Actual Sparsity: 89.36%\n",
            "Train Epoch: 248 [0/50000 (0%)]\tLoss: 0.592084\tAcc: 95.31%\n",
            "Train Epoch: 248 [12800/50000 (26%)]\tLoss: 0.588476\tAcc: 96.16%\n",
            "Train Epoch: 248 [25600/50000 (51%)]\tLoss: 0.580205\tAcc: 96.11%\n",
            "Train Epoch: 248 [38400/50000 (77%)]\tLoss: 0.595557\tAcc: 96.02%\n",
            "\n",
            "Test set: Average loss: 0.6501, Accuracy: 9379/10000 (93.79%)\n",
            "\n",
            "Epoch 249: Target Sparsity: 89.56%, Actual Sparsity: 89.53%\n",
            "Train Epoch: 249 [0/50000 (0%)]\tLoss: 0.595332\tAcc: 96.88%\n",
            "Train Epoch: 249 [12800/50000 (26%)]\tLoss: 0.587742\tAcc: 96.48%\n",
            "Train Epoch: 249 [25600/50000 (51%)]\tLoss: 0.588415\tAcc: 96.30%\n",
            "Train Epoch: 249 [38400/50000 (77%)]\tLoss: 0.611794\tAcc: 96.22%\n",
            "\n",
            "Test set: Average loss: 0.6498, Accuracy: 9387/10000 (93.87%)\n",
            "\n",
            "Epoch 250: Target Sparsity: 89.72%, Actual Sparsity: 89.70%\n",
            "Train Epoch: 250 [0/50000 (0%)]\tLoss: 0.553011\tAcc: 98.44%\n",
            "Train Epoch: 250 [12800/50000 (26%)]\tLoss: 0.589986\tAcc: 96.38%\n",
            "Train Epoch: 250 [25600/50000 (51%)]\tLoss: 0.579067\tAcc: 96.41%\n",
            "Train Epoch: 250 [38400/50000 (77%)]\tLoss: 0.586339\tAcc: 96.21%\n",
            "\n",
            "Test set: Average loss: 0.6499, Accuracy: 9378/10000 (93.78%)\n",
            "\n",
            "Epoch 251: Target Sparsity: 89.89%, Actual Sparsity: 89.86%\n",
            "Train Epoch: 251 [0/50000 (0%)]\tLoss: 0.588843\tAcc: 96.88%\n",
            "Train Epoch: 251 [12800/50000 (26%)]\tLoss: 0.626889\tAcc: 96.38%\n",
            "Train Epoch: 251 [25600/50000 (51%)]\tLoss: 0.575226\tAcc: 96.32%\n",
            "Train Epoch: 251 [38400/50000 (77%)]\tLoss: 0.594383\tAcc: 96.27%\n",
            "\n",
            "Test set: Average loss: 0.6502, Accuracy: 9386/10000 (93.86%)\n",
            "\n",
            "Epoch 252: Target Sparsity: 90.06%, Actual Sparsity: 90.03%\n",
            "Train Epoch: 252 [0/50000 (0%)]\tLoss: 0.543080\tAcc: 97.66%\n",
            "Train Epoch: 252 [12800/50000 (26%)]\tLoss: 0.646959\tAcc: 96.16%\n",
            "Train Epoch: 252 [25600/50000 (51%)]\tLoss: 0.639468\tAcc: 96.19%\n",
            "Train Epoch: 252 [38400/50000 (77%)]\tLoss: 0.547031\tAcc: 96.24%\n",
            "\n",
            "Test set: Average loss: 0.6500, Accuracy: 9379/10000 (93.79%)\n",
            "\n",
            "Epoch 253: Target Sparsity: 90.22%, Actual Sparsity: 90.19%\n",
            "Train Epoch: 253 [0/50000 (0%)]\tLoss: 0.598025\tAcc: 94.53%\n",
            "Train Epoch: 253 [12800/50000 (26%)]\tLoss: 0.734197\tAcc: 96.12%\n",
            "Train Epoch: 253 [25600/50000 (51%)]\tLoss: 0.640348\tAcc: 95.93%\n",
            "Train Epoch: 253 [38400/50000 (77%)]\tLoss: 0.585941\tAcc: 96.08%\n",
            "\n",
            "Test set: Average loss: 0.6488, Accuracy: 9395/10000 (93.95%)\n",
            "\n",
            "Epoch 254: Target Sparsity: 90.39%, Actual Sparsity: 90.36%\n",
            "Train Epoch: 254 [0/50000 (0%)]\tLoss: 0.608589\tAcc: 96.88%\n",
            "Train Epoch: 254 [12800/50000 (26%)]\tLoss: 0.686128\tAcc: 96.17%\n",
            "Train Epoch: 254 [25600/50000 (51%)]\tLoss: 0.589525\tAcc: 96.18%\n",
            "Train Epoch: 254 [38400/50000 (77%)]\tLoss: 0.586979\tAcc: 96.11%\n",
            "\n",
            "Test set: Average loss: 0.6486, Accuracy: 9398/10000 (93.98%)\n",
            "\n",
            "Epoch 255: Target Sparsity: 90.55%, Actual Sparsity: 90.52%\n",
            "Train Epoch: 255 [0/50000 (0%)]\tLoss: 0.688241\tAcc: 92.97%\n",
            "Train Epoch: 255 [12800/50000 (26%)]\tLoss: 0.622176\tAcc: 96.21%\n",
            "Train Epoch: 255 [25600/50000 (51%)]\tLoss: 0.573254\tAcc: 96.35%\n",
            "Train Epoch: 255 [38400/50000 (77%)]\tLoss: 0.574479\tAcc: 96.25%\n",
            "\n",
            "Test set: Average loss: 0.6544, Accuracy: 9368/10000 (93.68%)\n",
            "\n",
            "Epoch 256: Target Sparsity: 90.72%, Actual Sparsity: 90.69%\n",
            "Train Epoch: 256 [0/50000 (0%)]\tLoss: 0.663459\tAcc: 92.19%\n",
            "Train Epoch: 256 [12800/50000 (26%)]\tLoss: 0.593477\tAcc: 96.23%\n",
            "Train Epoch: 256 [25600/50000 (51%)]\tLoss: 0.568367\tAcc: 96.20%\n",
            "Train Epoch: 256 [38400/50000 (77%)]\tLoss: 0.572840\tAcc: 96.16%\n",
            "\n",
            "Test set: Average loss: 0.6549, Accuracy: 9372/10000 (93.72%)\n",
            "\n",
            "Epoch 257: Target Sparsity: 90.88%, Actual Sparsity: 90.85%\n",
            "Train Epoch: 257 [0/50000 (0%)]\tLoss: 0.565637\tAcc: 98.44%\n",
            "Train Epoch: 257 [12800/50000 (26%)]\tLoss: 0.556139\tAcc: 96.02%\n",
            "Train Epoch: 257 [25600/50000 (51%)]\tLoss: 0.625591\tAcc: 96.07%\n",
            "Train Epoch: 257 [38400/50000 (77%)]\tLoss: 0.628162\tAcc: 96.12%\n",
            "\n",
            "Test set: Average loss: 0.6609, Accuracy: 9345/10000 (93.45%)\n",
            "\n",
            "Epoch 258: Target Sparsity: 91.05%, Actual Sparsity: 91.02%\n",
            "Train Epoch: 258 [0/50000 (0%)]\tLoss: 0.605973\tAcc: 96.09%\n",
            "Train Epoch: 258 [12800/50000 (26%)]\tLoss: 0.622297\tAcc: 96.08%\n",
            "Train Epoch: 258 [25600/50000 (51%)]\tLoss: 0.592119\tAcc: 95.93%\n",
            "Train Epoch: 258 [38400/50000 (77%)]\tLoss: 0.614426\tAcc: 95.84%\n",
            "\n",
            "Test set: Average loss: 0.6615, Accuracy: 9335/10000 (93.35%)\n",
            "\n",
            "Epoch 259: Target Sparsity: 91.21%, Actual Sparsity: 91.18%\n",
            "Train Epoch: 259 [0/50000 (0%)]\tLoss: 0.618434\tAcc: 95.31%\n",
            "Train Epoch: 259 [12800/50000 (26%)]\tLoss: 0.628484\tAcc: 95.57%\n",
            "Train Epoch: 259 [25600/50000 (51%)]\tLoss: 0.568006\tAcc: 95.66%\n",
            "Train Epoch: 259 [38400/50000 (77%)]\tLoss: 0.586544\tAcc: 95.66%\n",
            "\n",
            "Test set: Average loss: 0.6647, Accuracy: 9318/10000 (93.18%)\n",
            "\n",
            "Epoch 260: Target Sparsity: 91.38%, Actual Sparsity: 91.35%\n",
            "Train Epoch: 260 [0/50000 (0%)]\tLoss: 0.583245\tAcc: 96.09%\n",
            "Train Epoch: 260 [12800/50000 (26%)]\tLoss: 0.579586\tAcc: 95.29%\n",
            "Train Epoch: 260 [25600/50000 (51%)]\tLoss: 0.612500\tAcc: 95.39%\n",
            "Train Epoch: 260 [38400/50000 (77%)]\tLoss: 0.619925\tAcc: 95.35%\n",
            "\n",
            "Test set: Average loss: 0.6635, Accuracy: 9325/10000 (93.25%)\n",
            "\n",
            "Epoch 261: Target Sparsity: 91.54%, Actual Sparsity: 91.52%\n",
            "Train Epoch: 261 [0/50000 (0%)]\tLoss: 0.625845\tAcc: 94.53%\n",
            "Train Epoch: 261 [12800/50000 (26%)]\tLoss: 0.615187\tAcc: 94.97%\n",
            "Train Epoch: 261 [25600/50000 (51%)]\tLoss: 0.629824\tAcc: 95.09%\n",
            "Train Epoch: 261 [38400/50000 (77%)]\tLoss: 0.626517\tAcc: 95.02%\n",
            "\n",
            "Test set: Average loss: 0.6741, Accuracy: 9281/10000 (92.81%)\n",
            "\n",
            "Epoch 262: Target Sparsity: 91.71%, Actual Sparsity: 91.68%\n",
            "Train Epoch: 262 [0/50000 (0%)]\tLoss: 0.607002\tAcc: 95.31%\n",
            "Train Epoch: 262 [12800/50000 (26%)]\tLoss: 0.698372\tAcc: 94.92%\n",
            "Train Epoch: 262 [25600/50000 (51%)]\tLoss: 0.573920\tAcc: 94.86%\n",
            "Train Epoch: 262 [38400/50000 (77%)]\tLoss: 0.773445\tAcc: 94.75%\n",
            "\n",
            "Test set: Average loss: 0.6779, Accuracy: 9270/10000 (92.70%)\n",
            "\n",
            "Epoch 263: Target Sparsity: 91.88%, Actual Sparsity: 91.85%\n",
            "Train Epoch: 263 [0/50000 (0%)]\tLoss: 0.668041\tAcc: 93.75%\n",
            "Train Epoch: 263 [12800/50000 (26%)]\tLoss: 0.613603\tAcc: 94.62%\n",
            "Train Epoch: 263 [25600/50000 (51%)]\tLoss: 0.723781\tAcc: 94.39%\n",
            "Train Epoch: 263 [38400/50000 (77%)]\tLoss: 0.649805\tAcc: 94.34%\n",
            "\n",
            "Test set: Average loss: 0.6816, Accuracy: 9240/10000 (92.40%)\n",
            "\n",
            "Epoch 264: Target Sparsity: 92.04%, Actual Sparsity: 92.01%\n",
            "Train Epoch: 264 [0/50000 (0%)]\tLoss: 0.629053\tAcc: 93.75%\n",
            "Train Epoch: 264 [12800/50000 (26%)]\tLoss: 0.579421\tAcc: 94.09%\n",
            "Train Epoch: 264 [25600/50000 (51%)]\tLoss: 0.627430\tAcc: 94.04%\n",
            "Train Epoch: 264 [38400/50000 (77%)]\tLoss: 0.623064\tAcc: 93.92%\n",
            "\n",
            "Test set: Average loss: 0.6785, Accuracy: 9258/10000 (92.58%)\n",
            "\n",
            "Epoch 265: Target Sparsity: 92.21%, Actual Sparsity: 92.18%\n",
            "Train Epoch: 265 [0/50000 (0%)]\tLoss: 0.688840\tAcc: 92.19%\n",
            "Train Epoch: 265 [12800/50000 (26%)]\tLoss: 0.619495\tAcc: 93.64%\n",
            "Train Epoch: 265 [25600/50000 (51%)]\tLoss: 0.602797\tAcc: 93.68%\n",
            "Train Epoch: 265 [38400/50000 (77%)]\tLoss: 0.645043\tAcc: 93.40%\n",
            "\n",
            "Test set: Average loss: 0.6858, Accuracy: 9225/10000 (92.25%)\n",
            "\n",
            "Epoch 266: Target Sparsity: 92.37%, Actual Sparsity: 92.34%\n",
            "Train Epoch: 266 [0/50000 (0%)]\tLoss: 0.633834\tAcc: 94.53%\n",
            "Train Epoch: 266 [12800/50000 (26%)]\tLoss: 0.752512\tAcc: 93.84%\n",
            "Train Epoch: 266 [25600/50000 (51%)]\tLoss: 0.656761\tAcc: 93.60%\n",
            "Train Epoch: 266 [38400/50000 (77%)]\tLoss: 0.640814\tAcc: 93.21%\n",
            "\n",
            "Test set: Average loss: 0.7199, Accuracy: 9081/10000 (90.81%)\n",
            "\n",
            "Epoch 267: Target Sparsity: 92.54%, Actual Sparsity: 92.51%\n",
            "Train Epoch: 267 [0/50000 (0%)]\tLoss: 0.611744\tAcc: 95.31%\n",
            "Train Epoch: 267 [12800/50000 (26%)]\tLoss: 0.709599\tAcc: 92.78%\n",
            "Train Epoch: 267 [25600/50000 (51%)]\tLoss: 0.733801\tAcc: 92.57%\n",
            "Train Epoch: 267 [38400/50000 (77%)]\tLoss: 0.741645\tAcc: 92.27%\n",
            "\n",
            "Test set: Average loss: 0.7088, Accuracy: 9132/10000 (91.32%)\n",
            "\n",
            "Epoch 268: Target Sparsity: 92.70%, Actual Sparsity: 92.67%\n",
            "Train Epoch: 268 [0/50000 (0%)]\tLoss: 0.662398\tAcc: 93.75%\n",
            "Train Epoch: 268 [12800/50000 (26%)]\tLoss: 0.704520\tAcc: 91.86%\n",
            "Train Epoch: 268 [25600/50000 (51%)]\tLoss: 0.666907\tAcc: 91.81%\n",
            "Train Epoch: 268 [38400/50000 (77%)]\tLoss: 0.718235\tAcc: 91.92%\n",
            "\n",
            "Test set: Average loss: 0.7355, Accuracy: 8973/10000 (89.73%)\n",
            "\n",
            "Epoch 269: Target Sparsity: 92.87%, Actual Sparsity: 92.84%\n",
            "Train Epoch: 269 [0/50000 (0%)]\tLoss: 0.699338\tAcc: 91.41%\n",
            "Train Epoch: 269 [12800/50000 (26%)]\tLoss: 0.693067\tAcc: 91.52%\n",
            "Train Epoch: 269 [25600/50000 (51%)]\tLoss: 0.659211\tAcc: 91.46%\n",
            "Train Epoch: 269 [38400/50000 (77%)]\tLoss: 0.715275\tAcc: 91.28%\n",
            "\n",
            "Test set: Average loss: 0.7213, Accuracy: 9072/10000 (90.72%)\n",
            "\n",
            "Epoch 270: Target Sparsity: 93.03%, Actual Sparsity: 93.00%\n",
            "Train Epoch: 270 [0/50000 (0%)]\tLoss: 0.737413\tAcc: 89.06%\n",
            "Train Epoch: 270 [12800/50000 (26%)]\tLoss: 0.791536\tAcc: 90.63%\n",
            "Train Epoch: 270 [25600/50000 (51%)]\tLoss: 0.675801\tAcc: 90.76%\n",
            "Train Epoch: 270 [38400/50000 (77%)]\tLoss: 0.770587\tAcc: 90.72%\n",
            "\n",
            "Test set: Average loss: 0.7219, Accuracy: 9054/10000 (90.54%)\n",
            "\n",
            "Epoch 271: Target Sparsity: 93.20%, Actual Sparsity: 93.17%\n",
            "Train Epoch: 271 [0/50000 (0%)]\tLoss: 0.675596\tAcc: 92.97%\n",
            "Train Epoch: 271 [12800/50000 (26%)]\tLoss: 0.664347\tAcc: 90.59%\n",
            "Train Epoch: 271 [25600/50000 (51%)]\tLoss: 0.678409\tAcc: 90.26%\n",
            "Train Epoch: 271 [38400/50000 (77%)]\tLoss: 0.631564\tAcc: 90.19%\n",
            "\n",
            "Test set: Average loss: 0.7363, Accuracy: 9012/10000 (90.12%)\n",
            "\n",
            "Epoch 272: Target Sparsity: 93.37%, Actual Sparsity: 93.34%\n",
            "Train Epoch: 272 [0/50000 (0%)]\tLoss: 0.774231\tAcc: 89.84%\n",
            "Train Epoch: 272 [12800/50000 (26%)]\tLoss: 0.587341\tAcc: 90.02%\n",
            "Train Epoch: 272 [25600/50000 (51%)]\tLoss: 0.696911\tAcc: 89.84%\n",
            "Train Epoch: 272 [38400/50000 (77%)]\tLoss: 0.777551\tAcc: 89.50%\n",
            "\n",
            "Test set: Average loss: 0.7449, Accuracy: 8949/10000 (89.49%)\n",
            "\n",
            "Epoch 273: Target Sparsity: 93.53%, Actual Sparsity: 93.50%\n",
            "Train Epoch: 273 [0/50000 (0%)]\tLoss: 0.672844\tAcc: 92.97%\n",
            "Train Epoch: 273 [12800/50000 (26%)]\tLoss: 0.753822\tAcc: 89.49%\n",
            "Train Epoch: 273 [25600/50000 (51%)]\tLoss: 0.730737\tAcc: 89.10%\n",
            "Train Epoch: 273 [38400/50000 (77%)]\tLoss: 0.774133\tAcc: 89.06%\n",
            "\n",
            "Test set: Average loss: 0.8207, Accuracy: 8710/10000 (87.10%)\n",
            "\n",
            "Epoch 274: Target Sparsity: 93.70%, Actual Sparsity: 93.67%\n",
            "Train Epoch: 274 [0/50000 (0%)]\tLoss: 0.693972\tAcc: 92.19%\n",
            "Train Epoch: 274 [12800/50000 (26%)]\tLoss: 0.750867\tAcc: 89.41%\n",
            "Train Epoch: 274 [25600/50000 (51%)]\tLoss: 0.768258\tAcc: 88.97%\n",
            "Train Epoch: 274 [38400/50000 (77%)]\tLoss: 0.741054\tAcc: 88.71%\n",
            "\n",
            "Test set: Average loss: 0.7825, Accuracy: 8797/10000 (87.97%)\n",
            "\n",
            "Epoch 275: Target Sparsity: 93.86%, Actual Sparsity: 93.83%\n",
            "Train Epoch: 275 [0/50000 (0%)]\tLoss: 0.784305\tAcc: 87.50%\n",
            "Train Epoch: 275 [12800/50000 (26%)]\tLoss: 0.743462\tAcc: 88.13%\n",
            "Train Epoch: 275 [25600/50000 (51%)]\tLoss: 0.734480\tAcc: 88.11%\n",
            "Train Epoch: 275 [38400/50000 (77%)]\tLoss: 0.778854\tAcc: 88.08%\n",
            "\n",
            "Test set: Average loss: 0.8394, Accuracy: 8601/10000 (86.01%)\n",
            "\n",
            "Epoch 276: Target Sparsity: 94.03%, Actual Sparsity: 94.00%\n",
            "Train Epoch: 276 [0/50000 (0%)]\tLoss: 0.802916\tAcc: 87.50%\n",
            "Train Epoch: 276 [12800/50000 (26%)]\tLoss: 0.766432\tAcc: 88.24%\n",
            "Train Epoch: 276 [25600/50000 (51%)]\tLoss: 0.781108\tAcc: 88.01%\n",
            "Train Epoch: 276 [38400/50000 (77%)]\tLoss: 0.812099\tAcc: 87.79%\n",
            "\n",
            "Test set: Average loss: 0.8154, Accuracy: 8684/10000 (86.84%)\n",
            "\n",
            "Epoch 277: Target Sparsity: 94.19%, Actual Sparsity: 94.16%\n",
            "Train Epoch: 277 [0/50000 (0%)]\tLoss: 0.758368\tAcc: 88.28%\n",
            "Train Epoch: 277 [12800/50000 (26%)]\tLoss: 0.824888\tAcc: 87.55%\n",
            "Train Epoch: 277 [25600/50000 (51%)]\tLoss: 0.813918\tAcc: 87.20%\n",
            "Train Epoch: 277 [38400/50000 (77%)]\tLoss: 0.831049\tAcc: 87.21%\n",
            "\n",
            "Test set: Average loss: 0.8146, Accuracy: 8686/10000 (86.86%)\n",
            "\n",
            "Epoch 278: Target Sparsity: 94.36%, Actual Sparsity: 94.33%\n",
            "Train Epoch: 278 [0/50000 (0%)]\tLoss: 0.775480\tAcc: 85.16%\n",
            "Train Epoch: 278 [12800/50000 (26%)]\tLoss: 0.781181\tAcc: 87.81%\n",
            "Train Epoch: 278 [25600/50000 (51%)]\tLoss: 0.744532\tAcc: 87.26%\n",
            "Train Epoch: 278 [38400/50000 (77%)]\tLoss: 0.785868\tAcc: 87.01%\n",
            "\n",
            "Test set: Average loss: 0.7911, Accuracy: 8756/10000 (87.56%)\n",
            "\n",
            "Epoch 279: Target Sparsity: 94.52%, Actual Sparsity: 94.49%\n",
            "Train Epoch: 279 [0/50000 (0%)]\tLoss: 0.732944\tAcc: 89.06%\n",
            "Train Epoch: 279 [12800/50000 (26%)]\tLoss: 0.813603\tAcc: 87.24%\n",
            "Train Epoch: 279 [25600/50000 (51%)]\tLoss: 0.811354\tAcc: 87.01%\n",
            "Train Epoch: 279 [38400/50000 (77%)]\tLoss: 0.782045\tAcc: 86.74%\n",
            "\n",
            "Test set: Average loss: 0.8029, Accuracy: 8712/10000 (87.12%)\n",
            "\n",
            "Epoch 280: Target Sparsity: 94.69%, Actual Sparsity: 94.66%\n",
            "Train Epoch: 280 [0/50000 (0%)]\tLoss: 0.727242\tAcc: 90.62%\n",
            "Train Epoch: 280 [12800/50000 (26%)]\tLoss: 0.882076\tAcc: 87.38%\n",
            "Train Epoch: 280 [25600/50000 (51%)]\tLoss: 0.783531\tAcc: 87.19%\n",
            "Train Epoch: 280 [38400/50000 (77%)]\tLoss: 0.804835\tAcc: 86.87%\n",
            "\n",
            "Test set: Average loss: 0.8307, Accuracy: 8561/10000 (85.61%)\n",
            "\n",
            "Epoch 281: Target Sparsity: 94.86%, Actual Sparsity: 94.83%\n",
            "Train Epoch: 281 [0/50000 (0%)]\tLoss: 0.826643\tAcc: 85.94%\n",
            "Train Epoch: 281 [12800/50000 (26%)]\tLoss: 0.885276\tAcc: 86.54%\n",
            "Train Epoch: 281 [25600/50000 (51%)]\tLoss: 0.827224\tAcc: 86.45%\n",
            "Train Epoch: 281 [38400/50000 (77%)]\tLoss: 0.769371\tAcc: 86.33%\n",
            "\n",
            "Test set: Average loss: 0.8162, Accuracy: 8634/10000 (86.34%)\n",
            "\n",
            "Epoch 282: Target Sparsity: 95.02%, Actual Sparsity: 94.99%\n",
            "Train Epoch: 282 [0/50000 (0%)]\tLoss: 0.853696\tAcc: 85.16%\n",
            "Train Epoch: 282 [12800/50000 (26%)]\tLoss: 0.834779\tAcc: 86.68%\n",
            "Train Epoch: 282 [25600/50000 (51%)]\tLoss: 0.850216\tAcc: 86.41%\n",
            "Train Epoch: 282 [38400/50000 (77%)]\tLoss: 0.742886\tAcc: 86.38%\n",
            "\n",
            "Test set: Average loss: 0.8294, Accuracy: 8609/10000 (86.09%)\n",
            "\n",
            "Epoch 283: Target Sparsity: 95.19%, Actual Sparsity: 95.16%\n",
            "Train Epoch: 283 [0/50000 (0%)]\tLoss: 0.740835\tAcc: 91.41%\n",
            "Train Epoch: 283 [12800/50000 (26%)]\tLoss: 0.762119\tAcc: 86.47%\n",
            "Train Epoch: 283 [25600/50000 (51%)]\tLoss: 0.869224\tAcc: 86.44%\n",
            "Train Epoch: 283 [38400/50000 (77%)]\tLoss: 0.896764\tAcc: 86.23%\n",
            "\n",
            "Test set: Average loss: 0.8278, Accuracy: 8609/10000 (86.09%)\n",
            "\n",
            "Epoch 284: Target Sparsity: 95.35%, Actual Sparsity: 95.32%\n",
            "Train Epoch: 284 [0/50000 (0%)]\tLoss: 0.815916\tAcc: 85.94%\n",
            "Train Epoch: 284 [12800/50000 (26%)]\tLoss: 0.831048\tAcc: 85.92%\n",
            "Train Epoch: 284 [25600/50000 (51%)]\tLoss: 0.798747\tAcc: 85.94%\n",
            "Train Epoch: 284 [38400/50000 (77%)]\tLoss: 0.841274\tAcc: 85.93%\n",
            "\n",
            "Test set: Average loss: 0.8506, Accuracy: 8521/10000 (85.21%)\n",
            "\n",
            "Epoch 285: Target Sparsity: 95.52%, Actual Sparsity: 95.49%\n",
            "Train Epoch: 285 [0/50000 (0%)]\tLoss: 0.792931\tAcc: 89.84%\n",
            "Train Epoch: 285 [12800/50000 (26%)]\tLoss: 0.824129\tAcc: 86.01%\n",
            "Train Epoch: 285 [25600/50000 (51%)]\tLoss: 0.815338\tAcc: 85.96%\n",
            "Train Epoch: 285 [38400/50000 (77%)]\tLoss: 0.895977\tAcc: 85.72%\n",
            "\n",
            "Test set: Average loss: 0.8869, Accuracy: 8393/10000 (83.93%)\n",
            "\n",
            "Epoch 286: Target Sparsity: 95.68%, Actual Sparsity: 95.65%\n",
            "Train Epoch: 286 [0/50000 (0%)]\tLoss: 0.853802\tAcc: 84.38%\n",
            "Train Epoch: 286 [12800/50000 (26%)]\tLoss: 0.821017\tAcc: 86.00%\n",
            "Train Epoch: 286 [25600/50000 (51%)]\tLoss: 0.782756\tAcc: 85.90%\n",
            "Train Epoch: 286 [38400/50000 (77%)]\tLoss: 0.867522\tAcc: 85.72%\n",
            "\n",
            "Test set: Average loss: 0.8481, Accuracy: 8496/10000 (84.96%)\n",
            "\n",
            "Epoch 287: Target Sparsity: 95.85%, Actual Sparsity: 95.82%\n",
            "Train Epoch: 287 [0/50000 (0%)]\tLoss: 0.945927\tAcc: 82.03%\n",
            "Train Epoch: 287 [12800/50000 (26%)]\tLoss: 0.866002\tAcc: 86.05%\n",
            "Train Epoch: 287 [25600/50000 (51%)]\tLoss: 0.903129\tAcc: 85.79%\n",
            "Train Epoch: 287 [38400/50000 (77%)]\tLoss: 0.934684\tAcc: 85.63%\n",
            "\n",
            "Test set: Average loss: 0.7905, Accuracy: 8770/10000 (87.70%)\n",
            "\n",
            "Epoch 288: Target Sparsity: 96.01%, Actual Sparsity: 95.98%\n",
            "Train Epoch: 288 [0/50000 (0%)]\tLoss: 0.867194\tAcc: 85.16%\n",
            "Train Epoch: 288 [12800/50000 (26%)]\tLoss: 0.843544\tAcc: 85.23%\n",
            "Train Epoch: 288 [25600/50000 (51%)]\tLoss: 0.856803\tAcc: 85.20%\n",
            "Train Epoch: 288 [38400/50000 (77%)]\tLoss: 0.830509\tAcc: 85.21%\n",
            "\n",
            "Test set: Average loss: 0.9017, Accuracy: 8223/10000 (82.23%)\n",
            "\n",
            "Epoch 289: Target Sparsity: 96.18%, Actual Sparsity: 96.15%\n",
            "Train Epoch: 289 [0/50000 (0%)]\tLoss: 0.837566\tAcc: 84.38%\n",
            "Train Epoch: 289 [12800/50000 (26%)]\tLoss: 0.852922\tAcc: 85.69%\n",
            "Train Epoch: 289 [25600/50000 (51%)]\tLoss: 0.940192\tAcc: 85.54%\n",
            "Train Epoch: 289 [38400/50000 (77%)]\tLoss: 0.825763\tAcc: 85.43%\n",
            "\n",
            "Test set: Average loss: 0.9034, Accuracy: 8259/10000 (82.59%)\n",
            "\n",
            "Epoch 290: Target Sparsity: 96.34%, Actual Sparsity: 96.31%\n",
            "Train Epoch: 290 [0/50000 (0%)]\tLoss: 0.938774\tAcc: 80.47%\n",
            "Train Epoch: 290 [12800/50000 (26%)]\tLoss: 0.977254\tAcc: 85.60%\n",
            "Train Epoch: 290 [25600/50000 (51%)]\tLoss: 0.812125\tAcc: 85.53%\n",
            "Train Epoch: 290 [38400/50000 (77%)]\tLoss: 0.731802\tAcc: 85.34%\n",
            "\n",
            "Test set: Average loss: 0.8339, Accuracy: 8543/10000 (85.43%)\n",
            "\n",
            "Epoch 291: Target Sparsity: 96.51%, Actual Sparsity: 96.48%\n",
            "Train Epoch: 291 [0/50000 (0%)]\tLoss: 0.879205\tAcc: 83.59%\n",
            "Train Epoch: 291 [12800/50000 (26%)]\tLoss: 0.774586\tAcc: 85.16%\n",
            "Train Epoch: 291 [25600/50000 (51%)]\tLoss: 0.854475\tAcc: 85.25%\n",
            "Train Epoch: 291 [38400/50000 (77%)]\tLoss: 0.817113\tAcc: 85.42%\n",
            "\n",
            "Test set: Average loss: 0.8401, Accuracy: 8563/10000 (85.63%)\n",
            "\n",
            "Epoch 292: Target Sparsity: 96.68%, Actual Sparsity: 96.65%\n",
            "Train Epoch: 292 [0/50000 (0%)]\tLoss: 0.804273\tAcc: 86.72%\n",
            "Train Epoch: 292 [12800/50000 (26%)]\tLoss: 0.898883\tAcc: 85.16%\n",
            "Train Epoch: 292 [25600/50000 (51%)]\tLoss: 0.835082\tAcc: 85.07%\n",
            "Train Epoch: 292 [38400/50000 (77%)]\tLoss: 0.805588\tAcc: 85.11%\n",
            "\n",
            "Test set: Average loss: 0.8174, Accuracy: 8640/10000 (86.40%)\n",
            "\n",
            "Epoch 293: Target Sparsity: 96.84%, Actual Sparsity: 96.81%\n",
            "Train Epoch: 293 [0/50000 (0%)]\tLoss: 0.820218\tAcc: 88.28%\n",
            "Train Epoch: 293 [12800/50000 (26%)]\tLoss: 0.997738\tAcc: 85.18%\n",
            "Train Epoch: 293 [25600/50000 (51%)]\tLoss: 0.873238\tAcc: 85.40%\n",
            "Train Epoch: 293 [38400/50000 (77%)]\tLoss: 0.835321\tAcc: 85.27%\n",
            "\n",
            "Test set: Average loss: 0.9222, Accuracy: 8153/10000 (81.53%)\n",
            "\n",
            "Epoch 294: Target Sparsity: 97.01%, Actual Sparsity: 96.98%\n",
            "Train Epoch: 294 [0/50000 (0%)]\tLoss: 0.832557\tAcc: 86.72%\n",
            "Train Epoch: 294 [12800/50000 (26%)]\tLoss: 0.793384\tAcc: 85.43%\n",
            "Train Epoch: 294 [25600/50000 (51%)]\tLoss: 0.929950\tAcc: 85.23%\n",
            "Train Epoch: 294 [38400/50000 (77%)]\tLoss: 0.845290\tAcc: 85.08%\n",
            "\n",
            "Test set: Average loss: 0.8730, Accuracy: 8477/10000 (84.77%)\n",
            "\n",
            "Epoch 295: Target Sparsity: 97.17%, Actual Sparsity: 97.14%\n",
            "Train Epoch: 295 [0/50000 (0%)]\tLoss: 0.721694\tAcc: 92.19%\n",
            "Train Epoch: 295 [12800/50000 (26%)]\tLoss: 0.835715\tAcc: 84.32%\n",
            "Train Epoch: 295 [25600/50000 (51%)]\tLoss: 0.987541\tAcc: 84.79%\n",
            "Train Epoch: 295 [38400/50000 (77%)]\tLoss: 0.814740\tAcc: 84.95%\n",
            "\n",
            "Test set: Average loss: 0.8907, Accuracy: 8304/10000 (83.04%)\n",
            "\n",
            "Epoch 296: Target Sparsity: 97.34%, Actual Sparsity: 97.31%\n",
            "Train Epoch: 296 [0/50000 (0%)]\tLoss: 0.825988\tAcc: 89.06%\n",
            "Train Epoch: 296 [12800/50000 (26%)]\tLoss: 0.694364\tAcc: 85.31%\n",
            "Train Epoch: 296 [25600/50000 (51%)]\tLoss: 0.692099\tAcc: 85.39%\n",
            "Train Epoch: 296 [38400/50000 (77%)]\tLoss: 0.873439\tAcc: 85.25%\n",
            "\n",
            "Test set: Average loss: 0.8988, Accuracy: 8283/10000 (82.83%)\n",
            "\n",
            "Epoch 297: Target Sparsity: 97.50%, Actual Sparsity: 97.47%\n",
            "Train Epoch: 297 [0/50000 (0%)]\tLoss: 0.782288\tAcc: 90.62%\n",
            "Train Epoch: 297 [12800/50000 (26%)]\tLoss: 0.909361\tAcc: 85.29%\n",
            "Train Epoch: 297 [25600/50000 (51%)]\tLoss: 0.830451\tAcc: 85.18%\n",
            "Train Epoch: 297 [38400/50000 (77%)]\tLoss: 0.913802\tAcc: 85.07%\n",
            "\n",
            "Test set: Average loss: 0.8111, Accuracy: 8684/10000 (86.84%)\n",
            "\n",
            "Epoch 298: Target Sparsity: 97.67%, Actual Sparsity: 97.64%\n",
            "Train Epoch: 298 [0/50000 (0%)]\tLoss: 0.784883\tAcc: 88.28%\n",
            "Train Epoch: 298 [12800/50000 (26%)]\tLoss: 0.788051\tAcc: 85.35%\n",
            "Train Epoch: 298 [25600/50000 (51%)]\tLoss: 0.808753\tAcc: 85.28%\n",
            "Train Epoch: 298 [38400/50000 (77%)]\tLoss: 0.795428\tAcc: 85.07%\n",
            "\n",
            "Test set: Average loss: 0.9374, Accuracy: 8050/10000 (80.50%)\n",
            "\n",
            "Epoch 299: Target Sparsity: 97.83%, Actual Sparsity: 97.80%\n",
            "Train Epoch: 299 [0/50000 (0%)]\tLoss: 0.799275\tAcc: 85.94%\n",
            "Train Epoch: 299 [12800/50000 (26%)]\tLoss: 0.930851\tAcc: 85.29%\n",
            "Train Epoch: 299 [25600/50000 (51%)]\tLoss: 0.963560\tAcc: 85.35%\n",
            "Train Epoch: 299 [38400/50000 (77%)]\tLoss: 0.845425\tAcc: 85.16%\n",
            "\n",
            "Test set: Average loss: 0.8491, Accuracy: 8490/10000 (84.90%)\n",
            "\n",
            "Epoch 300: Target Sparsity: 98.00%, Actual Sparsity: 97.97%\n",
            "Train Epoch: 300 [0/50000 (0%)]\tLoss: 0.747471\tAcc: 89.84%\n",
            "Train Epoch: 300 [12800/50000 (26%)]\tLoss: 0.778068\tAcc: 85.16%\n",
            "Train Epoch: 300 [25600/50000 (51%)]\tLoss: 0.752690\tAcc: 85.22%\n",
            "Train Epoch: 300 [38400/50000 (77%)]\tLoss: 0.783879\tAcc: 85.11%\n",
            "\n",
            "Test set: Average loss: 0.8667, Accuracy: 8429/10000 (84.29%)\n",
            "\n",
            "Epoch 301: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 301 [0/50000 (0%)]\tLoss: 0.817202\tAcc: 83.59%\n",
            "Train Epoch: 301 [12800/50000 (26%)]\tLoss: 0.875492\tAcc: 85.06%\n",
            "Train Epoch: 301 [25600/50000 (51%)]\tLoss: 0.902793\tAcc: 84.93%\n",
            "Train Epoch: 301 [38400/50000 (77%)]\tLoss: 0.795576\tAcc: 84.82%\n",
            "\n",
            "Test set: Average loss: 0.8319, Accuracy: 8574/10000 (85.74%)\n",
            "\n",
            "Epoch 302: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 302 [0/50000 (0%)]\tLoss: 0.808608\tAcc: 85.94%\n",
            "Train Epoch: 302 [12800/50000 (26%)]\tLoss: 0.827484\tAcc: 84.75%\n",
            "Train Epoch: 302 [25600/50000 (51%)]\tLoss: 0.787420\tAcc: 84.79%\n",
            "Train Epoch: 302 [38400/50000 (77%)]\tLoss: 0.812837\tAcc: 84.91%\n",
            "\n",
            "Test set: Average loss: 0.8242, Accuracy: 8606/10000 (86.06%)\n",
            "\n",
            "Epoch 303: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 303 [0/50000 (0%)]\tLoss: 0.787394\tAcc: 87.50%\n",
            "Train Epoch: 303 [12800/50000 (26%)]\tLoss: 0.822359\tAcc: 85.27%\n",
            "Train Epoch: 303 [25600/50000 (51%)]\tLoss: 0.767224\tAcc: 85.06%\n",
            "Train Epoch: 303 [38400/50000 (77%)]\tLoss: 1.030014\tAcc: 84.90%\n",
            "\n",
            "Test set: Average loss: 0.8494, Accuracy: 8504/10000 (85.04%)\n",
            "\n",
            "Epoch 304: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 304 [0/50000 (0%)]\tLoss: 0.828321\tAcc: 87.50%\n",
            "Train Epoch: 304 [12800/50000 (26%)]\tLoss: 0.816644\tAcc: 85.16%\n",
            "Train Epoch: 304 [25600/50000 (51%)]\tLoss: 0.938185\tAcc: 84.72%\n",
            "Train Epoch: 304 [38400/50000 (77%)]\tLoss: 0.743684\tAcc: 84.58%\n",
            "\n",
            "Test set: Average loss: 0.8840, Accuracy: 8376/10000 (83.76%)\n",
            "\n",
            "Epoch 305: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 305 [0/50000 (0%)]\tLoss: 0.790871\tAcc: 88.28%\n",
            "Train Epoch: 305 [12800/50000 (26%)]\tLoss: 0.838950\tAcc: 84.90%\n",
            "Train Epoch: 305 [25600/50000 (51%)]\tLoss: 0.912846\tAcc: 84.82%\n",
            "Train Epoch: 305 [38400/50000 (77%)]\tLoss: 0.946241\tAcc: 84.74%\n",
            "\n",
            "Test set: Average loss: 0.8673, Accuracy: 8430/10000 (84.30%)\n",
            "\n",
            "Epoch 306: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 306 [0/50000 (0%)]\tLoss: 0.866110\tAcc: 85.16%\n",
            "Train Epoch: 306 [12800/50000 (26%)]\tLoss: 0.776042\tAcc: 84.56%\n",
            "Train Epoch: 306 [25600/50000 (51%)]\tLoss: 0.847983\tAcc: 84.65%\n",
            "Train Epoch: 306 [38400/50000 (77%)]\tLoss: 0.940768\tAcc: 84.62%\n",
            "\n",
            "Test set: Average loss: 0.8354, Accuracy: 8597/10000 (85.97%)\n",
            "\n",
            "Epoch 307: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 307 [0/50000 (0%)]\tLoss: 0.820285\tAcc: 83.59%\n",
            "Train Epoch: 307 [12800/50000 (26%)]\tLoss: 0.801790\tAcc: 85.40%\n",
            "Train Epoch: 307 [25600/50000 (51%)]\tLoss: 0.778299\tAcc: 84.86%\n",
            "Train Epoch: 307 [38400/50000 (77%)]\tLoss: 0.816695\tAcc: 84.92%\n",
            "\n",
            "Test set: Average loss: 0.8758, Accuracy: 8380/10000 (83.80%)\n",
            "\n",
            "Epoch 308: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 308 [0/50000 (0%)]\tLoss: 0.805287\tAcc: 87.50%\n",
            "Train Epoch: 308 [12800/50000 (26%)]\tLoss: 0.815193\tAcc: 84.92%\n",
            "Train Epoch: 308 [25600/50000 (51%)]\tLoss: 0.792443\tAcc: 84.51%\n",
            "Train Epoch: 308 [38400/50000 (77%)]\tLoss: 0.827954\tAcc: 84.61%\n",
            "\n",
            "Test set: Average loss: 0.7911, Accuracy: 8757/10000 (87.57%)\n",
            "\n",
            "Epoch 309: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 309 [0/50000 (0%)]\tLoss: 0.782723\tAcc: 88.28%\n",
            "Train Epoch: 309 [12800/50000 (26%)]\tLoss: 0.816011\tAcc: 85.43%\n",
            "Train Epoch: 309 [25600/50000 (51%)]\tLoss: 0.818328\tAcc: 84.94%\n",
            "Train Epoch: 309 [38400/50000 (77%)]\tLoss: 0.815010\tAcc: 84.96%\n",
            "\n",
            "Test set: Average loss: 0.9160, Accuracy: 8296/10000 (82.96%)\n",
            "\n",
            "Epoch 310: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 310 [0/50000 (0%)]\tLoss: 0.898679\tAcc: 82.03%\n",
            "Train Epoch: 310 [12800/50000 (26%)]\tLoss: 0.810785\tAcc: 84.58%\n",
            "Train Epoch: 310 [25600/50000 (51%)]\tLoss: 0.919356\tAcc: 84.65%\n",
            "Train Epoch: 310 [38400/50000 (77%)]\tLoss: 0.920502\tAcc: 84.62%\n",
            "\n",
            "Test set: Average loss: 0.8436, Accuracy: 8537/10000 (85.37%)\n",
            "\n",
            "Epoch 311: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 311 [0/50000 (0%)]\tLoss: 0.808566\tAcc: 82.03%\n",
            "Train Epoch: 311 [12800/50000 (26%)]\tLoss: 0.852397\tAcc: 85.16%\n",
            "Train Epoch: 311 [25600/50000 (51%)]\tLoss: 0.829776\tAcc: 85.14%\n",
            "Train Epoch: 311 [38400/50000 (77%)]\tLoss: 0.757699\tAcc: 85.13%\n",
            "\n",
            "Test set: Average loss: 0.8959, Accuracy: 8314/10000 (83.14%)\n",
            "\n",
            "Epoch 312: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 312 [0/50000 (0%)]\tLoss: 0.874097\tAcc: 84.38%\n",
            "Train Epoch: 312 [12800/50000 (26%)]\tLoss: 0.920655\tAcc: 85.21%\n",
            "Train Epoch: 312 [25600/50000 (51%)]\tLoss: 0.849358\tAcc: 85.31%\n",
            "Train Epoch: 312 [38400/50000 (77%)]\tLoss: 0.857470\tAcc: 85.12%\n",
            "\n",
            "Test set: Average loss: 0.8095, Accuracy: 8679/10000 (86.79%)\n",
            "\n",
            "Epoch 313: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 313 [0/50000 (0%)]\tLoss: 0.814173\tAcc: 87.50%\n",
            "Train Epoch: 313 [12800/50000 (26%)]\tLoss: 0.960150\tAcc: 85.23%\n",
            "Train Epoch: 313 [25600/50000 (51%)]\tLoss: 0.862072\tAcc: 85.50%\n",
            "Train Epoch: 313 [38400/50000 (77%)]\tLoss: 0.866408\tAcc: 85.31%\n",
            "\n",
            "Test set: Average loss: 0.8310, Accuracy: 8580/10000 (85.80%)\n",
            "\n",
            "Epoch 314: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 314 [0/50000 (0%)]\tLoss: 0.881276\tAcc: 88.28%\n",
            "Train Epoch: 314 [12800/50000 (26%)]\tLoss: 0.897629\tAcc: 85.78%\n",
            "Train Epoch: 314 [25600/50000 (51%)]\tLoss: 0.861091\tAcc: 85.63%\n",
            "Train Epoch: 314 [38400/50000 (77%)]\tLoss: 0.809730\tAcc: 85.42%\n",
            "\n",
            "Test set: Average loss: 0.8191, Accuracy: 8632/10000 (86.32%)\n",
            "\n",
            "Epoch 315: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 315 [0/50000 (0%)]\tLoss: 0.830414\tAcc: 85.94%\n",
            "Train Epoch: 315 [12800/50000 (26%)]\tLoss: 0.788232\tAcc: 85.61%\n",
            "Train Epoch: 315 [25600/50000 (51%)]\tLoss: 0.769583\tAcc: 85.57%\n",
            "Train Epoch: 315 [38400/50000 (77%)]\tLoss: 0.852579\tAcc: 85.46%\n",
            "\n",
            "Test set: Average loss: 0.8084, Accuracy: 8692/10000 (86.92%)\n",
            "\n",
            "Epoch 316: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 316 [0/50000 (0%)]\tLoss: 0.803677\tAcc: 89.06%\n",
            "Train Epoch: 316 [12800/50000 (26%)]\tLoss: 0.936211\tAcc: 85.91%\n",
            "Train Epoch: 316 [25600/50000 (51%)]\tLoss: 0.875481\tAcc: 85.75%\n",
            "Train Epoch: 316 [38400/50000 (77%)]\tLoss: 0.893678\tAcc: 85.68%\n",
            "\n",
            "Test set: Average loss: 0.8491, Accuracy: 8478/10000 (84.78%)\n",
            "\n",
            "Epoch 317: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 317 [0/50000 (0%)]\tLoss: 0.848739\tAcc: 83.59%\n",
            "Train Epoch: 317 [12800/50000 (26%)]\tLoss: 0.837345\tAcc: 86.29%\n",
            "Train Epoch: 317 [25600/50000 (51%)]\tLoss: 0.877627\tAcc: 86.07%\n",
            "Train Epoch: 317 [38400/50000 (77%)]\tLoss: 0.825435\tAcc: 85.77%\n",
            "\n",
            "Test set: Average loss: 0.8073, Accuracy: 8684/10000 (86.84%)\n",
            "\n",
            "Epoch 318: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 318 [0/50000 (0%)]\tLoss: 0.716678\tAcc: 89.84%\n",
            "Train Epoch: 318 [12800/50000 (26%)]\tLoss: 0.787406\tAcc: 86.48%\n",
            "Train Epoch: 318 [25600/50000 (51%)]\tLoss: 0.905445\tAcc: 86.49%\n",
            "Train Epoch: 318 [38400/50000 (77%)]\tLoss: 0.783853\tAcc: 86.24%\n",
            "\n",
            "Test set: Average loss: 0.8573, Accuracy: 8434/10000 (84.34%)\n",
            "\n",
            "Epoch 319: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 319 [0/50000 (0%)]\tLoss: 0.776139\tAcc: 89.06%\n",
            "Train Epoch: 319 [12800/50000 (26%)]\tLoss: 0.752531\tAcc: 86.23%\n",
            "Train Epoch: 319 [25600/50000 (51%)]\tLoss: 0.742346\tAcc: 86.53%\n",
            "Train Epoch: 319 [38400/50000 (77%)]\tLoss: 0.840281\tAcc: 86.39%\n",
            "\n",
            "Test set: Average loss: 0.7746, Accuracy: 8847/10000 (88.47%)\n",
            "\n",
            "Epoch 320: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 320 [0/50000 (0%)]\tLoss: 0.847373\tAcc: 84.38%\n",
            "Train Epoch: 320 [12800/50000 (26%)]\tLoss: 0.785238\tAcc: 86.59%\n",
            "Train Epoch: 320 [25600/50000 (51%)]\tLoss: 0.826529\tAcc: 86.54%\n",
            "Train Epoch: 320 [38400/50000 (77%)]\tLoss: 0.834448\tAcc: 86.47%\n",
            "\n",
            "Test set: Average loss: 0.8748, Accuracy: 8387/10000 (83.87%)\n",
            "\n",
            "Epoch 321: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 321 [0/50000 (0%)]\tLoss: 0.824295\tAcc: 88.28%\n",
            "Train Epoch: 321 [12800/50000 (26%)]\tLoss: 0.804009\tAcc: 87.23%\n",
            "Train Epoch: 321 [25600/50000 (51%)]\tLoss: 0.844408\tAcc: 87.07%\n",
            "Train Epoch: 321 [38400/50000 (77%)]\tLoss: 0.921912\tAcc: 86.91%\n",
            "\n",
            "Test set: Average loss: 0.8058, Accuracy: 8666/10000 (86.66%)\n",
            "\n",
            "Epoch 322: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 322 [0/50000 (0%)]\tLoss: 0.758754\tAcc: 90.62%\n",
            "Train Epoch: 322 [12800/50000 (26%)]\tLoss: 0.780582\tAcc: 86.88%\n",
            "Train Epoch: 322 [25600/50000 (51%)]\tLoss: 0.797755\tAcc: 86.88%\n",
            "Train Epoch: 322 [38400/50000 (77%)]\tLoss: 0.842779\tAcc: 86.96%\n",
            "\n",
            "Test set: Average loss: 0.7703, Accuracy: 8866/10000 (88.66%)\n",
            "\n",
            "Epoch 323: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 323 [0/50000 (0%)]\tLoss: 0.787557\tAcc: 85.94%\n",
            "Train Epoch: 323 [12800/50000 (26%)]\tLoss: 0.767025\tAcc: 87.21%\n",
            "Train Epoch: 323 [25600/50000 (51%)]\tLoss: 0.696197\tAcc: 87.23%\n",
            "Train Epoch: 323 [38400/50000 (77%)]\tLoss: 0.764457\tAcc: 87.15%\n",
            "\n",
            "Test set: Average loss: 0.7671, Accuracy: 8862/10000 (88.62%)\n",
            "\n",
            "Epoch 324: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 324 [0/50000 (0%)]\tLoss: 0.821601\tAcc: 84.38%\n",
            "Train Epoch: 324 [12800/50000 (26%)]\tLoss: 0.747436\tAcc: 87.25%\n",
            "Train Epoch: 324 [25600/50000 (51%)]\tLoss: 0.722283\tAcc: 87.31%\n",
            "Train Epoch: 324 [38400/50000 (77%)]\tLoss: 0.779312\tAcc: 87.40%\n",
            "\n",
            "Test set: Average loss: 0.7850, Accuracy: 8755/10000 (87.55%)\n",
            "\n",
            "Epoch 325: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 325 [0/50000 (0%)]\tLoss: 0.744385\tAcc: 90.62%\n",
            "Train Epoch: 325 [12800/50000 (26%)]\tLoss: 0.738008\tAcc: 88.42%\n",
            "Train Epoch: 325 [25600/50000 (51%)]\tLoss: 0.813392\tAcc: 88.11%\n",
            "Train Epoch: 325 [38400/50000 (77%)]\tLoss: 0.893890\tAcc: 88.01%\n",
            "\n",
            "Test set: Average loss: 0.7878, Accuracy: 8783/10000 (87.83%)\n",
            "\n",
            "Epoch 326: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 326 [0/50000 (0%)]\tLoss: 0.718649\tAcc: 90.62%\n",
            "Train Epoch: 326 [12800/50000 (26%)]\tLoss: 0.808946\tAcc: 88.08%\n",
            "Train Epoch: 326 [25600/50000 (51%)]\tLoss: 0.782791\tAcc: 88.43%\n",
            "Train Epoch: 326 [38400/50000 (77%)]\tLoss: 0.924729\tAcc: 88.39%\n",
            "\n",
            "Test set: Average loss: 0.7815, Accuracy: 8783/10000 (87.83%)\n",
            "\n",
            "Epoch 327: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 327 [0/50000 (0%)]\tLoss: 0.756052\tAcc: 87.50%\n",
            "Train Epoch: 327 [12800/50000 (26%)]\tLoss: 0.761615\tAcc: 88.76%\n",
            "Train Epoch: 327 [25600/50000 (51%)]\tLoss: 0.962448\tAcc: 88.35%\n",
            "Train Epoch: 327 [38400/50000 (77%)]\tLoss: 0.782388\tAcc: 88.41%\n",
            "\n",
            "Test set: Average loss: 0.8150, Accuracy: 8655/10000 (86.55%)\n",
            "\n",
            "Epoch 328: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 328 [0/50000 (0%)]\tLoss: 0.780238\tAcc: 90.62%\n",
            "Train Epoch: 328 [12800/50000 (26%)]\tLoss: 0.745739\tAcc: 89.09%\n",
            "Train Epoch: 328 [25600/50000 (51%)]\tLoss: 0.795100\tAcc: 88.79%\n",
            "Train Epoch: 328 [38400/50000 (77%)]\tLoss: 0.720669\tAcc: 88.81%\n",
            "\n",
            "Test set: Average loss: 0.7981, Accuracy: 8735/10000 (87.35%)\n",
            "\n",
            "Epoch 329: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 329 [0/50000 (0%)]\tLoss: 0.723532\tAcc: 92.19%\n",
            "Train Epoch: 329 [12800/50000 (26%)]\tLoss: 0.793551\tAcc: 89.31%\n",
            "Train Epoch: 329 [25600/50000 (51%)]\tLoss: 0.799281\tAcc: 89.15%\n",
            "Train Epoch: 329 [38400/50000 (77%)]\tLoss: 0.696362\tAcc: 89.13%\n",
            "\n",
            "Test set: Average loss: 0.7412, Accuracy: 8958/10000 (89.58%)\n",
            "\n",
            "Epoch 330: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 330 [0/50000 (0%)]\tLoss: 0.721978\tAcc: 90.62%\n",
            "Train Epoch: 330 [12800/50000 (26%)]\tLoss: 0.781364\tAcc: 89.60%\n",
            "Train Epoch: 330 [25600/50000 (51%)]\tLoss: 0.771851\tAcc: 89.59%\n",
            "Train Epoch: 330 [38400/50000 (77%)]\tLoss: 0.726564\tAcc: 89.48%\n",
            "\n",
            "Test set: Average loss: 0.7225, Accuracy: 9057/10000 (90.57%)\n",
            "\n",
            "Epoch 331: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 331 [0/50000 (0%)]\tLoss: 0.738460\tAcc: 86.72%\n",
            "Train Epoch: 331 [12800/50000 (26%)]\tLoss: 0.680948\tAcc: 89.54%\n",
            "Train Epoch: 331 [25600/50000 (51%)]\tLoss: 0.765021\tAcc: 89.91%\n",
            "Train Epoch: 331 [38400/50000 (77%)]\tLoss: 0.744741\tAcc: 89.79%\n",
            "\n",
            "Test set: Average loss: 0.7335, Accuracy: 9008/10000 (90.08%)\n",
            "\n",
            "Epoch 332: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 332 [0/50000 (0%)]\tLoss: 0.725776\tAcc: 92.19%\n",
            "Train Epoch: 332 [12800/50000 (26%)]\tLoss: 0.774827\tAcc: 90.16%\n",
            "Train Epoch: 332 [25600/50000 (51%)]\tLoss: 0.720390\tAcc: 90.12%\n",
            "Train Epoch: 332 [38400/50000 (77%)]\tLoss: 0.766190\tAcc: 89.98%\n",
            "\n",
            "Test set: Average loss: 0.7264, Accuracy: 9032/10000 (90.32%)\n",
            "\n",
            "Epoch 333: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 333 [0/50000 (0%)]\tLoss: 0.716416\tAcc: 92.19%\n",
            "Train Epoch: 333 [12800/50000 (26%)]\tLoss: 0.664904\tAcc: 91.11%\n",
            "Train Epoch: 333 [25600/50000 (51%)]\tLoss: 0.742229\tAcc: 90.65%\n",
            "Train Epoch: 333 [38400/50000 (77%)]\tLoss: 0.702687\tAcc: 90.48%\n",
            "\n",
            "Test set: Average loss: 0.7133, Accuracy: 9117/10000 (91.17%)\n",
            "\n",
            "Epoch 334: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 334 [0/50000 (0%)]\tLoss: 0.669053\tAcc: 91.41%\n",
            "Train Epoch: 334 [12800/50000 (26%)]\tLoss: 0.714554\tAcc: 91.00%\n",
            "Train Epoch: 334 [25600/50000 (51%)]\tLoss: 0.703238\tAcc: 90.87%\n",
            "Train Epoch: 334 [38400/50000 (77%)]\tLoss: 0.679952\tAcc: 90.79%\n",
            "\n",
            "Test set: Average loss: 0.7371, Accuracy: 9002/10000 (90.02%)\n",
            "\n",
            "Epoch 335: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 335 [0/50000 (0%)]\tLoss: 0.722660\tAcc: 89.84%\n",
            "Train Epoch: 335 [12800/50000 (26%)]\tLoss: 0.758734\tAcc: 91.17%\n",
            "Train Epoch: 335 [25600/50000 (51%)]\tLoss: 0.660795\tAcc: 91.28%\n",
            "Train Epoch: 335 [38400/50000 (77%)]\tLoss: 0.748425\tAcc: 91.21%\n",
            "\n",
            "Test set: Average loss: 0.7012, Accuracy: 9147/10000 (91.47%)\n",
            "\n",
            "Epoch 336: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 336 [0/50000 (0%)]\tLoss: 0.729843\tAcc: 90.62%\n",
            "Train Epoch: 336 [12800/50000 (26%)]\tLoss: 0.683518\tAcc: 91.51%\n",
            "Train Epoch: 336 [25600/50000 (51%)]\tLoss: 0.769329\tAcc: 91.42%\n",
            "Train Epoch: 336 [38400/50000 (77%)]\tLoss: 0.667023\tAcc: 91.57%\n",
            "\n",
            "Test set: Average loss: 0.6981, Accuracy: 9150/10000 (91.50%)\n",
            "\n",
            "Epoch 337: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 337 [0/50000 (0%)]\tLoss: 0.801591\tAcc: 88.28%\n",
            "Train Epoch: 337 [12800/50000 (26%)]\tLoss: 0.731465\tAcc: 91.85%\n",
            "Train Epoch: 337 [25600/50000 (51%)]\tLoss: 0.731106\tAcc: 91.84%\n",
            "Train Epoch: 337 [38400/50000 (77%)]\tLoss: 0.659809\tAcc: 91.95%\n",
            "\n",
            "Test set: Average loss: 0.7127, Accuracy: 9129/10000 (91.29%)\n",
            "\n",
            "Epoch 338: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 338 [0/50000 (0%)]\tLoss: 0.718897\tAcc: 92.19%\n",
            "Train Epoch: 338 [12800/50000 (26%)]\tLoss: 0.745952\tAcc: 92.23%\n",
            "Train Epoch: 338 [25600/50000 (51%)]\tLoss: 0.713326\tAcc: 92.37%\n",
            "Train Epoch: 338 [38400/50000 (77%)]\tLoss: 0.697358\tAcc: 92.36%\n",
            "\n",
            "Test set: Average loss: 0.6829, Accuracy: 9233/10000 (92.33%)\n",
            "\n",
            "Epoch 339: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 339 [0/50000 (0%)]\tLoss: 0.629527\tAcc: 93.75%\n",
            "Train Epoch: 339 [12800/50000 (26%)]\tLoss: 0.671970\tAcc: 93.13%\n",
            "Train Epoch: 339 [25600/50000 (51%)]\tLoss: 0.719733\tAcc: 92.98%\n",
            "Train Epoch: 339 [38400/50000 (77%)]\tLoss: 0.668868\tAcc: 92.82%\n",
            "\n",
            "Test set: Average loss: 0.6942, Accuracy: 9192/10000 (91.92%)\n",
            "\n",
            "Epoch 340: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 340 [0/50000 (0%)]\tLoss: 0.683269\tAcc: 90.62%\n",
            "Train Epoch: 340 [12800/50000 (26%)]\tLoss: 0.676705\tAcc: 93.66%\n",
            "Train Epoch: 340 [25600/50000 (51%)]\tLoss: 0.697229\tAcc: 93.46%\n",
            "Train Epoch: 340 [38400/50000 (77%)]\tLoss: 0.711366\tAcc: 93.32%\n",
            "\n",
            "Test set: Average loss: 0.6809, Accuracy: 9250/10000 (92.50%)\n",
            "\n",
            "Epoch 341: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 341 [0/50000 (0%)]\tLoss: 0.625324\tAcc: 96.88%\n",
            "Train Epoch: 341 [12800/50000 (26%)]\tLoss: 0.624824\tAcc: 93.77%\n",
            "Train Epoch: 341 [25600/50000 (51%)]\tLoss: 0.669788\tAcc: 93.59%\n",
            "Train Epoch: 341 [38400/50000 (77%)]\tLoss: 0.728310\tAcc: 93.58%\n",
            "\n",
            "Test set: Average loss: 0.6741, Accuracy: 9278/10000 (92.78%)\n",
            "\n",
            "Epoch 342: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 342 [0/50000 (0%)]\tLoss: 0.656733\tAcc: 93.75%\n",
            "Train Epoch: 342 [12800/50000 (26%)]\tLoss: 0.637070\tAcc: 93.98%\n",
            "Train Epoch: 342 [25600/50000 (51%)]\tLoss: 0.583764\tAcc: 93.86%\n",
            "Train Epoch: 342 [38400/50000 (77%)]\tLoss: 0.661942\tAcc: 93.80%\n",
            "\n",
            "Test set: Average loss: 0.6751, Accuracy: 9274/10000 (92.74%)\n",
            "\n",
            "Epoch 343: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 343 [0/50000 (0%)]\tLoss: 0.591425\tAcc: 96.09%\n",
            "Train Epoch: 343 [12800/50000 (26%)]\tLoss: 0.616112\tAcc: 94.27%\n",
            "Train Epoch: 343 [25600/50000 (51%)]\tLoss: 0.704458\tAcc: 94.23%\n",
            "Train Epoch: 343 [38400/50000 (77%)]\tLoss: 0.645049\tAcc: 94.18%\n",
            "\n",
            "Test set: Average loss: 0.6705, Accuracy: 9300/10000 (93.00%)\n",
            "\n",
            "Epoch 344: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 344 [0/50000 (0%)]\tLoss: 0.624819\tAcc: 95.31%\n",
            "Train Epoch: 344 [12800/50000 (26%)]\tLoss: 0.617231\tAcc: 94.55%\n",
            "Train Epoch: 344 [25600/50000 (51%)]\tLoss: 0.571851\tAcc: 94.43%\n",
            "Train Epoch: 344 [38400/50000 (77%)]\tLoss: 0.696588\tAcc: 94.35%\n",
            "\n",
            "Test set: Average loss: 0.6644, Accuracy: 9320/10000 (93.20%)\n",
            "\n",
            "Epoch 345: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 345 [0/50000 (0%)]\tLoss: 0.609093\tAcc: 96.88%\n",
            "Train Epoch: 345 [12800/50000 (26%)]\tLoss: 0.595925\tAcc: 94.61%\n",
            "Train Epoch: 345 [25600/50000 (51%)]\tLoss: 0.613033\tAcc: 94.69%\n",
            "Train Epoch: 345 [38400/50000 (77%)]\tLoss: 0.637759\tAcc: 94.59%\n",
            "\n",
            "Test set: Average loss: 0.6600, Accuracy: 9332/10000 (93.32%)\n",
            "\n",
            "Epoch 346: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 346 [0/50000 (0%)]\tLoss: 0.638220\tAcc: 96.09%\n",
            "Train Epoch: 346 [12800/50000 (26%)]\tLoss: 0.629193\tAcc: 94.73%\n",
            "Train Epoch: 346 [25600/50000 (51%)]\tLoss: 0.596142\tAcc: 94.77%\n",
            "Train Epoch: 346 [38400/50000 (77%)]\tLoss: 0.576759\tAcc: 94.65%\n",
            "\n",
            "Test set: Average loss: 0.6614, Accuracy: 9341/10000 (93.41%)\n",
            "\n",
            "Epoch 347: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 347 [0/50000 (0%)]\tLoss: 0.645383\tAcc: 92.97%\n",
            "Train Epoch: 347 [12800/50000 (26%)]\tLoss: 0.607651\tAcc: 94.94%\n",
            "Train Epoch: 347 [25600/50000 (51%)]\tLoss: 0.632715\tAcc: 95.11%\n",
            "Train Epoch: 347 [38400/50000 (77%)]\tLoss: 0.625272\tAcc: 95.09%\n",
            "\n",
            "Test set: Average loss: 0.6610, Accuracy: 9341/10000 (93.41%)\n",
            "\n",
            "Epoch 348: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 348 [0/50000 (0%)]\tLoss: 0.672743\tAcc: 92.19%\n",
            "Train Epoch: 348 [12800/50000 (26%)]\tLoss: 0.585656\tAcc: 95.42%\n",
            "Train Epoch: 348 [25600/50000 (51%)]\tLoss: 0.613474\tAcc: 95.27%\n",
            "Train Epoch: 348 [38400/50000 (77%)]\tLoss: 0.598519\tAcc: 95.19%\n",
            "\n",
            "Test set: Average loss: 0.6591, Accuracy: 9344/10000 (93.44%)\n",
            "\n",
            "Epoch 349: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 349 [0/50000 (0%)]\tLoss: 0.632380\tAcc: 94.53%\n",
            "Train Epoch: 349 [12800/50000 (26%)]\tLoss: 0.626467\tAcc: 95.34%\n",
            "Train Epoch: 349 [25600/50000 (51%)]\tLoss: 0.663590\tAcc: 95.32%\n",
            "Train Epoch: 349 [38400/50000 (77%)]\tLoss: 0.663389\tAcc: 95.28%\n",
            "\n",
            "Test set: Average loss: 0.6578, Accuracy: 9341/10000 (93.41%)\n",
            "\n",
            "Epoch 350: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 350 [0/50000 (0%)]\tLoss: 0.646922\tAcc: 93.75%\n",
            "Train Epoch: 350 [12800/50000 (26%)]\tLoss: 0.666959\tAcc: 95.23%\n",
            "Train Epoch: 350 [25600/50000 (51%)]\tLoss: 0.607628\tAcc: 95.36%\n",
            "Train Epoch: 350 [38400/50000 (77%)]\tLoss: 0.700397\tAcc: 95.37%\n",
            "\n",
            "Test set: Average loss: 0.6572, Accuracy: 9334/10000 (93.34%)\n",
            "\n",
            "Epoch 351: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 351 [0/50000 (0%)]\tLoss: 0.598271\tAcc: 96.88%\n",
            "Train Epoch: 351 [12800/50000 (26%)]\tLoss: 0.611455\tAcc: 95.22%\n",
            "Train Epoch: 351 [25600/50000 (51%)]\tLoss: 0.605082\tAcc: 95.05%\n",
            "Train Epoch: 351 [38400/50000 (77%)]\tLoss: 0.610245\tAcc: 95.19%\n",
            "\n",
            "Test set: Average loss: 0.6577, Accuracy: 9343/10000 (93.43%)\n",
            "\n",
            "Epoch 352: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 352 [0/50000 (0%)]\tLoss: 0.585461\tAcc: 96.88%\n",
            "Train Epoch: 352 [12800/50000 (26%)]\tLoss: 0.621992\tAcc: 95.52%\n",
            "Train Epoch: 352 [25600/50000 (51%)]\tLoss: 0.636818\tAcc: 95.54%\n",
            "Train Epoch: 352 [38400/50000 (77%)]\tLoss: 0.561115\tAcc: 95.44%\n",
            "\n",
            "Test set: Average loss: 0.6572, Accuracy: 9342/10000 (93.42%)\n",
            "\n",
            "Epoch 353: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 353 [0/50000 (0%)]\tLoss: 0.660101\tAcc: 93.75%\n",
            "Train Epoch: 353 [12800/50000 (26%)]\tLoss: 0.628993\tAcc: 95.00%\n",
            "Train Epoch: 353 [25600/50000 (51%)]\tLoss: 0.642191\tAcc: 95.22%\n",
            "Train Epoch: 353 [38400/50000 (77%)]\tLoss: 0.589008\tAcc: 95.21%\n",
            "\n",
            "Test set: Average loss: 0.6576, Accuracy: 9336/10000 (93.36%)\n",
            "\n",
            "Epoch 354: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 354 [0/50000 (0%)]\tLoss: 0.623988\tAcc: 95.31%\n",
            "Train Epoch: 354 [12800/50000 (26%)]\tLoss: 0.639810\tAcc: 95.39%\n",
            "Train Epoch: 354 [25600/50000 (51%)]\tLoss: 0.637504\tAcc: 95.21%\n",
            "Train Epoch: 354 [38400/50000 (77%)]\tLoss: 0.650594\tAcc: 95.15%\n",
            "\n",
            "Test set: Average loss: 0.6580, Accuracy: 9341/10000 (93.41%)\n",
            "\n",
            "Epoch 355: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 355 [0/50000 (0%)]\tLoss: 0.589804\tAcc: 96.88%\n",
            "Train Epoch: 355 [12800/50000 (26%)]\tLoss: 0.648573\tAcc: 95.55%\n",
            "Train Epoch: 355 [25600/50000 (51%)]\tLoss: 0.671591\tAcc: 95.51%\n",
            "Train Epoch: 355 [38400/50000 (77%)]\tLoss: 0.643699\tAcc: 95.39%\n",
            "\n",
            "Test set: Average loss: 0.6574, Accuracy: 9343/10000 (93.43%)\n",
            "\n",
            "Epoch 356: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 356 [0/50000 (0%)]\tLoss: 0.606662\tAcc: 96.09%\n",
            "Train Epoch: 356 [12800/50000 (26%)]\tLoss: 0.549115\tAcc: 95.32%\n",
            "Train Epoch: 356 [25600/50000 (51%)]\tLoss: 0.642965\tAcc: 95.20%\n",
            "Train Epoch: 356 [38400/50000 (77%)]\tLoss: 0.643313\tAcc: 95.21%\n",
            "\n",
            "Test set: Average loss: 0.6617, Accuracy: 9324/10000 (93.24%)\n",
            "\n",
            "Epoch 357: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 357 [0/50000 (0%)]\tLoss: 0.635629\tAcc: 94.53%\n",
            "Train Epoch: 357 [12800/50000 (26%)]\tLoss: 0.601238\tAcc: 95.20%\n",
            "Train Epoch: 357 [25600/50000 (51%)]\tLoss: 0.635632\tAcc: 95.02%\n",
            "Train Epoch: 357 [38400/50000 (77%)]\tLoss: 0.612380\tAcc: 94.85%\n",
            "\n",
            "Test set: Average loss: 0.6608, Accuracy: 9336/10000 (93.36%)\n",
            "\n",
            "Epoch 358: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 358 [0/50000 (0%)]\tLoss: 0.605687\tAcc: 96.09%\n",
            "Train Epoch: 358 [12800/50000 (26%)]\tLoss: 0.681955\tAcc: 95.07%\n",
            "Train Epoch: 358 [25600/50000 (51%)]\tLoss: 0.647029\tAcc: 94.97%\n",
            "Train Epoch: 358 [38400/50000 (77%)]\tLoss: 0.619865\tAcc: 94.84%\n",
            "\n",
            "Test set: Average loss: 0.6668, Accuracy: 9319/10000 (93.19%)\n",
            "\n",
            "Epoch 359: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 359 [0/50000 (0%)]\tLoss: 0.596601\tAcc: 96.88%\n",
            "Train Epoch: 359 [12800/50000 (26%)]\tLoss: 0.616266\tAcc: 95.22%\n",
            "Train Epoch: 359 [25600/50000 (51%)]\tLoss: 0.627760\tAcc: 94.99%\n",
            "Train Epoch: 359 [38400/50000 (77%)]\tLoss: 0.589756\tAcc: 94.82%\n",
            "\n",
            "Test set: Average loss: 0.6735, Accuracy: 9289/10000 (92.89%)\n",
            "\n",
            "Epoch 360: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 360 [0/50000 (0%)]\tLoss: 0.699692\tAcc: 93.75%\n",
            "Train Epoch: 360 [12800/50000 (26%)]\tLoss: 0.673373\tAcc: 94.46%\n",
            "Train Epoch: 360 [25600/50000 (51%)]\tLoss: 0.647725\tAcc: 94.34%\n",
            "Train Epoch: 360 [38400/50000 (77%)]\tLoss: 0.654339\tAcc: 94.35%\n",
            "\n",
            "Test set: Average loss: 0.6845, Accuracy: 9230/10000 (92.30%)\n",
            "\n",
            "Epoch 361: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 361 [0/50000 (0%)]\tLoss: 0.661679\tAcc: 94.53%\n",
            "Train Epoch: 361 [12800/50000 (26%)]\tLoss: 0.733610\tAcc: 94.10%\n",
            "Train Epoch: 361 [25600/50000 (51%)]\tLoss: 0.631420\tAcc: 94.09%\n",
            "Train Epoch: 361 [38400/50000 (77%)]\tLoss: 0.638965\tAcc: 94.16%\n",
            "\n",
            "Test set: Average loss: 0.6826, Accuracy: 9248/10000 (92.48%)\n",
            "\n",
            "Epoch 362: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 362 [0/50000 (0%)]\tLoss: 0.653003\tAcc: 93.75%\n",
            "Train Epoch: 362 [12800/50000 (26%)]\tLoss: 0.667205\tAcc: 93.84%\n",
            "Train Epoch: 362 [25600/50000 (51%)]\tLoss: 0.683163\tAcc: 93.82%\n",
            "Train Epoch: 362 [38400/50000 (77%)]\tLoss: 0.648989\tAcc: 93.79%\n",
            "\n",
            "Test set: Average loss: 0.6860, Accuracy: 9233/10000 (92.33%)\n",
            "\n",
            "Epoch 363: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 363 [0/50000 (0%)]\tLoss: 0.617003\tAcc: 96.09%\n",
            "Train Epoch: 363 [12800/50000 (26%)]\tLoss: 0.691592\tAcc: 93.29%\n",
            "Train Epoch: 363 [25600/50000 (51%)]\tLoss: 0.641812\tAcc: 93.26%\n",
            "Train Epoch: 363 [38400/50000 (77%)]\tLoss: 0.615804\tAcc: 93.22%\n",
            "\n",
            "Test set: Average loss: 0.7107, Accuracy: 9127/10000 (91.27%)\n",
            "\n",
            "Epoch 364: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 364 [0/50000 (0%)]\tLoss: 0.701119\tAcc: 91.41%\n",
            "Train Epoch: 364 [12800/50000 (26%)]\tLoss: 0.626563\tAcc: 92.80%\n",
            "Train Epoch: 364 [25600/50000 (51%)]\tLoss: 0.668262\tAcc: 92.67%\n",
            "Train Epoch: 364 [38400/50000 (77%)]\tLoss: 0.634183\tAcc: 92.56%\n",
            "\n",
            "Test set: Average loss: 0.7148, Accuracy: 9127/10000 (91.27%)\n",
            "\n",
            "Epoch 365: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 365 [0/50000 (0%)]\tLoss: 0.634026\tAcc: 92.19%\n",
            "Train Epoch: 365 [12800/50000 (26%)]\tLoss: 0.763238\tAcc: 92.54%\n",
            "Train Epoch: 365 [25600/50000 (51%)]\tLoss: 0.648569\tAcc: 92.35%\n",
            "Train Epoch: 365 [38400/50000 (77%)]\tLoss: 0.671887\tAcc: 92.19%\n",
            "\n",
            "Test set: Average loss: 0.7220, Accuracy: 9067/10000 (90.67%)\n",
            "\n",
            "Epoch 366: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 366 [0/50000 (0%)]\tLoss: 0.692968\tAcc: 92.19%\n",
            "Train Epoch: 366 [12800/50000 (26%)]\tLoss: 0.735368\tAcc: 92.11%\n",
            "Train Epoch: 366 [25600/50000 (51%)]\tLoss: 0.743171\tAcc: 91.83%\n",
            "Train Epoch: 366 [38400/50000 (77%)]\tLoss: 0.620956\tAcc: 91.81%\n",
            "\n",
            "Test set: Average loss: 0.7093, Accuracy: 9126/10000 (91.26%)\n",
            "\n",
            "Epoch 367: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 367 [0/50000 (0%)]\tLoss: 0.681806\tAcc: 94.53%\n",
            "Train Epoch: 367 [12800/50000 (26%)]\tLoss: 0.622357\tAcc: 91.27%\n",
            "Train Epoch: 367 [25600/50000 (51%)]\tLoss: 0.754696\tAcc: 91.01%\n",
            "Train Epoch: 367 [38400/50000 (77%)]\tLoss: 0.684978\tAcc: 91.02%\n",
            "\n",
            "Test set: Average loss: 0.7399, Accuracy: 8992/10000 (89.92%)\n",
            "\n",
            "Epoch 368: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 368 [0/50000 (0%)]\tLoss: 0.686609\tAcc: 92.97%\n",
            "Train Epoch: 368 [12800/50000 (26%)]\tLoss: 0.778132\tAcc: 90.94%\n",
            "Train Epoch: 368 [25600/50000 (51%)]\tLoss: 0.789954\tAcc: 90.83%\n",
            "Train Epoch: 368 [38400/50000 (77%)]\tLoss: 0.743299\tAcc: 90.57%\n",
            "\n",
            "Test set: Average loss: 0.7312, Accuracy: 9026/10000 (90.26%)\n",
            "\n",
            "Epoch 369: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 369 [0/50000 (0%)]\tLoss: 0.767714\tAcc: 86.72%\n",
            "Train Epoch: 369 [12800/50000 (26%)]\tLoss: 0.740069\tAcc: 90.71%\n",
            "Train Epoch: 369 [25600/50000 (51%)]\tLoss: 0.807889\tAcc: 90.42%\n",
            "Train Epoch: 369 [38400/50000 (77%)]\tLoss: 0.770969\tAcc: 90.28%\n",
            "\n",
            "Test set: Average loss: 0.7652, Accuracy: 8902/10000 (89.02%)\n",
            "\n",
            "Epoch 370: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 370 [0/50000 (0%)]\tLoss: 0.738657\tAcc: 91.41%\n",
            "Train Epoch: 370 [12800/50000 (26%)]\tLoss: 0.702585\tAcc: 89.98%\n",
            "Train Epoch: 370 [25600/50000 (51%)]\tLoss: 0.746596\tAcc: 89.68%\n",
            "Train Epoch: 370 [38400/50000 (77%)]\tLoss: 0.793907\tAcc: 89.58%\n",
            "\n",
            "Test set: Average loss: 0.7451, Accuracy: 8959/10000 (89.59%)\n",
            "\n",
            "Epoch 371: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 371 [0/50000 (0%)]\tLoss: 0.727054\tAcc: 91.41%\n",
            "Train Epoch: 371 [12800/50000 (26%)]\tLoss: 0.834697\tAcc: 88.99%\n",
            "Train Epoch: 371 [25600/50000 (51%)]\tLoss: 0.769307\tAcc: 88.87%\n",
            "Train Epoch: 371 [38400/50000 (77%)]\tLoss: 0.681024\tAcc: 88.82%\n",
            "\n",
            "Test set: Average loss: 0.7626, Accuracy: 8890/10000 (88.90%)\n",
            "\n",
            "Epoch 372: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 372 [0/50000 (0%)]\tLoss: 0.810929\tAcc: 85.16%\n",
            "Train Epoch: 372 [12800/50000 (26%)]\tLoss: 0.802163\tAcc: 88.23%\n",
            "Train Epoch: 372 [25600/50000 (51%)]\tLoss: 0.839363\tAcc: 88.37%\n",
            "Train Epoch: 372 [38400/50000 (77%)]\tLoss: 0.806780\tAcc: 88.42%\n",
            "\n",
            "Test set: Average loss: 0.7614, Accuracy: 8906/10000 (89.06%)\n",
            "\n",
            "Epoch 373: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 373 [0/50000 (0%)]\tLoss: 0.697730\tAcc: 92.97%\n",
            "Train Epoch: 373 [12800/50000 (26%)]\tLoss: 0.815026\tAcc: 88.47%\n",
            "Train Epoch: 373 [25600/50000 (51%)]\tLoss: 0.803202\tAcc: 88.67%\n",
            "Train Epoch: 373 [38400/50000 (77%)]\tLoss: 0.794414\tAcc: 88.40%\n",
            "\n",
            "Test set: Average loss: 0.7724, Accuracy: 8870/10000 (88.70%)\n",
            "\n",
            "Epoch 374: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 374 [0/50000 (0%)]\tLoss: 0.719730\tAcc: 92.97%\n",
            "Train Epoch: 374 [12800/50000 (26%)]\tLoss: 0.829811\tAcc: 88.30%\n",
            "Train Epoch: 374 [25600/50000 (51%)]\tLoss: 0.950365\tAcc: 88.32%\n",
            "Train Epoch: 374 [38400/50000 (77%)]\tLoss: 0.799913\tAcc: 88.07%\n",
            "\n",
            "Test set: Average loss: 0.7893, Accuracy: 8759/10000 (87.59%)\n",
            "\n",
            "Epoch 375: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 375 [0/50000 (0%)]\tLoss: 0.734231\tAcc: 91.41%\n",
            "Train Epoch: 375 [12800/50000 (26%)]\tLoss: 0.769088\tAcc: 87.81%\n",
            "Train Epoch: 375 [25600/50000 (51%)]\tLoss: 0.839071\tAcc: 87.52%\n",
            "Train Epoch: 375 [38400/50000 (77%)]\tLoss: 0.852255\tAcc: 87.33%\n",
            "\n",
            "Test set: Average loss: 0.7744, Accuracy: 8854/10000 (88.54%)\n",
            "\n",
            "Epoch 376: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 376 [0/50000 (0%)]\tLoss: 0.782492\tAcc: 90.62%\n",
            "Train Epoch: 376 [12800/50000 (26%)]\tLoss: 0.714954\tAcc: 87.00%\n",
            "Train Epoch: 376 [25600/50000 (51%)]\tLoss: 0.813540\tAcc: 87.05%\n",
            "Train Epoch: 376 [38400/50000 (77%)]\tLoss: 0.870777\tAcc: 87.02%\n",
            "\n",
            "Test set: Average loss: 0.8356, Accuracy: 8586/10000 (85.86%)\n",
            "\n",
            "Epoch 377: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 377 [0/50000 (0%)]\tLoss: 0.913419\tAcc: 81.25%\n",
            "Train Epoch: 377 [12800/50000 (26%)]\tLoss: 0.739221\tAcc: 86.73%\n",
            "Train Epoch: 377 [25600/50000 (51%)]\tLoss: 0.781562\tAcc: 86.79%\n",
            "Train Epoch: 377 [38400/50000 (77%)]\tLoss: 0.832562\tAcc: 86.65%\n",
            "\n",
            "Test set: Average loss: 0.8208, Accuracy: 8636/10000 (86.36%)\n",
            "\n",
            "Epoch 378: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 378 [0/50000 (0%)]\tLoss: 0.743398\tAcc: 88.28%\n",
            "Train Epoch: 378 [12800/50000 (26%)]\tLoss: 0.870123\tAcc: 86.79%\n",
            "Train Epoch: 378 [25600/50000 (51%)]\tLoss: 0.828555\tAcc: 86.36%\n",
            "Train Epoch: 378 [38400/50000 (77%)]\tLoss: 0.908709\tAcc: 86.43%\n",
            "\n",
            "Test set: Average loss: 0.7912, Accuracy: 8783/10000 (87.83%)\n",
            "\n",
            "Epoch 379: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 379 [0/50000 (0%)]\tLoss: 0.785944\tAcc: 88.28%\n",
            "Train Epoch: 379 [12800/50000 (26%)]\tLoss: 0.953740\tAcc: 86.60%\n",
            "Train Epoch: 379 [25600/50000 (51%)]\tLoss: 0.833696\tAcc: 86.46%\n",
            "Train Epoch: 379 [38400/50000 (77%)]\tLoss: 0.686061\tAcc: 86.48%\n",
            "\n",
            "Test set: Average loss: 0.8030, Accuracy: 8732/10000 (87.32%)\n",
            "\n",
            "Epoch 380: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 380 [0/50000 (0%)]\tLoss: 0.886013\tAcc: 85.94%\n",
            "Train Epoch: 380 [12800/50000 (26%)]\tLoss: 0.791464\tAcc: 86.49%\n",
            "Train Epoch: 380 [25600/50000 (51%)]\tLoss: 0.901330\tAcc: 86.24%\n",
            "Train Epoch: 380 [38400/50000 (77%)]\tLoss: 0.830904\tAcc: 86.15%\n",
            "\n",
            "Test set: Average loss: 0.7946, Accuracy: 8739/10000 (87.39%)\n",
            "\n",
            "Epoch 381: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 381 [0/50000 (0%)]\tLoss: 0.771871\tAcc: 85.94%\n",
            "Train Epoch: 381 [12800/50000 (26%)]\tLoss: 0.807073\tAcc: 86.57%\n",
            "Train Epoch: 381 [25600/50000 (51%)]\tLoss: 0.843168\tAcc: 85.91%\n",
            "Train Epoch: 381 [38400/50000 (77%)]\tLoss: 0.730978\tAcc: 85.76%\n",
            "\n",
            "Test set: Average loss: 0.8605, Accuracy: 8454/10000 (84.54%)\n",
            "\n",
            "Epoch 382: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 382 [0/50000 (0%)]\tLoss: 0.867235\tAcc: 85.16%\n",
            "Train Epoch: 382 [12800/50000 (26%)]\tLoss: 0.747320\tAcc: 86.19%\n",
            "Train Epoch: 382 [25600/50000 (51%)]\tLoss: 0.835761\tAcc: 86.02%\n",
            "Train Epoch: 382 [38400/50000 (77%)]\tLoss: 0.941820\tAcc: 85.88%\n",
            "\n",
            "Test set: Average loss: 0.8280, Accuracy: 8599/10000 (85.99%)\n",
            "\n",
            "Epoch 383: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 383 [0/50000 (0%)]\tLoss: 0.860882\tAcc: 86.72%\n",
            "Train Epoch: 383 [12800/50000 (26%)]\tLoss: 0.903100\tAcc: 85.84%\n",
            "Train Epoch: 383 [25600/50000 (51%)]\tLoss: 0.964848\tAcc: 86.11%\n",
            "Train Epoch: 383 [38400/50000 (77%)]\tLoss: 0.818687\tAcc: 85.65%\n",
            "\n",
            "Test set: Average loss: 0.8112, Accuracy: 8704/10000 (87.04%)\n",
            "\n",
            "Epoch 384: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 384 [0/50000 (0%)]\tLoss: 0.946941\tAcc: 82.81%\n",
            "Train Epoch: 384 [12800/50000 (26%)]\tLoss: 0.913850\tAcc: 84.85%\n",
            "Train Epoch: 384 [25600/50000 (51%)]\tLoss: 0.794189\tAcc: 85.04%\n",
            "Train Epoch: 384 [38400/50000 (77%)]\tLoss: 0.811937\tAcc: 85.20%\n",
            "\n",
            "Test set: Average loss: 0.9732, Accuracy: 7999/10000 (79.99%)\n",
            "\n",
            "Epoch 385: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 385 [0/50000 (0%)]\tLoss: 0.812314\tAcc: 86.72%\n",
            "Train Epoch: 385 [12800/50000 (26%)]\tLoss: 0.851248\tAcc: 85.19%\n",
            "Train Epoch: 385 [25600/50000 (51%)]\tLoss: 0.826151\tAcc: 85.31%\n",
            "Train Epoch: 385 [38400/50000 (77%)]\tLoss: 0.855636\tAcc: 85.07%\n",
            "\n",
            "Test set: Average loss: 0.8143, Accuracy: 8630/10000 (86.30%)\n",
            "\n",
            "Epoch 386: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 386 [0/50000 (0%)]\tLoss: 0.833445\tAcc: 85.94%\n",
            "Train Epoch: 386 [12800/50000 (26%)]\tLoss: 0.875004\tAcc: 85.71%\n",
            "Train Epoch: 386 [25600/50000 (51%)]\tLoss: 0.738805\tAcc: 85.55%\n",
            "Train Epoch: 386 [38400/50000 (77%)]\tLoss: 0.863603\tAcc: 85.35%\n",
            "\n",
            "Test set: Average loss: 0.8298, Accuracy: 8615/10000 (86.15%)\n",
            "\n",
            "Epoch 387: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 387 [0/50000 (0%)]\tLoss: 0.913495\tAcc: 80.47%\n",
            "Train Epoch: 387 [12800/50000 (26%)]\tLoss: 0.775508\tAcc: 85.01%\n",
            "Train Epoch: 387 [25600/50000 (51%)]\tLoss: 0.765585\tAcc: 84.99%\n",
            "Train Epoch: 387 [38400/50000 (77%)]\tLoss: 0.854889\tAcc: 85.13%\n",
            "\n",
            "Test set: Average loss: 0.8096, Accuracy: 8673/10000 (86.73%)\n",
            "\n",
            "Epoch 388: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 388 [0/50000 (0%)]\tLoss: 0.836935\tAcc: 87.50%\n",
            "Train Epoch: 388 [12800/50000 (26%)]\tLoss: 0.853862\tAcc: 85.27%\n",
            "Train Epoch: 388 [25600/50000 (51%)]\tLoss: 0.816455\tAcc: 85.36%\n",
            "Train Epoch: 388 [38400/50000 (77%)]\tLoss: 0.843299\tAcc: 85.09%\n",
            "\n",
            "Test set: Average loss: 0.8492, Accuracy: 8547/10000 (85.47%)\n",
            "\n",
            "Epoch 389: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 389 [0/50000 (0%)]\tLoss: 0.826864\tAcc: 88.28%\n",
            "Train Epoch: 389 [12800/50000 (26%)]\tLoss: 0.881353\tAcc: 85.16%\n",
            "Train Epoch: 389 [25600/50000 (51%)]\tLoss: 0.885362\tAcc: 84.92%\n",
            "Train Epoch: 389 [38400/50000 (77%)]\tLoss: 0.748835\tAcc: 84.79%\n",
            "\n",
            "Test set: Average loss: 0.8062, Accuracy: 8675/10000 (86.75%)\n",
            "\n",
            "Epoch 390: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 390 [0/50000 (0%)]\tLoss: 0.735339\tAcc: 92.19%\n",
            "Train Epoch: 390 [12800/50000 (26%)]\tLoss: 0.830861\tAcc: 84.82%\n",
            "Train Epoch: 390 [25600/50000 (51%)]\tLoss: 0.834643\tAcc: 84.77%\n",
            "Train Epoch: 390 [38400/50000 (77%)]\tLoss: 0.807615\tAcc: 84.84%\n",
            "\n",
            "Test set: Average loss: 0.8292, Accuracy: 8562/10000 (85.62%)\n",
            "\n",
            "Epoch 391: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 391 [0/50000 (0%)]\tLoss: 0.797939\tAcc: 87.50%\n",
            "Train Epoch: 391 [12800/50000 (26%)]\tLoss: 0.848574\tAcc: 84.93%\n",
            "Train Epoch: 391 [25600/50000 (51%)]\tLoss: 0.830808\tAcc: 85.02%\n",
            "Train Epoch: 391 [38400/50000 (77%)]\tLoss: 0.841412\tAcc: 84.80%\n",
            "\n",
            "Test set: Average loss: 0.7944, Accuracy: 8718/10000 (87.18%)\n",
            "\n",
            "Epoch 392: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 392 [0/50000 (0%)]\tLoss: 0.865372\tAcc: 83.59%\n",
            "Train Epoch: 392 [12800/50000 (26%)]\tLoss: 0.778182\tAcc: 84.92%\n",
            "Train Epoch: 392 [25600/50000 (51%)]\tLoss: 0.845827\tAcc: 84.49%\n",
            "Train Epoch: 392 [38400/50000 (77%)]\tLoss: 0.896996\tAcc: 84.45%\n",
            "\n",
            "Test set: Average loss: 0.9829, Accuracy: 7954/10000 (79.54%)\n",
            "\n",
            "Epoch 393: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 393 [0/50000 (0%)]\tLoss: 0.925239\tAcc: 80.47%\n",
            "Train Epoch: 393 [12800/50000 (26%)]\tLoss: 0.870976\tAcc: 84.51%\n",
            "Train Epoch: 393 [25600/50000 (51%)]\tLoss: 0.936546\tAcc: 84.63%\n",
            "Train Epoch: 393 [38400/50000 (77%)]\tLoss: 0.926529\tAcc: 84.58%\n",
            "\n",
            "Test set: Average loss: 0.8897, Accuracy: 8336/10000 (83.36%)\n",
            "\n",
            "Epoch 394: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 394 [0/50000 (0%)]\tLoss: 0.982543\tAcc: 82.03%\n",
            "Train Epoch: 394 [12800/50000 (26%)]\tLoss: 0.837156\tAcc: 83.97%\n",
            "Train Epoch: 394 [25600/50000 (51%)]\tLoss: 0.910070\tAcc: 84.01%\n",
            "Train Epoch: 394 [38400/50000 (77%)]\tLoss: 0.883725\tAcc: 84.10%\n",
            "\n",
            "Test set: Average loss: 0.7989, Accuracy: 8702/10000 (87.02%)\n",
            "\n",
            "Epoch 395: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 395 [0/50000 (0%)]\tLoss: 0.888834\tAcc: 83.59%\n",
            "Train Epoch: 395 [12800/50000 (26%)]\tLoss: 0.930739\tAcc: 83.72%\n",
            "Train Epoch: 395 [25600/50000 (51%)]\tLoss: 0.878841\tAcc: 84.11%\n",
            "Train Epoch: 395 [38400/50000 (77%)]\tLoss: 0.874150\tAcc: 84.13%\n",
            "\n",
            "Test set: Average loss: 0.8531, Accuracy: 8477/10000 (84.77%)\n",
            "\n",
            "Epoch 396: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 396 [0/50000 (0%)]\tLoss: 0.810538\tAcc: 86.72%\n",
            "Train Epoch: 396 [12800/50000 (26%)]\tLoss: 0.959831\tAcc: 84.56%\n",
            "Train Epoch: 396 [25600/50000 (51%)]\tLoss: 0.800693\tAcc: 84.51%\n",
            "Train Epoch: 396 [38400/50000 (77%)]\tLoss: 0.904617\tAcc: 84.50%\n",
            "\n",
            "Test set: Average loss: 0.8898, Accuracy: 8337/10000 (83.37%)\n",
            "\n",
            "Epoch 397: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 397 [0/50000 (0%)]\tLoss: 0.866805\tAcc: 84.38%\n",
            "Train Epoch: 397 [12800/50000 (26%)]\tLoss: 0.872054\tAcc: 84.58%\n",
            "Train Epoch: 397 [25600/50000 (51%)]\tLoss: 0.837378\tAcc: 84.38%\n",
            "Train Epoch: 397 [38400/50000 (77%)]\tLoss: 0.878782\tAcc: 84.20%\n",
            "\n",
            "Test set: Average loss: 0.8589, Accuracy: 8456/10000 (84.56%)\n",
            "\n",
            "Epoch 398: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 398 [0/50000 (0%)]\tLoss: 0.846350\tAcc: 85.16%\n",
            "Train Epoch: 398 [12800/50000 (26%)]\tLoss: 0.878502\tAcc: 84.43%\n",
            "Train Epoch: 398 [25600/50000 (51%)]\tLoss: 0.987603\tAcc: 84.39%\n",
            "Train Epoch: 398 [38400/50000 (77%)]\tLoss: 0.931985\tAcc: 84.30%\n",
            "\n",
            "Test set: Average loss: 0.8138, Accuracy: 8641/10000 (86.41%)\n",
            "\n",
            "Epoch 399: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 399 [0/50000 (0%)]\tLoss: 0.840414\tAcc: 84.38%\n",
            "Train Epoch: 399 [12800/50000 (26%)]\tLoss: 0.801676\tAcc: 84.44%\n",
            "Train Epoch: 399 [25600/50000 (51%)]\tLoss: 0.885109\tAcc: 84.32%\n",
            "Train Epoch: 399 [38400/50000 (77%)]\tLoss: 0.848593\tAcc: 84.28%\n",
            "\n",
            "Test set: Average loss: 0.9280, Accuracy: 8199/10000 (81.99%)\n",
            "\n",
            "Epoch 400: Fine-tuning with Sparsity: 97.97%\n",
            "Train Epoch: 400 [0/50000 (0%)]\tLoss: 0.937807\tAcc: 81.25%\n",
            "Train Epoch: 400 [12800/50000 (26%)]\tLoss: 0.960625\tAcc: 84.29%\n",
            "Train Epoch: 400 [25600/50000 (51%)]\tLoss: 0.875886\tAcc: 84.28%\n",
            "Train Epoch: 400 [38400/50000 (77%)]\tLoss: 0.863042\tAcc: 84.37%\n",
            "\n",
            "Test set: Average loss: 0.8792, Accuracy: 8385/10000 (83.85%)\n",
            "\n",
            "\n",
            "Training completed!\n",
            "Best accuracy: 93.98%\n",
            "Best accuracy at >97% sparsity: 93.44%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_414c02c6-804c-4525-be89-2da132d4d7b1\", \"improved_training_results.pdf\", 31377)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ PDF downloaded to your computer!\n",
            "\n",
            "==================================================\n",
            "Final Results:\n",
            "Best overall accuracy: 93.98%\n",
            "Best accuracy at >97% sparsity: 93.44%\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}